<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Midterm Review | MATH 253: Machine Learning</title>
  <meta name="description" content="This is the class activity manual for Math 253 at Macalester College.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Midterm Review | MATH 253: Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is the class activity manual for Math 253 at Macalester College." />
  <meta name="github-repo" content="lmyint/253_spring_2019" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Midterm Review | MATH 253: Machine Learning" />
  
  <meta name="twitter:description" content="This is the class activity manual for Math 253 at Macalester College." />
  




  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="hierarchical-clustering.html">
<link rel="next" href="final-project-instructions.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #2a211c; color: #bdae9d; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; background-color: #2a211c; color: #bdae9d; border-right: 1px solid #bdae9d; }
td.sourceCode { padding-left: 5px; }
pre, code { color: #bdae9d; background-color: #2a211c; }
code > span.kw { color: #43a8ed; font-weight: bold; } /* Keyword */
code > span.dt { text-decoration: underline; } /* DataType */
code > span.dv { color: #44aa43; } /* DecVal */
code > span.bn { color: #44aa43; } /* BaseN */
code > span.fl { color: #44aa43; } /* Float */
code > span.ch { color: #049b0a; } /* Char */
code > span.st { color: #049b0a; } /* String */
code > span.co { color: #0066ff; font-style: italic; } /* Comment */
code > span.al { color: #ffff00; } /* Alert */
code > span.fu { color: #ff9358; font-weight: bold; } /* Function */
code > span.er { color: #ffff00; font-weight: bold; } /* Error */
code > span.wa { color: #ffff00; font-weight: bold; } /* Warning */
code > span.cn { } /* Constant */
code > span.sc { color: #049b0a; } /* SpecialChar */
code > span.vs { color: #049b0a; } /* VerbatimString */
code > span.ss { color: #049b0a; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { } /* Variable */
code > span.cf { color: #43a8ed; font-weight: bold; } /* ControlFlow */
code > span.op { } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { font-weight: bold; } /* Preprocessor */
code > span.at { } /* Attribute */
code > span.do { color: #0066ff; font-style: italic; } /* Documentation */
code > span.an { color: #0066ff; font-weight: bold; font-style: italic; } /* Annotation */
code > span.co { color: #0066ff; font-weight: bold; font-style: italic; } /* Comment */
code > span.in { color: #0066ff; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href = "./">MATH 253: Machine Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="schedule.html"><a href="schedule.html"><i class="fa fa-check"></i>Schedule</a><ul>
<li class="chapter" data-level="" data-path="schedule.html"><a href="schedule.html#tentative-overall-schedule"><i class="fa fa-check"></i>Tentative overall schedule</a></li>
<li class="chapter" data-level="" data-path="schedule.html"><a href="schedule.html#week-2-128---21"><i class="fa fa-check"></i>Week 2: 1/28 - 2/1</a></li>
<li class="chapter" data-level="" data-path="schedule.html"><a href="schedule.html#week-3-24---28"><i class="fa fa-check"></i>Week 3: 2/4 - 2/8</a></li>
<li class="chapter" data-level="" data-path="schedule.html"><a href="schedule.html#week-4-211---215"><i class="fa fa-check"></i>Week 4: 2/11 - 2/15</a></li>
<li class="chapter" data-level="" data-path="schedule.html"><a href="schedule.html#week-5-218---222"><i class="fa fa-check"></i>Week 5: 2/18 - 2/22</a></li>
<li class="chapter" data-level="" data-path="schedule.html"><a href="schedule.html#week-6-225---31"><i class="fa fa-check"></i>Week 6: 2/25 - 3/1</a></li>
<li class="chapter" data-level="" data-path="schedule.html"><a href="schedule.html#week-7-34---38"><i class="fa fa-check"></i>Week 7: 3/4 - 3/8</a></li>
<li class="chapter" data-level="" data-path="schedule.html"><a href="schedule.html#week-8-311---315"><i class="fa fa-check"></i>Week 8: 3/11 - 3/15</a></li>
<li class="chapter" data-level="" data-path="schedule.html"><a href="schedule.html#week-9-325---329"><i class="fa fa-check"></i>Week 9: 3/25 - 3/29</a></li>
<li class="chapter" data-level="" data-path="schedule.html"><a href="schedule.html#week-10-41---45"><i class="fa fa-check"></i>Week 10: 4/1 - 4/5</a></li>
<li class="chapter" data-level="" data-path="schedule.html"><a href="schedule.html#week-11-48---412"><i class="fa fa-check"></i>Week 11: 4/8 - 4/12</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="ml-and-society.html"><a href="ml-and-society.html"><i class="fa fa-check"></i>ML and Society</a></li>
<li class="part"><span><b>I Regression: Model Evaluation</b></span></li>
<li class="chapter" data-level="1" data-path="motivation-and-review.html"><a href="motivation-and-review.html"><i class="fa fa-check"></i><b>1</b> Motivation and Review</a><ul>
<li class="chapter" data-level="1.1" data-path="motivation-and-review.html"><a href="motivation-and-review.html#activity-motivating-main-ideas"><i class="fa fa-check"></i><b>1.1</b> Activity: motivating main ideas</a><ul>
<li class="chapter" data-level="" data-path="motivation-and-review.html"><a href="motivation-and-review.html#situation-a"><i class="fa fa-check"></i>Situation A</a></li>
<li class="chapter" data-level="" data-path="motivation-and-review.html"><a href="motivation-and-review.html#situation-b"><i class="fa fa-check"></i>Situation B</a></li>
<li class="chapter" data-level="" data-path="motivation-and-review.html"><a href="motivation-and-review.html#situation-c"><i class="fa fa-check"></i>Situation C</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="motivation-and-review.html"><a href="motivation-and-review.html#review-exercises"><i class="fa fa-check"></i><b>1.2</b> Review exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="regression-assumptions.html"><a href="regression-assumptions.html"><i class="fa fa-check"></i><b>2</b> Regression Assumptions</a><ul>
<li class="chapter" data-level="2.1" data-path="regression-assumptions.html"><a href="regression-assumptions.html#discussion"><i class="fa fa-check"></i><b>2.1</b> Discussion</a></li>
<li class="chapter" data-level="2.2" data-path="regression-assumptions.html"><a href="regression-assumptions.html#exercises"><i class="fa fa-check"></i><b>2.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="accuracy-metrics-for-regression.html"><a href="accuracy-metrics-for-regression.html"><i class="fa fa-check"></i><b>3</b> Accuracy Metrics for Regression</a><ul>
<li class="chapter" data-level="3.1" data-path="accuracy-metrics-for-regression.html"><a href="accuracy-metrics-for-regression.html#discussion-1"><i class="fa fa-check"></i><b>3.1</b> Discussion</a></li>
<li class="chapter" data-level="3.2" data-path="accuracy-metrics-for-regression.html"><a href="accuracy-metrics-for-regression.html#exercises-1"><i class="fa fa-check"></i><b>3.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="cross-validation.html"><a href="cross-validation.html"><i class="fa fa-check"></i><b>4</b> Cross-Validation</a><ul>
<li class="chapter" data-level="4.1" data-path="cross-validation.html"><a href="cross-validation.html#discussion-2"><i class="fa fa-check"></i><b>4.1</b> Discussion</a></li>
<li class="chapter" data-level="4.2" data-path="cross-validation.html"><a href="cross-validation.html#exercises-2"><i class="fa fa-check"></i><b>4.2</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>II Regression: Model Building</b></span></li>
<li class="chapter" data-level="5" data-path="subset-selection.html"><a href="subset-selection.html"><i class="fa fa-check"></i><b>5</b> Subset Selection</a><ul>
<li class="chapter" data-level="5.1" data-path="subset-selection.html"><a href="subset-selection.html#discussion-3"><i class="fa fa-check"></i><b>5.1</b> Discussion</a></li>
<li class="chapter" data-level="5.2" data-path="subset-selection.html"><a href="subset-selection.html#exercises-3"><i class="fa fa-check"></i><b>5.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="shrinkageregularization.html"><a href="shrinkageregularization.html"><i class="fa fa-check"></i><b>6</b> Shrinkage/Regularization</a><ul>
<li class="chapter" data-level="6.1" data-path="shrinkageregularization.html"><a href="shrinkageregularization.html#discussion-4"><i class="fa fa-check"></i><b>6.1</b> Discussion</a></li>
<li class="chapter" data-level="6.2" data-path="shrinkageregularization.html"><a href="shrinkageregularization.html#exercises-4"><i class="fa fa-check"></i><b>6.2</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>III Regression: More Flexibility</b></span></li>
<li class="chapter" data-level="7" data-path="knn-bias-variance-tradeoff.html"><a href="knn-bias-variance-tradeoff.html"><i class="fa fa-check"></i><b>7</b> KNN &amp; Bias-Variance Tradeoff</a><ul>
<li class="chapter" data-level="7.1" data-path="knn-bias-variance-tradeoff.html"><a href="knn-bias-variance-tradeoff.html#discussion-5"><i class="fa fa-check"></i><b>7.1</b> Discussion</a></li>
<li class="chapter" data-level="7.2" data-path="knn-bias-variance-tradeoff.html"><a href="knn-bias-variance-tradeoff.html#exercises-5"><i class="fa fa-check"></i><b>7.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="splines.html"><a href="splines.html"><i class="fa fa-check"></i><b>8</b> Splines</a><ul>
<li class="chapter" data-level="8.1" data-path="splines.html"><a href="splines.html#discussion-6"><i class="fa fa-check"></i><b>8.1</b> Discussion</a></li>
<li class="chapter" data-level="8.2" data-path="splines.html"><a href="splines.html#exercises-6"><i class="fa fa-check"></i><b>8.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="local-regression-and-gams.html"><a href="local-regression-and-gams.html"><i class="fa fa-check"></i><b>9</b> Local Regression and GAMs</a><ul>
<li class="chapter" data-level="9.1" data-path="local-regression-and-gams.html"><a href="local-regression-and-gams.html#discussion-7"><i class="fa fa-check"></i><b>9.1</b> Discussion</a></li>
<li class="chapter" data-level="9.2" data-path="local-regression-and-gams.html"><a href="local-regression-and-gams.html#exercises-7"><i class="fa fa-check"></i><b>9.2</b> Exercises</a></li>
<li class="chapter" data-level="9.3" data-path="local-regression-and-gams.html"><a href="local-regression-and-gams.html#how-to-choose-between-methods"><i class="fa fa-check"></i><b>9.3</b> How to choose between methods?!?</a></li>
</ul></li>
<li class="part"><span><b>IV Classification</b></span></li>
<li class="chapter" data-level="10" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>10</b> Logistic Regression</a><ul>
<li class="chapter" data-level="10.1" data-path="logistic-regression.html"><a href="logistic-regression.html#discussion-8"><i class="fa fa-check"></i><b>10.1</b> Discussion</a></li>
<li class="chapter" data-level="10.2" data-path="logistic-regression.html"><a href="logistic-regression.html#exercises-8"><i class="fa fa-check"></i><b>10.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="revisiting-old-tools.html"><a href="revisiting-old-tools.html"><i class="fa fa-check"></i><b>11</b> Revisiting Old Tools</a><ul>
<li class="chapter" data-level="11.1" data-path="revisiting-old-tools.html"><a href="revisiting-old-tools.html#discussion-9"><i class="fa fa-check"></i><b>11.1</b> Discussion</a></li>
<li class="chapter" data-level="11.2" data-path="revisiting-old-tools.html"><a href="revisiting-old-tools.html#exercises-9"><i class="fa fa-check"></i><b>11.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>12</b> Decision Trees</a><ul>
<li class="chapter" data-level="12.1" data-path="decision-trees.html"><a href="decision-trees.html#discussion-10"><i class="fa fa-check"></i><b>12.1</b> Discussion</a></li>
<li class="chapter" data-level="12.2" data-path="decision-trees.html"><a href="decision-trees.html#exercises-10"><i class="fa fa-check"></i><b>12.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="bagging-and-random-forests.html"><a href="bagging-and-random-forests.html"><i class="fa fa-check"></i><b>13</b> Bagging and Random Forests</a><ul>
<li class="chapter" data-level="13.1" data-path="bagging-and-random-forests.html"><a href="bagging-and-random-forests.html#exercises-11"><i class="fa fa-check"></i><b>13.1</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="support-vector-machines-part-1.html"><a href="support-vector-machines-part-1.html"><i class="fa fa-check"></i><b>14</b> Support Vector Machines (Part 1)</a><ul>
<li class="chapter" data-level="14.1" data-path="support-vector-machines-part-1.html"><a href="support-vector-machines-part-1.html#exercises-12"><i class="fa fa-check"></i><b>14.1</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="support-vector-machines-part-2.html"><a href="support-vector-machines-part-2.html"><i class="fa fa-check"></i><b>15</b> Support Vector Machines (Part 2)</a><ul>
<li class="chapter" data-level="15.1" data-path="support-vector-machines-part-2.html"><a href="support-vector-machines-part-2.html#exercises-13"><i class="fa fa-check"></i><b>15.1</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>V Unsupervised Learning</b></span></li>
<li class="chapter" data-level="16" data-path="k-means-clustering.html"><a href="k-means-clustering.html"><i class="fa fa-check"></i><b>16</b> K-Means Clustering</a><ul>
<li class="chapter" data-level="16.1" data-path="k-means-clustering.html"><a href="k-means-clustering.html#exercises-14"><i class="fa fa-check"></i><b>16.1</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html"><i class="fa fa-check"></i><b>17</b> Hierarchical Clustering</a><ul>
<li class="chapter" data-level="17.1" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#exercises-15"><i class="fa fa-check"></i><b>17.1</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>Review</b></span></li>
<li class="chapter" data-level="" data-path="midterm-review.html"><a href="midterm-review.html"><i class="fa fa-check"></i>Midterm Review</a><ul>
<li class="chapter" data-level="" data-path="midterm-review.html"><a href="midterm-review.html#organizing-information-on-all-methods"><i class="fa fa-check"></i>Organizing information on all methods</a></li>
<li class="chapter" data-level="" data-path="midterm-review.html"><a href="midterm-review.html#review-of-specific-methods"><i class="fa fa-check"></i>Review of specific methods</a><ul>
<li class="chapter" data-level="" data-path="midterm-review.html"><a href="midterm-review.html#subset-selection-1"><i class="fa fa-check"></i>Subset selection</a></li>
<li class="chapter" data-level="" data-path="midterm-review.html"><a href="midterm-review.html#lasso"><i class="fa fa-check"></i>LASSO</a></li>
<li class="chapter" data-level="" data-path="midterm-review.html"><a href="midterm-review.html#splines-1"><i class="fa fa-check"></i>Splines</a></li>
<li class="chapter" data-level="" data-path="midterm-review.html"><a href="midterm-review.html#generalzed-additive-models-gams"><i class="fa fa-check"></i>Generalzed additive models (GAMs)</a></li>
<li class="chapter" data-level="" data-path="midterm-review.html"><a href="midterm-review.html#knn-and-decision-trees"><i class="fa fa-check"></i>KNN and decision trees</a></li>
<li class="chapter" data-level="" data-path="midterm-review.html"><a href="midterm-review.html#logistic-regression-1"><i class="fa fa-check"></i>Logistic regression</a></li>
<li class="chapter" data-level="" data-path="midterm-review.html"><a href="midterm-review.html#bagging-and-random-forests-1"><i class="fa fa-check"></i>Bagging and random forests</a></li>
<li class="chapter" data-level="" data-path="midterm-review.html"><a href="midterm-review.html#test-set-performance"><i class="fa fa-check"></i>Test set performance</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Final Project</b></span></li>
<li class="chapter" data-level="" data-path="final-project-instructions.html"><a href="final-project-instructions.html"><i class="fa fa-check"></i>Final Project Instructions</a><ul>
<li class="chapter" data-level="" data-path="final-project-instructions.html"><a href="final-project-instructions.html#overview"><i class="fa fa-check"></i>Overview</a></li>
<li class="chapter" data-level="" data-path="final-project-instructions.html"><a href="final-project-instructions.html#initial-check-in"><i class="fa fa-check"></i>Initial check-in</a></li>
<li class="chapter" data-level="" data-path="final-project-instructions.html"><a href="final-project-instructions.html#final-report"><i class="fa fa-check"></i>Final report</a></li>
<li class="chapter" data-level="" data-path="final-project-instructions.html"><a href="final-project-instructions.html#final-presentation"><i class="fa fa-check"></i>Final presentation</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="cross-validation-1.html"><a href="cross-validation-1.html"><i class="fa fa-check"></i><b>A</b> Cross-Validation</a><ul>
<li class="chapter" data-level="A.1" data-path="cross-validation-1.html"><a href="cross-validation-1.html#objects"><i class="fa fa-check"></i><b>A.1</b> Objects</a></li>
<li class="chapter" data-level="A.2" data-path="cross-validation-1.html"><a href="cross-validation-1.html#subsetting"><i class="fa fa-check"></i><b>A.2</b> Subsetting</a></li>
<li class="chapter" data-level="A.3" data-path="cross-validation-1.html"><a href="cross-validation-1.html#writing-r-functions"><i class="fa fa-check"></i><b>A.3</b> Writing R functions</a></li>
<li class="chapter" data-level="A.4" data-path="cross-validation-1.html"><a href="cross-validation-1.html#for-loops-and-control-flow"><i class="fa fa-check"></i><b>A.4</b> <code>for</code>-loops and control flow</a></li>
<li class="chapter" data-level="A.5" data-path="cross-validation-1.html"><a href="cross-validation-1.html#building-our-cross-validation-function"><i class="fa fa-check"></i><b>A.5</b> Building our cross-validation function!</a></li>
<li class="chapter" data-level="A.6" data-path="cross-validation-1.html"><a href="cross-validation-1.html#aside-apply-functions"><i class="fa fa-check"></i><b>A.6</b> Aside: <code>apply()</code> functions</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="splines-2.html"><a href="splines-2.html"><i class="fa fa-check"></i><b>B</b> Splines</a><ul>
<li class="chapter" data-level="B.1" data-path="splines-2.html"><a href="splines-2.html#exercise"><i class="fa fa-check"></i><b>B.1</b> Exercise</a></li>
<li class="chapter" data-level="B.2" data-path="splines-2.html"><a href="splines-2.html#debriefing"><i class="fa fa-check"></i><b>B.2</b> Debriefing</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MATH 253: Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="midterm-review" class="section level1 unnumbered">
<h1>Midterm Review</h1>
<p><a href="https://docs.google.com/document/d/1lT7Qz7Lw4Q0qBv3Z5k7VlZQKtJtrPUfLS570CmmgYnM/edit?usp=sharing">Shared Google doc for midterm review</a></p>
<p><br> <br></p>
<div id="organizing-information-on-all-methods" class="section level2 unnumbered">
<h2>Organizing information on all methods</h2>
<div class="figure">
<img src="images/class_flow.png" />

</div>
<p>For each statistical learning method that we’ve learned about, you might find it useful to organize information on the following aspects:</p>
<ul>
<li>Name of method</li>
<li>How does this method work?</li>
<li>How to use this method to make predictions?</li>
<li>Tuning parameters</li>
<li>When would I use this method? (Pros and cons)</li>
</ul>
<p><br> <br></p>
<p><strong>How does this method work?</strong></p>
<p>It is important that you understand how these different methods work from an algorithmic standpoint. If you do, you could implement them yourselves with enough familiarity with a programming language. For this part, write <strong>rough pseudocode</strong> describing how the method works to build the model. Try to be explicit to the level of “for every … in …, do the following” whenever iteration is required in the algorithm.</p>
<p><br> <br></p>
<p><strong>How to use this method to make predictions?</strong></p>
<p>Once we have a trained model, how do we use it to make a prediction for a new test case? For regression tasks, how is a single quantitative prediction made? For classification tasks, how is a single soft or hard prediction made?</p>
<p><br> <br></p>
<p><strong>Tuning parameters</strong></p>
<p>Tuning parameters are parameters that we can adjust to tweak these methods. How do adjustments to parameters affect model complexity, bias, variance?</p>
<p><br> <br></p>
<p><strong>When would I use this method?</strong></p>
<p>Pros: What nice things does this method allow me to do?</p>
<p>Cons: But no method is perfect…do any of the themes below help us think about potential cons of the method?</p>
<p><br> <br></p>
<p><strong>Themes</strong></p>
<p>Not all of themes relate to pros/cons, but they are different ways to think about specific methods and general behavior of methods.</p>
<ul>
<li>“Greedy” algorithmic behaivor</li>
<li>Penalities</li>
<li>Overfitting</li>
<li>Bias-variance tradeoff</li>
<li>Parametric vs. nonparametric methods</li>
<li>Curse of dimensionality</li>
<li>Training performance vs. test performance</li>
</ul>
<p><br> <br></p>
<p><strong>Methods</strong></p>
<ul>
<li>Subset selection
<ul>
<li>Best subset</li>
<li>Stepwise selection (forward, backward)</li>
</ul></li>
<li>LASSO</li>
<li>KNN</li>
<li>Splines</li>
<li>Local regression (LOESS)</li>
<li>GAMs</li>
<li>Logistic regression</li>
<li>Decision trees</li>
<li>Bagged trees</li>
<li>Random forests</li>
</ul>
<p><br> <br> <br> <br> <br></p>
</div>
<div id="review-of-specific-methods" class="section level2 unnumbered">
<h2>Review of specific methods</h2>
<p>Below is a collection of assorted review ideas and practice questions (not necessarily exhaustive).</p>
<div id="subset-selection-1" class="section level3 unnumbered">
<h3>Subset selection</h3>
<p>Questions:</p>
<ul>
<li>For a dataset with 5 variables, describe in detail the forward and backward stepwise selection process. Exactly how many models are fit and compared at each stage? What about with best subset selection?</li>
</ul>
<p><br> <br></p>
</div>
<div id="lasso" class="section level3 unnumbered">
<h3>LASSO</h3>
<p>Questions:</p>
<ul>
<li>Compile a list of all the instances where we have seen penalty terms so far. Describe a common underlying framework for thinking about all of these instances.</li>
</ul>
<p><br> <br></p>
</div>
<div id="splines-1" class="section level3 unnumbered">
<h3>Splines</h3>
<p>Review ideas:</p>
<ul>
<li>A natural cubic spline is a function that consists of piecewise polynomials stitched together across multiple subregions of a predictor.
<ul>
<li>These subregions are formed by cutting the span of the predictor at break points called <strong>knots</strong>.</li>
<li>The piecewise polynomials are continuous at the knots and have continuous first and second derivatives at the knots.</li>
<li>The spline function is linear in the region lower than the minimum value of the predictor variable and linear in the region higher than the maximum value. (This point is not the most important, but the first two points above are more important.)</li>
</ul></li>
<li>If you choose to break a predictor by choosing <span class="math inline">\(K\)</span> knots, <span class="math inline">\(K+1\)</span> regions are formed, which is the <strong>degrees of freedom (df)</strong> tuning parameter for splines.</li>
<li>The nice thing about a spline is that you can represent this spline function with a <strong>weighted sum</strong> of simpler polynomial functions.
<ul>
<li>e.g. We desire the relationship between <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span> to be modeled with a spline with 1 knot at <span class="math inline">\(x = k\)</span>. I can create a spline function to represent this relationship by adding and subtracting together a certain amount of <span class="math inline">\(f_1(x) = x\)</span>, <span class="math inline">\(f_2(x) = x^2\)</span>, <span class="math inline">\(f_3(x) = x^3\)</span>, and <span class="math inline">\(f_4(x) = (x-k)^3_+\)</span>.<br />
The + in <span class="math inline">\((x-k)^3_+\)</span> denotes the positive part. That is, <span class="math inline">\((x-k)^3_+\)</span> equals <span class="math inline">\((x-k)^3\)</span> when <span class="math inline">\(x &gt; k\)</span> and equals 0 otherwise.<br />
If you are curious about the math behind this look at the “Splines” chapter in the Appendix.</li>
</ul></li>
<li>These simpler polynomial functions thus define <strong>variable transformations</strong> of the predictor. These transformed versions of the predictors can be treated as variables like any other predictors to be included in an ordinary regression model.</li>
</ul>
<p><br> <br></p>
</div>
<div id="generalzed-additive-models-gams" class="section level3 unnumbered">
<h3>Generalzed additive models (GAMs)</h3>
<p>Review ideas:</p>
<p>Instead of assuming <strong>linear</strong> relationships between predictors and the response…</p>
<p><span class="math display">\[\text{Outstate} = \beta_0 + \beta_1\text{PhD} + \beta_2\text{Expend} + \varepsilon\]</span></p>
<p>…let’s be more general and say that the relationships can be arbitrary functions:</p>
<p><span class="math display">\[\text{Outstate} = \beta_0 + f_1(\text{PhD}) + f_2(\text{Expend}) + \varepsilon\]</span></p>
<ul>
<li>If the functions <span class="math inline">\(f_1, f_2\)</span> can be described with splines, then the model is just like an ordinary linear regression model and can be fit with least squares.
<ul>
<li>Let’s say that we want 2 knots for each of <code>PhD</code> and <code>Expend</code>.</li>
<li>2 knots = 3 regions = 3 degrees of freedom (df)</li>
<li>Then <span class="math inline">\(f_1\)</span> can be represented mathematically with a weighted sum of 3 simpler polynomials <span class="math inline">\(a_1(\text{PhD})\)</span>, <span class="math inline">\(a_2(\text{PhD})\)</span>, and <span class="math inline">\(a_3(\text{PhD})\)</span>. These simpler polynomials define 3 transformed versions of the original <code>PhD</code> variable.</li>
<li>And <span class="math inline">\(f_2\)</span> can be represented mathematically with a weighted sum of 3 simpler polynomials <span class="math inline">\(b_1(\text{Expend})\)</span>, <span class="math inline">\(b_2(\text{Expend})\)</span>, and <span class="math inline">\(b_3(\text{Expend})\)</span>. These simpler polynomials define 3 transformed versions of the original <code>Expend</code> variable.</li>
</ul></li>
<li>We can also have these functions be described with LOESS functions.
<ul>
<li><span class="math inline">\(f_1\)</span> can be represented by a function drawn from local linear regressions.</li>
<li><span class="math inline">\(f_2\)</span> can be represented by a function drawn from local linear regressions.</li>
</ul></li>
<li>Note that regardless of how <span class="math inline">\(f_1, f_2, \ldots\)</span> are built, we interpret them as how the response changes with that predictor, holding constant the other predictors.</li>
</ul>
<p><br> <br></p>
</div>
<div id="knn-and-decision-trees" class="section level3 unnumbered">
<h3>KNN and decision trees</h3>
<p>Questions:</p>
<ul>
<li>KNN and decision trees are both nonparametric methods for regression and classification. Contrast how these models are different from parametric linear and logistic regression models.</li>
<li>KNN and decision trees are both nonparametric methods, but differ in the form that they assume for the relationship between the response and the predictors. Describe this and draw a 2-dimensional (2 predictor) example to highlight the difference.</li>
<li>In our ISLR book: exercises 4 and 5 in section 8.4</li>
</ul>
<p><br> <br></p>
</div>
<div id="logistic-regression-1" class="section level3 unnumbered">
<h3>Logistic regression</h3>
<p>Questions:</p>
<ul>
<li>In our ISLR book: exercises 6 and 9 in section 4.7</li>
<li>Can the “No Information Rate” be computed with 3 or more classes? If so, how?</li>
<li>Can sensitivity, specificity, positive predictive value, and negative predictive value be defined with 3 or more classes? If so, how?</li>
<li>Can ROC curves, AUC be computed with 3 or more classes? If so, how?</li>
</ul>
<p><br> <br></p>
</div>
<div id="bagging-and-random-forests-1" class="section level3 unnumbered">
<h3>Bagging and random forests</h3>
<p>Questions:</p>
<ul>
<li>We’ve discussed bootstrap aggregating (bagging) in the context of decision trees. Could bagging apply to linear or logistic regression? How?</li>
<li>Come up with a specific numerical example with a small dataset and small number of trees that fully shows how out of bag (OOB) test error is computed.</li>
</ul>
<p><br> <br></p>
</div>
<div id="test-set-performance" class="section level3 unnumbered">
<h3>Test set performance</h3>
<p>Questions:</p>
<ul>
<li>We have mostly focused on test RMSE (regression) and test overall accuracy (classification). We have many metrics for evaluating classification models that are useful in different ways/in different contexts. There are test versions of these metrics. For example, there are notions of test sensitivity, test specificity, test AUC, test confusion matrix. How are these computed with cross-validation or an out-of-bag (OOB) procedure?</li>
</ul>

</div>
</div>
</div>



            </section>

          </div>
        </div>
      </div>
<a href="hierarchical-clustering.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="final-project-instructions.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
