[
["index.html", "MATH 253: Machine Learning Preface", " MATH 253: Machine Learning Preface Image source This is the class manual for Machine Learning (MATH 253) at Macalester College. The content here was made by Leslie Myint and draws upon our class textbook, An Introduction to Statistical Learning with Applications in R, and on materials prepared by Alicia Johnson. This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],
["schedule.html", "Schedule Tentative overall schedule Week 2: 1/28 - 2/1 Week 3: 2/4 - 2/8 Week 4: 2/11 - 2/15 Week 5: 2/18 - 2/22 Week 6: 2/25 - 3/1 Week 7: 3/4 - 3/8 Week 8: 3/11 - 3/15 Week 9: 3/25 - 3/29", " Schedule Topics for each class day, as well as links to the pre-class videos and slides, are listed below. Before each class, watch the pre-class video and answer the corresponding comprehension questions on Moodle. Tentative overall schedule Regression tasks: Weeks 2-5 Topics: model evaluation, model building, nonparametric methods, tools for modeling nonlinearity Classification tasks: Weeks 6-9 Topics: model evaluation, logistic regression, tree-based methods, support vector methods Unsupervised learning: Weeks 10-11 Topics: principal components analysis/regression, clustering Other topics + project work time: Weeks 12-14 Suggested topic: deep learning Week 2: 1/28 - 2/1 Monday: Assumptions of linear regression (Video, Slides) Related ISLR reading: Section 2.1 gives some general background that is not particular to linear regression assumptions but sets up some key foundational ideas. Friday: Model evaluation metrics for regression (Video, Slides) Related ISLR reading: Sections 2.2.1-2.2.2 talk about MSE, training and test data, test error, cross-validation, and the bias-variance tradeoff. The bias-variance tradeoff will come up a little later in class, but feel free to preview the ideas now. Week 3: 2/4 - 2/8 Monday: Cross-validation (Video, Slides) Related ISLR reading: Section 5.1 is devoted to cross-validation. See also Sections 2.2.1-2.2.2. Wednesday: We’ll finish reviewing cross validation, then talk briefly about variable selection methods for building models (Video, Slides) Related ISLR reading: Section 6.1 Friday: Quiz 1. Remaining time will be left for working on homework. Week 4: 2/11 - 2/15 Monday: Shrinkage/regularization methods for model building (Video, Slides) Related ISLR reading: Section 6.2 Wednesday: Continue discussing shrinkage methods Friday: K-nearest neighbors regression and the bias-variance tradeoff (Video, Slides) Related ISLR reading: Section 2.2.2 for the bias-variance tradeoff and Section 3.5 for K-nearest neighbors regression Week 5: 2/18 - 2/22 Monday: Modeling nonlinear trends with natural splines (Video, Slides) Related ISLR reading: Sections 7.1-7.4 Wednesday: Local regression and generalized additive models (Video, Slides) Related ISLR reading: Sections 7.6-7.7 Friday: Quiz 2. Remaining time for a review activity. Week 6: 2/25 - 3/1 Monday: Logistic regression (Video, Slides) Related ISLR reading: Sections 4.1-4.3 Wednesday: Finish up logistic regression. Using old tools in the classification setting Related ISLR reading: Section 7.7.2 (GAMs for Classification Problems), pages 39-42 (KNN for classification) Friday: Decision trees (Video, Slides) Related ISLR reading: 8.1 Week 7: 3/4 - 3/8 Monday: Decision trees Wednesday: Bagging and random forests (Video, Slides) Related ISLR reading: 8.2 Friday: Quiz 3. Finish up bagging and random forests. Week 8: 3/11 - 3/15 Review &amp; midterm exam Happy Spring Break! Week 9: 3/25 - 3/29 "],
["ml-and-society.html", "ML and Society", " ML and Society Below are some links relating to the role that machine learning plays a role in our society and in various disciplines. If you come across something interesting and would like to share it here, feel free to email the instructor! Amazon scraps secret AI recruiting tool that showed bias against women WSJ Podcast: Machine Love: Dating in the Digital Age (Thanks, Meera!) Related: How a Math Genius Hacked OkCupid to Find True Love ML and astrophysics (Thanks, Karen!) Automated background checks are deciding who’s fit for a home (Thanks, Eleanor!) How many high school stars make it in the NBA? The AI-Art Gold Rush Is Here (Thanks, Blake!) "],
["motivation-and-review.html", "Topic 1 Motivation and Review 1.1 Activity: motivating main ideas 1.2 Review exercises", " Topic 1 Motivation and Review 1.1 Activity: motivating main ideas In each of the following situations, there is some behind-the-scenes code that performs an analysis and generates some output plots. Brainstorm what research question(s) are trying to be answered in each of the situations by looking at the first few rows of data and the plots. Situation A We have 10,000 observations on the following variables: ID: Identification Income: Income in $10,000’s Limit: Credit limit Rating: Credit rating Cards: Number of credit cards Age: Age in years Education: Number of years of education Gender: A factor with levels Male and Female Student: A factor with levels No and Yes indicating whether the individual was a student Married: A factor with levels No and Yes indicating whether the individual was married Ethnicity: A factor with levels African American, Asian, and Caucasian indicating the individual’s ethnicity Balance: Average credit card balance in $. ## Look at the first few rows head(Credit) ## ID Income Limit Rating Cards Age Education Gender Student Married ## 1 1 14.891 3606 283 2 34 11 Male No Yes ## 2 2 106.025 6645 483 3 82 15 Female Yes Yes ## 3 3 104.593 7075 514 4 71 11 Male No No ## 4 4 148.924 9504 681 3 36 11 Female No No ## 5 5 55.882 4897 357 2 68 16 Male No Yes ## 6 6 80.180 8047 569 4 77 10 Male No No ## Ethnicity Balance ## 1 Caucasian 333 ## 2 Asian 903 ## 3 Asian 580 ## 4 Asian 964 ## 5 Caucasian 331 ## 6 Caucasian 1151 Situation B We have 400 observations of the following variables: admit: binary variable; 0 = rejected, 1 = admitted gre: applicant’s GRE score GPA: applicant’s undergraduate GPA rank: A “prestige” ranking of the applicant’s undergraduate institution. 1 to 4 going from least to most prestigious head(grad) ## admit gre gpa rank ## 1 No 380 3.61 3 ## 2 Yes 660 3.67 3 ## 3 Yes 800 4.00 1 ## 4 Yes 640 3.19 4 ## 5 No 520 2.93 4 ## 6 Yes 760 3.00 2 Situation C We have 85 different Halloween candies and measured the following variables: competitorname: The name of the Halloween candy. chocolate: Does it contain chocolate? fruity: Is it fruit flavored? caramel: Is there caramel in the candy? peanutyalmondy: Does it contain peanuts, peanut butter or almonds? nougat: Does it contain nougat? crispedricewafer: Does it contain crisped rice, wafers, or a cookie component? hard: Is it a hard candy? bar: Is it a candy bar? pluribus: Is it one of many candies in a bag or box? sugarpercent: The percentile of sugar it falls under within the dataset. pricepercent: The unit price percentile compared to the rest of the dataset. winpercent: The overall win percentage according to 269,000 matchups. head(candy_rankings) ## competitorname chocolate fruity caramel peanutyalmondy nougat ## 1 100 Grand TRUE FALSE TRUE FALSE FALSE ## 2 3 Musketeers TRUE FALSE FALSE FALSE TRUE ## 3 One dime FALSE FALSE FALSE FALSE FALSE ## 4 One quarter FALSE FALSE FALSE FALSE FALSE ## 5 Air Heads FALSE TRUE FALSE FALSE FALSE ## 6 Almond Joy TRUE FALSE FALSE TRUE FALSE ## crispedricewafer hard bar pluribus sugarpercent pricepercent ## 1 TRUE FALSE TRUE FALSE 0.732 0.860 ## 2 FALSE FALSE TRUE FALSE 0.604 0.511 ## 3 FALSE FALSE FALSE FALSE 0.011 0.116 ## 4 FALSE FALSE FALSE FALSE 0.011 0.511 ## 5 FALSE FALSE FALSE FALSE 0.906 0.511 ## 6 FALSE FALSE TRUE FALSE 0.465 0.767 ## winpercent ## 1 66.97173 ## 2 67.60294 ## 3 32.26109 ## 4 46.11650 ## 5 52.34146 ## 6 50.34755 1.2 Review exercises The following exercises are meant to help you remember key concepts from Math 155 and help you get used to working with R again. All code is provided, but make sure that you understand the general structure so that you could write your own code in the future, with the aid of a reference sheet. (Suggestion: Make a code reference sheet for yourself!) So that you don’t have to copy and paste the code, download and work from the template file here. This is an RMarkdown file that you can open in RStudio. You won’t need to write new code, but you may find it helpful to add notes and comments. (Note: When you go to save this file to your computer, your browser may append a “.txt” to the end of the filename. Delete this extra file extension, and select “All Files” from the “Format” dropdown menu at the bottom of the Save As box.) You will submit your answers to these questions for Homework 1 (on Moodle). Data context: We have data on over a thousand homes in upstate New York. This data contains information on house price as well as several other physical characteristics of the house. library(ggplot2) library(dplyr) homes &lt;- read.delim(&quot;http://sites.williams.edu/rdeveaux/files/2014/09/Saratoga.txt&quot;) Before getting started on the exercises, take a minute to familiarize yourself with the data structure: # Look at the first 6 rows. What are the cases? What are the variables? head(homes) # Obtain the dimensions of the data. How many cases? How many variables? dim(homes) # Look at just the variable names colnames(homes) Visualizing the response variable Construct a visualization of house price: # Histogram ggplot(homes, aes(x = Price)) + geom_histogram() # Density plot ggplot(homes, aes(x = Price)) + geom_density() Question: How would you describe the shape of the distribution? Left-skewed Right-skewed A simple linear regression model We want to explore the relationship between house price and square footage (Living.Area variable): Price = \\(f\\)(Living.Area) + \\(\\varepsilon\\) = \\(\\beta_0\\) + \\(\\beta_1\\)Living.Area + \\(\\varepsilon\\) We can visualize the relationship with a scatterplot with an overlaid estimated linear trend line (geom_smooth(method = &quot;lm&quot;)): ggplot(homes, aes(x = Living.Area, y = Price)) + geom_point() + geom_smooth(method = &quot;lm&quot;) In R we can obtain an equation for the model line above. This equation is an estimate for the population model: Price = \\(\\hat{f}\\)(Living.Area) + \\(\\varepsilon\\) = \\(\\hat\\beta_0\\) + \\(\\hat\\beta_1\\)Living.Area + \\(\\varepsilon\\) # Fit the model mod1 &lt;- lm(Price ~ Living.Area, data = homes) # Print the summarized output from the model summary(mod1) Question: Which of the following is the correct estimated model formula? Price = 113.123 + 13439.394 Living.Area + \\(\\varepsilon\\) Price = 13439.394 + 113.123 Living.Area + \\(\\varepsilon\\) Predictions Consider a house that has a square footage of 1000 square feet. Use mod1 to predict the price of this house. (Round to the nearest dollar.) Residuals A particular house in the dataset has a square footage of 1000 and was priced at $100,000. Using your prediction from the previous exercise (rounded to the nearest dollar), calculate the residual (true value - predicted value) for this house. Adding a categorical variable In addition to Living.Area let’s consider the Fuel.Type variable which has 3 categories: fuel types 2, 3, or 4. We can visualize how fuel type relates to house price with these plots: # Adding fuel as a color to the existing scatterplot ggplot(homes, aes(x = Living.Area, y = Price, color = factor(Fuel.Type))) + geom_point(alpha = 0.2) + geom_smooth(method = &quot;lm&quot;) # Visualizing price and fuel alone ggplot(homes, aes(x = factor(Fuel.Type), y = Price)) + geom_boxplot() We can model this with the multiple linear regression model (“multiple” = multiple predictor variables): Price = \\(f\\)(Living.Area, Fuel.Type) + \\(\\varepsilon\\) Price = \\(\\beta_0\\) + \\(\\beta_1\\)Living.Area + \\(\\beta_2\\)Fuel.Type2 + \\(\\beta_3\\)Fuel.Type3 + \\(\\varepsilon\\) Note that Fuel.Type2 and Fuel.Type3 are indicator variables where, for example, Fuel.Type2 equals 1 if the house uses fuel type 2 and 0 otherwise. To fit this model in R: mod2 &lt;- lm(Price ~ Living.Area + factor(Fuel.Type), data = homes) summary(mod2) (Note: enclosing Fuel.Type within factor() forces R to treat it as a categorical variable. By default, Fuel.Type consists of the integers 2, 3, and 4.) Question: Using this model, what price would you predict for a house that uses fuel type 2 and has a square footage of 1000? (Round to the nearest dollar.) Interpreting coefficients in a multivariate model What is the interpretation of the factor(Fuel.Type)3 coefficient in mod2? The average price for a house that uses fuel type 3. The average price for a house that uses fuel type 3, holding constant square footage. The difference in average price for a house that uses fuel type 3 compared to a house that uses fuel type 2 The difference in average price for a house that uses fuel type 3 compared to a house that uses fuel type 2, holding constant square footage. Interpreting coefficients in a multivariate model What is the interpretation of the Living.Area coefficient in mod2? The average increase in price for each extra square foot The average increase in price for each extra square foot, holding constant fuel type The average increase in price for each extra square foot, only for houses that use fuel type 2 The average increase in price for each extra square foot, only for houses that use fuel type 3 The average increase in price for each extra square foot, only for houses that use fuel type 4 Statistical inference: confidence intervals In mod2 we see that the estimate of \\(\\beta_1\\) is \\(\\hat\\beta_1 = 110.231\\) and has a standard error of \\(SE(\\hat\\beta_1) = 2.784\\). Which of the following provides an approximate 95% confidence interval for \\(\\beta_1\\)? \\(110.231 \\pm 2.784 = (107.447, 113.015)\\) \\(110.231 \\pm 2\\times 2.784 = (104.663, 115.799)\\) \\(110.231 \\pm 3\\times 2.784 = (101.879, 118.583)\\) Confidence interval interpretation How can we interpret the 95% confidence interval? There’s a 95% chance that \\(\\beta_1\\) is in this interval. There’s a 95% chance that our sample would produce a 95% confidence interval that covers \\(\\beta_1\\). Confidence interval interpretation Notice that 0 is not in the 95% confidence interval for \\(\\beta_1\\). What does this tell us? We have significant evidence that house price increases as square footage increases, holding constant fuel type. We do not have significant evidence that house price increases as square footage increases, holding constant fuel type. Statistical inference: p-values The p-value in the Living.Area row corresponds to the test that: \\(H_0: \\beta_1 = 0\\) \\(H_a: \\beta_1 \\neq 0\\) What can we conclude from the p-value? We have significant evidence that house price increases as square footage increases, holding constant fuel type. We do not have significant evidence that house price increases as square footage increases, holding constant fuel type. Interpreting p-values How can we interpret the p-value? There’s a tiny chance that there’s no relationship between house price and square footage holding constant fuel type (\\(\\beta_1=0\\)). There’s a tiny chance that there’s any relationship between house price and square footage holding constant fuel type (\\(\\beta_1 \\neq 0\\)). If in fact only there were no relationship between house price and square footage holding constant fuel type, there’s a tiny chance we’d have gotten this sample of data in which there were such a strong positive relationship. Thinking about fireplaces Let’s think about both square footage (Living.Area) and about whether or not a house has any fireplaces. Below, we create a binary variable called AnyFireplaces that is TRUE if the number of fireplaces is greater than 0 and that is FALSE otherwise. homes &lt;- homes %&gt;% mutate(AnyFireplaces = Fireplaces &gt; 0) We can visualize both of these predictors as follows: ggplot(homes, aes(x = Living.Area, y = Price, color = AnyFireplaces)) + geom_point(alpha = 0.2) + geom_smooth(method = &quot;lm&quot;) Consider two models that use AnyFireplaces: mod3 &lt;- lm(Price ~ Living.Area + AnyFireplaces, data = homes) summary(mod3) mod4 &lt;- lm(Price ~ Living.Area * AnyFireplaces, data = homes) summary(mod4) Question: Consider the research question: “Is a square foot worth the same amount in a home with a fireplace as in a home without a fireplace?” Which of mod3 and mod4 can answer this question? mod3 mod4 Consider the research question: “How much is a square foot worth, holding fixed whether or not a house has a fireplace?” Which of mod3 and mod4 can answer this question? mod3 mod4 Which of mod3 and mod4 would be called an interaction model? mod3 mod4 Summarizing interaction models Using the interaction model chosen in the previous exercise, which of the following represents the relationship between Price and Living.Area for homes without fireplaces? Price = (40901.294-37610.413) + 92.364 Living.Area Price = (40901.294-37610.413) + (92.364+26.852) Living.Area Price = 40901.294 + (92.364+26.852) Living.Area Price = 40901.294 + 92.364 Living.Area Summarizing interaction models Which of the following represents the relationship between Price and Living.Area for homes with fireplaces? Price = (40901.294-37610.413) + 92.364 Living.Area Price = (40901.294-37610.413) + (92.364+26.852) Living.Area Price = 40901.294 + (92.364+26.852) Living.Area Price = 40901.294 + 92.364 Living.Area Inference in interaction models At a significance level of 0.01, what can we conclude about the relationship between price and square footage in homes without fireplaces and in homes with fireplaces in the general population? A square foot in a home with a fireplace is worth more than a square foot in a home without a fireplace. We don’t have evidence to say that a square foot in a home with a fireplace is worth more than a square foot in a home without a fireplace. Inference in interaction models What could we say about the 99% confidence interval for the interaction coefficient? It lies completely above 0. It lies completely below 0. It contains 0. "],
["regression-assumptions.html", "Topic 2 Regression Assumptions 2.1 Discussion 2.2 Exercises", " Topic 2 Regression Assumptions 2.1 Discussion Regression tasks In regression tasks, our goal is to build a model (a function) \\(f\\) that predicts the quantitative response values well: \\[y = f(x) + \\varepsilon\\] \\(y\\): quantitative response variable \\(x = (x_1, x_2, \\ldots, x_p)\\): are \\(p\\) explanatory variables, predictors, features \\(f(x)\\) is a function that captures the underlying trend/relationship between the response and the predictors \\(\\varepsilon\\) is random unexplainable error/noise The linear regression model you learned about in MATH 155 is a special case of \\(f\\): \\[ \\begin{align*} y &amp;= f(x) + \\varepsilon \\\\ &amp;= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p + \\varepsilon \\end{align*} \\] Evaluating regression models Should meet assumptions required for statistical inference Should explain a substantial proportion of the variation in the response Should produce accurate predictions Linear regression assumptions \\[\\varepsilon \\stackrel{ind}{\\sim} N(0, \\sigma^2)\\] Assumptions are to ensure that statistical inference procedures (p-values, confidence intervals) “work as advertised”: The process of creating a 95% CI is a procedure (add and subtract about 2 standard errors from the estimate). It “works as advertised” if in 95% of possible samples it creates intervals that contain the true population value. (The other 5% of samples are unlucky ones.) It does not “work as advertised” if only 90% of possible samples result in intervals (95% CIs) that contain the true population value. 2.2 Exercises You can download a template RMarkdown file to start from here. We will continue looking at the New York housing data. Each case (row) in the dataset is a house in New York, and on each house, we have information on several variables. We’ll focus on the response variable Price (in dollars) and a single predictor variable Age (in years). # Load required packages library(ggplot2) library(dplyr) # Read in the data homes &lt;- read.delim(&quot;http://sites.williams.edu/rdeveaux/files/2014/09/Saratoga.txt&quot;) # Look at the first 6 rows head(homes) ## Price Lot.Size Waterfront Age Land.Value New.Construct Central.Air ## 1 132500 0.09 0 42 50000 0 0 ## 2 181115 0.92 0 0 22300 0 0 ## 3 109000 0.19 0 133 7300 0 0 ## 4 155000 0.41 0 13 18700 0 0 ## 5 86060 0.11 0 0 15000 1 1 ## 6 120000 0.68 0 31 14000 0 0 ## Fuel.Type Heat.Type Sewer.Type Living.Area Pct.College Bedrooms ## 1 3 4 2 906 35 2 ## 2 2 3 2 1953 51 3 ## 3 2 3 3 1944 51 4 ## 4 2 2 2 1944 51 3 ## 5 2 2 3 840 51 2 ## 6 2 2 2 1152 22 4 ## Fireplaces Bathrooms Rooms ## 1 1 1.0 5 ## 2 0 2.5 6 ## 3 1 1.0 8 ## 4 1 1.5 5 ## 5 0 1.0 3 ## 6 1 1.0 8 We will pretend that the homes dataset contains the full population of New York houses. Let’s draw a random sample of 500 houses from the “population”. We can do this with the sample_n() function available in the dplyr package: # Randomly sample 500 homes homes_sample &lt;- sample_n(homes, size = 500) Checking the independence assumption In thinking about what the cases are, do you think the independence assumption holds? A first try at a model Visualize the relationship between house price and house age with a scatterplot and smooth trend line. How would you describe the overall shape of the trend? Is it linear? ggplot(homes_sample, aes(x = ???, y = ???)) + geom_???() + geom_smooth() Using our sample (homes_sample), fit a linear regression model where Price is predicted by Age: # Fit the model mod1 &lt;- lm(Price ~ Age, data = homes_sample) # Display the summary table summary(mod1) Check the trend and homoskedasticity assumptions by plotting the residuals versus the fitted (predicted) values. The points should be evenly scattered around the y = 0 line. Do you think these assumptions are met? # Put the residuals and predicted values into a dataset mod1_output &lt;- data.frame( residual = residuals(mod1), predicted = fitted.values(mod1) ) # Plot ggplot(mod1_output, aes(x = ???, y = ???)) + geom_point() + geom_smooth(color = &quot;blue&quot;, lwd = 3) + # Add a smooth trend geom_hline(yintercept = 0, color = &quot;red&quot;) # Add the y = 0 line Check the normality assumption by making a QQ-plot of the residuals. In a QQ-plot, each residual (y-axis) is plotted against its theoretical corresponding value from a standard normal distribution (\\(N(0,1^2)\\)) on the x-axis. That is, the first quartile of the residuals is plotted against the first quartile of \\(N(0,1^2)\\), the median of the residuals is plotted against the median of \\(N(0,1^2)\\), and so on. If the residuals follow a normal distribution, then the points should fall on a line. Do you think the normality assumption holds? ggplot(mod1_output, aes(sample = residual)) + geom_qq() + geom_qq_line() A second model The diagnostic plots we made above suggest that key assumptions are not being met. Let’s explore how transforming variables can help us meet those assumptions. One of the most common variable transformations that can help fix an unmet homoskedasticity assumption is a logarithmic transformation of the response variable. We will also try to better model the nonlinear shape of the Price vs. Age trend by using a logarithmic transformation. The mutate() function in the dplyr package allows us to create these new transformed variables: # log() = natural logarithm # The %&gt;% is a &quot;pipe&quot;: it takes the dataset to the left and provides it as input to the code that follows homes_sample &lt;- homes_sample %&gt;% mutate( log_price = log(Price), log_age = log(Age + 1) # Some Age&#39;s are 0, so add 1 to prevent log(0), which is undefined ) Fit a linear regression model where log_price is predicted by log_age and obtain the residuals and fitted values: mod2 &lt;- lm(???, data = homes_sample) mod2_output &lt;- data.frame( residual = residuals(???), predicted = fitted.values(???) ) Check the trend, homoskedasticity, and normality assumptions for mod2. Do these assumptions seem to hold better for mod1 or mod2? Implications for statistical inference Since we have the entire population of New York homes, we can investigate whether or not confidence intervals “work as advertised” for the two models we investigated. Display the 95% confidence intervals for the coefficients in mod1 and mod2 with the confint() function: confint(mod1) confint(mod2) By fitting the Price ~ Age model in the full population, we know that the true population value of the Age coefficient is -636.2551305. And by fitting the log_price ~ log_age model in the full population, we know that the true population value of the log_age coefficient is -0.134966. Does the 95% confidence interval “work as advertised” in this case? But what we did in part a just looked at one sample. If the 95% CI truly were working as advertised, 95% of samples would create 95% CIs that contain the true population value. We can run some simulations and see how 95% CIs “work” in 1000 different samples. We find that for mod1, 95% CIs contain the true value of the Age coefficient 899 times. We find that for mod2, 95% CIs contain the true value of the log_age coefficient 964 times. With regards to statistical inference, what can you conclude about assumption violations and fixing those violations? "],
["accuracy-metrics-for-regression.html", "Topic 3 Accuracy Metrics for Regression 3.1 Discussion 3.2 Exercises", " Topic 3 Accuracy Metrics for Regression 3.1 Discussion Evaluating regression models Should meet assumptions required for statistical inference Should explain a substantial proportion of the variation in the response Should produce accurate predictions For both of these points, we can look at residuals. Sum of squared residuals \\[RSS = \\sum_{i=1}^n (y_i - \\hat{y_i})^2 = (y_1 - \\hat{y}_1)^2 + (y_2 - \\hat{y}_2)^2 + \\cdots + (y_n - \\hat{y}_n)^2\\] The sum (and mean) of the residuals is always zero when an intercept is included in the linear regression model -&gt; add up the squared residuals Not very interpretable Due to missing values in predictors, sample size can vary from analysis to analysis (hard to compare RSS) Mean squared error \\[MSE = \\frac{RSS}{n} = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\] More interpretable than RSS: on average how far are our predictions from the true values (in squared distance)? Interpretation downside: the units are squared units Square root of MSE (RMSE = root mean squared error) is often used: \\(RMSE = \\sqrt{MSE}\\) It’s tempting to try to interpret RMSE as the average distance of our predictions from the true values because the units align with the response variable, but it’s not technically quite right due to the square root. Mean absolute error \\[MAE = \\frac{1}{n}\\sum_{i=1}^n \\|y_i - \\hat{y}_i\\|\\] Where \\(\\|y_i - \\hat{y}_i\\|\\) indicates the absolute value of the residual Very interpretable: on average how far are our predictions from the true values R-squared Define the total sum of squares (\\(TSS\\)) as the sum of squared deviations of each response \\(y_i\\) from the mean response \\(\\bar{y}\\): \\[TSS = \\sum_{i=1}^n (y_i - \\bar{y})^2\\] \\[R^2 = 1-\\frac{RSS}{TSS} = \\frac{\\text{Var(fitted)}}{\\text{Var(response)}}\\] Very interpretable: the proportion of variation in the response that is explained by the model Problems with R-squared and MSE R-squared automatically increases with added predictors (even useless ones) MSE automatically decreases with added predictors (even useless ones) Example below: dataset with 20 cases. Random numbers are used as predictors. Alternative metrics: Instead of R-squared, use adjusted R-squared Instead of MSE, we’ll use cross-validation (coming up next) Overfitting The example above is a demonstration of overfitting. With more and more predictors, greater chance that some are useless. Including useless predictors in a model is like reading too much into the noise. With overfitting, models don’t tend to generalize well. 3.2 Exercises You can download a template RMarkdown file to start from here. You’ll be working a dataset containing physical measurements on 80 adult males. These measurements include body fat percentage estimates as well as body circumference measurements. fatBrozek: Percent body fat using Brozek’s equation: 457/Density - 414.2 fatSiri: Percent body fat using Siri’s equation: 495/Density - 450 density: Density determined from underwater weighing (gm/cm^3). age: Age (years) weight: Weight (lbs) height: Height (inches) neck: Neck circumference (cm) chest: Chest circumference (cm) abdomen: Abdomen circumference (cm) hip: Hip circumference (cm) thigh: Thigh circumference (cm) knee: Knee circumference (cm) ankle: Ankle circumference (cm) biceps: Biceps (extended) circumference (cm) forearm: Forearm circumference (cm) wrist: Wrist circumference (cm) It takes a lot of effort to estimate body fat percentage accurately through underwater weighing. The goal is to build the best predictive model for fatSiri using just circumference measurements, which are more easily attainable. (Don’t use fatBrozek or density as predictors.) library(ggplot2) library(dplyr) bodyfat &lt;- read.csv(&quot;https://www.dropbox.com/s/js2gxnazybokbzh/bodyfat_train.csv?dl=1&quot;) # Remove the fatBrozek and density variables bodyfat &lt;- bodyfat %&gt;% select(-fatBrozek, -density) Using tools from Math 155 and 253 (e.g. exploratory plots, p-values, confidence intervals, adjusted R-squared), experiment with different models to try to build the best predictive model possible. What are the adjusted R-squared and MSE for this model? Code notes: if you want to extract the adjusted R-squared from a fitted model, you can use the following. your_model &lt;- lm(fatSiri ~ age, data = bodyfat) summary(your_model)$adj.r.squared And if you want to compute MSE, you can use the function below: mse &lt;- function(mod) { mean(residuals(mod)^2) } mse(your_model) Now that you’ve selected your best model, deploy it in the real world by applying it to a new set of 172 adult males. The predict() function allows you to supply a fitted model and a new dataset of predictors (the newdata argument). bodyfat_test &lt;- read.csv(&quot;https://www.dropbox.com/s/7gizws208u0oywq/bodyfat_test.csv?dl=1&quot;) # Predict test_predictions &lt;- predict(your_model, newdata = bodyfat_test) # Compute MSE # The $ extracts a particular column from a dataset mean((bodyfat_test$fatSiri - test_predictions)^2) Thinking about main themes How did your MSE on the original dataset of 80 males compare to the MSE on the new data of 172 males? What conclusions can you draw from this exploration in relation to overfitting? Thinking more about overfitting Do you think that a model with more predictors or less predictors is more prone to overfitting? Why? The method used to find the coefficients in linear regression is called the least squares method. We find coefficients \\(\\hat{\\beta}_1, \\hat{\\beta}_2, \\ldots, \\hat{\\beta}_p\\) that minimize the sum of squared residuals \\(RSS\\). Given your answer in part a, can you think of a way to modify the least squares criterion to penalize weak predictors being included in a model? That is, can you brainstorm a possible penalty to add below? Least squares criterion: find \\(\\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p\\) that minimize \\(RSS\\) Penalized least squares criterion: find \\(\\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p\\) that minimize \\(RSS + \\text{penalty}\\) Suggestion: Draw inspiration from the “penalty” term in the adjusted R-squared formula from the video. Extra! If you have time and are interested in learning about writing R functions, try the following. Using the mse() function above as a guide, write a function that compute the MSE of a model on new data. What inputs do you need? These must be supplied as arguments to the function. These are given in the parentheses. You can take multiple intermediate steps within the function. This is often recommended for multi-step tasks because it makes the code easier to read. Annotate the steps of your function with comments. (Start a comment line with a #.) "],
["cross-validation.html", "Topic 4 Cross-Validation 4.1 Discussion 4.2 Exercises", " Topic 4 Cross-Validation 4.1 Discussion Overfitting From ISLR, page 32: When a given method yields a small training MSE but a large test MSE, we are said to be overfitting the data. This happens because our statistical learning procedure is working too hard to find patterns in the training data, and may be picking up some patterns that are just caused by random chance rather than by true [trends]. When we overfit the training data, the test MSE will be very large because the supposed patterns that the method found in the training data simply don’t exist in the test data. Note that regardless of whether or not overfitting has occurred, we almost always expect the training MSE to be smaller than the test MSE because most statistical learning methods either directly or indirectly seek to minimize the training MSE. Overfitting refers specifically to the case in which a less flexible model would have yielded a smaller test MSE. “Flexibility” in linear regression refers to the number of coefficients More coefficients (more predictors) = more flexible Fewer coefficients (fewer predictors) = less flexible How do we prevent overfitting? Want an accuracy metric that allows us to choose which of several models will be most accurate on new data Adjusted R-squared is a good idea but restricted to linear regression Cross-validation is a much more general technique that allows estimation of the true error rate on new data (the test error) Use specific statistical learning methods suited to discourage including weak/useless predictors Shrinkage methods (coming soon!) 4.2 Exercises Goals For what purposes would you use cross-validation? How is cross-validation useful for preventing overfitting? You can download a template RMarkdown file to start from here. You’ll continue working on the body fat dataset from last time. The following variables were recorded. fatBrozek: Percent body fat using Brozek’s equation: 457/Density - 414.2 fatSiri: Percent body fat using Siri’s equation: 495/Density - 450 density: Density determined from underwater weighing (gm/cm^3). age: Age (years) weight: Weight (lbs) height: Height (inches) neck: Neck circumference (cm) chest: Chest circumference (cm) abdomen: Abdomen circumference (cm) hip: Hip circumference (cm) thigh: Thigh circumference (cm) knee: Knee circumference (cm) ankle: Ankle circumference (cm) biceps: Biceps (extended) circumference (cm) forearm: Forearm circumference (cm) wrist: Wrist circumference (cm) The focus is on predicting body fat percentage using Siri’s equation (fatSiri) from easily measured variables: age, weight, height, and circumferences. library(ggplot2) library(dplyr) bodyfat_train &lt;- read.csv(&quot;https://www.dropbox.com/s/js2gxnazybokbzh/bodyfat_train.csv?dl=1&quot;) # Remove the fatBrozek and density variables bodyfat_train &lt;- bodyfat_train %&gt;% select(-fatBrozek, -density) 4 models Consider the 4 models below: mod1 &lt;- lm(fatSiri ~ age+weight+neck+abdomen+thigh+forearm, data = bodyfat_train) mod2 &lt;- lm(fatSiri ~ age+weight+neck+abdomen+thigh+biceps+forearm, data = bodyfat_train) mod3 &lt;- lm(fatSiri ~ age+weight+neck+chest+abdomen+hip+thigh+biceps+forearm, data = bodyfat_train) mod4 &lt;- lm(fatSiri ~ ., data = bodyfat_train) # The . means all predictors Before looking at the summary tables, predict: Which model will have the highest R squared? Which model will have the lowest training MSE? Find/compute the R squared and MSE for the 4 models to check your answers in part a. Which model do you think will perform worst on new test data? Why? We’ll use the caret package to perform cross-validation (and to run many different machine learning methods throughout the course). The caret package is a great resource for the machine learning community because it aggregates machine learning methods written by tons of different authors in tons of different R packages into one single package. The advantage is that instead of learning 10 different styles of code for 10 different machine learning methods, we can use fairly similar code throughout the course. Install the caret package by running install.packages(&quot;caret&quot;) in the Console. Cross-validation with the caret package Use the code below to perform 10-fold cross validation for mod1 to estimate the test MSE (\\(\\text{CV}_{(10)}\\)). # Load the package library(caret) # Set up what type of cross-validation is desired train_ctrl_cv10 &lt;- trainControl(method = &quot;cv&quot;, number = 10) # To ensure the same results each time the code is run set.seed(253) # Fit (train) the model as written in mod1 # Also supply information about the type of CV desired for evaluating the model with the trControl argument # na.action = na.omit prevents errors if the data has missing values mod1_cv10 &lt;- train( fatSiri ~ age+weight+neck+abdomen+thigh+forearm, data = bodyfat_train, method = &quot;lm&quot;, trControl = train_ctrl_cv10, na.action = na.omit ) # The $ extracts components of an object # Peek at the &quot;resample&quot; part of mod1_cv10 - what info does it contain? mod1_cv10$resample # Estimate of test MSE # RMSE = square root of MSE mean(mod1_cv10$resample$RMSE^2) Adapt the code above to perform: 10-fold CV for model 2 LOOCV for model 1 In doing so, look carefully at the structure of the code. What parts need to be repeated? What parts don’t? (Hint: nrow(dataset) gives the number of cases in a dataset.) Looking at the evaluation metrics A completed table of evaluation metrics is below. Which model performed the best on the training data? Which model performed best on the test set? Which model would be preferred using \\(\\text{CV}_{(10)}\\) or LOOCV estimates of the test error? How is cross-validation helping us avoid overfitting? Model \\(R^2\\) Training MSE \\(\\text{CV}_{(10)}\\) LOOCV Test set MSE mod1 0.8103 14.52153 17.21062 18.16816 23.92333 mod2 0.8146 14.18762 19.64114 19.29848 23.90547 mod3 0.816 14.08022 21.24115 20.28958 23.63958 mod4 0.8162 14.06917 21.88440 21.26073 24.65370 Practical issues: choosing \\(k\\) What do you think are the pros/cons of low vs. high \\(k\\)? If possible, it is advisable to choose \\(k\\) to be a divisor of the sample size. Why do you think that is? Extra! Writing R functions If you’re interested in learning about writing R functions, look at the following function that the instructor used to fill out the above evaluation metrics table. bodyfat_test &lt;- read.csv(&quot;https://www.dropbox.com/s/7gizws208u0oywq/bodyfat_test.csv?dl=1&quot;) evaluate_model &lt;- function(formula) { train_ctrl_cv10 &lt;- trainControl(method = &quot;cv&quot;, number = 10) train_ctrl_loocv &lt;- trainControl(method = &quot;cv&quot;, number = nrow(bodyfat_train)) mod_cv10 &lt;- train(formula, data = bodyfat_train, method = &quot;lm&quot;, trControl = train_ctrl_cv10, na.action = na.omit) mod_loocv &lt;- train(formula, data = bodyfat_train, method = &quot;lm&quot;, trControl = train_ctrl_loocv, na.action = na.omit) model_predictions &lt;- predict(mod_cv10, newdata = bodyfat_test) test_mse &lt;- mean((bodyfat_test$fatSiri - model_predictions)^2) c( cv10 = mean(mod_cv10$resample$RMSE^2), loocv = mean(mod_loocv$resample$RMSE^2), test = test_mse ) } set.seed(253) evaluate_model(fatSiri ~ age+weight+neck+abdomen+thigh+forearm) evaluate_model(fatSiri ~ age+weight+neck+abdomen+thigh+biceps+forearm) evaluate_model(fatSiri ~ age+weight+neck+chest+abdomen+hip+thigh+biceps+forearm) evaluate_model(fatSiri ~ .) Step through each line and see if you can understand the structure. How would you modify the function to work on arbitrary data? How would you have to change the function arguments (within the parentheses on the first line)? How would you modify the function to allow the user to choose \\(k\\)? "],
["subset-selection.html", "Topic 5 Subset Selection 5.1 Discussion 5.2 Exercises", " Topic 5 Subset Selection 5.1 Discussion Subset selection methods Automated methods that take different strategies for exploring subsets of the predictors Stepwise selection methods: add or remove variables one at a time Best subset selection: brute force method that tries all possible subsets of predictors 5.2 Exercises You can download a template RMarkdown file to start from here. We’ll continue using the body fat dataset to explore subset selection methods. library(ggplot2) library(dplyr) bodyfat &lt;- read.csv(&quot;http://www.macalester.edu/~ajohns24/data/bodyfatsub.csv&quot;) # Take out the redundant Density and HeightFt variables bodyfat &lt;- bodyfat %&gt;% select(-Density, -HeightFt) Backward stepwise selection: by hand In the backward stepwise procedure, we start with the full model, full_model, with all predictors: full_model &lt;- lm(BodyFat ~ Age + Weight + Height + Neck + Chest + Abdomen + Hip + Thigh + Knee + Ankle + Biceps + Forearm + Wrist, data = bodyfat) Then… Identify which predictor contributes the least to the model. One approach is to identify the least significant predictor. Fit a new model which eliminates this predictor. Identify the least significant predictor in this model. Fit a new model which eliminates this predictor. Repeat 1 more time to get the hang of it. Interpreting the results Examine which predictors remain after the previous exercise. Are you surprised that, for example, Wrist is still in the model but Weight is not? Does this mean that Wrist is a better predictor of body fat percentage than Weight is? Forward selection is another stepwise technique. Can you guess how this differs from backward selection? Best subset selection is another subset selection technique that looks at every possible subset of predictors, fits all of these models, and picks the best one. From the perspective of computational time, why is this not a preferable approach? Planning backward selection using CV Using p-values to perform backward selection by hand is convenient but not the most direct way to target predictive accuracy. Outline the steps that you would take to use cross-validation to perform backward selection. (Write an algorithm in words.) Backward stepwise selection in caret We can use the caret package to perform backward stepwise selection with cross-validation as shown below. (Note: CV is only used to pick among the best 1, 2, 3, …, and 13 variable models. To find the best 1, 2, 3, …, and 13 variable models, training MSE is used. caret uses training MSE because within a subset size, all models have the same number of coefficients, which makes both ordinary R-squared and training MSE ok.) Just focus on the structure of the code and how different parts of the output are used. Is there a use case that you are interested in but don’t see below? Feel free to ask the instructor about it! library(caret) # Set up cross-validation information train_ctrl_cv10 &lt;- trainControl(method = &quot;cv&quot;, number = 10) # Perform backward stepwise selection # The first time you run this, you&#39;ll be prompted to install the &quot;leaps&quot; package set.seed(253) back_step &lt;- train( BodyFat ~ ., data = bodyfat, method = &quot;leapBackward&quot;, tuneGrid = data.frame(nvmax = 1:13), trControl = train_ctrl_cv10, metric = &quot;RMSE&quot;, na.action = na.omit ) # Look at accuracy/error metrics for the different subset sizes back_step$results # What tuning parameter gave the best performance? # i.e. What subset size gave the best model? back_step$bestTune # Obtain the coefficients for the best model coefficients(back_step$finalModel, id = back_step$bestTune$nvmax) # Use the best model to make predictions on new data predict(back_step, newdata = bodyfat) Some notes about the code: The BodyFat ~ . formula tells R that BodyFat is the response and that all predictors (specified with the .) will be considered. The tuneGrid argument allows us to input tuning parameters into the fitting process. The tuning parameters here are the number of variables included in the models (nvmax). This can vary from 1 to 13 (the maximum number of predictors possible). The metric argument indicates how the best of the 1-variable, 2-variable, etc. models will be chosen. We’ll use RMSE (root mean squared error). When you look at back_step$results, you’ll see a matrix of output. The rows correspond to the different subset sizes. For each subset size you’ll see the RMSE, \\(R^2\\), and MAE accuracy/error metrics. Recall that these are estimates that arise by taking the mean of the values given in the 10 CV iterations. The 10 values from the 10 iterations also have a standard deviation. These are reported in the last 3 columns. What use might the standard deviation have in picking a final model? "],
["shrinkageregularization.html", "Topic 6 Shrinkage/Regularization 6.1 Discussion 6.2 Exercises", " Topic 6 Shrinkage/Regularization 6.1 Discussion Exploring doing LASSO by hand I took a subset of the Hitters dataset to focus on only the Walks and Assists variables. I made this penalized_rss() function to compute the penalized sum of squared residuals given a value for the lambda penalty and guesses for beta1 (for Walks) and for beta2 (Assists). penalized_rss &lt;- function(lambda, beta1, beta2) { # Predict salary using beta1, beta2 and predictor info pred &lt;- hitters_subs$Walks*beta1 + hitters_subs$Assists*beta2 # Compute residuals resid &lt;- hitters_subs$Salary - pred # Compute RSS rss &lt;- sum(resid^2, na.rm = TRUE) # Compute penalized RSS prss &lt;- rss + lambda*(abs(beta1) + abs(beta2)) prss } I wanted to compute the penalized RSS for many different combinations of lambda, beta1, and beta2. Here are some of those combinations: ## lambda beta1 beta2 ## 1e+03 1.4 7.2 ## 1e+02 3.2 4.4 ## 0e+00 2.0 2.0 ## 1e+05 1.0 7.8 ## 1e+03 1.6 5.2 ## 1e+02 2.8 0.4 I then actually computed the penalized RSS for these combinations using the penalized_rss() function. ## lambda beta1 beta2 pen_rss ## 1e+03 1.4 7.2 53150907 ## 1e+02 3.2 4.4 52964196 ## 0e+00 2.0 2.0 53098255 ## 1e+05 1.0 7.8 54062874 ## 1e+03 1.6 5.2 53133494 ## 1e+02 2.8 0.4 53024238 For each lambda value that I tried, I manually fit a LASSO model by finding the beta1 and beta2 values that minimized the penalized RSS. These are shown here: ## # A tibble: 6 x 4 ## lambda beta1 beta2 pen_rss ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 10 9.8 52264853. ## 2 100 10 9.6 52266813. ## 3 1000 10 8 52283711. ## 4 10000 10 0 52392761. ## 5 100000 10 0 53292761. ## 6 1000000 0 0 53319113. Thought exercise: How do the results above demonstrate the shrinkage property of LASSO? 6.2 Exercises You can download a template RMarkdown file to start from here. We’ll explore LASSO modeling using the Hitters dataset in the ISLR package (associated with the optional textbook). You’ll need to install the ISLR package in the Console first. You should also install the glmnet package as we’ll be using it subsequently for fitting LASSO models. install.packages(c(&quot;ISLR&quot;, &quot;glmnet&quot;)) # Load the data library(ISLR) data(Hitters) # Examine the data codebook ?Hitters The Hitters dataset contains a number of stats on major league baseball players in 1987. Our goal will be to build a regression model that predicts player Salary. Get to know the Hitters data Peek at the first few rows. How many players are in the dataset? How many possible predictors of salary are there? Developing some intuition A natural model to start with is one with all possible predictors. The following model is fit with ordinary (not penalized) least squares: least_squares_mod &lt;- lm(Salary ~ ., data = Hitters) coefficients(least_squares_mod) Use caret to perform 7-fold cross-validation to estimate the test error of this model. Use the straight average of the RMSE column instead of squaring the values first. (Why 7? Think about the number of cases in the folds.) How do you think the estimated test error would change with fewer predictors? Briefly describe how the output of a stepwise selection procedure could help you choose a smaller model (a model with fewer predictors). This model fit with ordinary least squares corresponds to a special case of penalized least squares. What is the value of \\(\\lambda\\) in this special case? As \\(\\lambda\\) increases, what would you expect to happen to the number of predictors that remain in the model? LASSO for specific \\(\\lambda\\) The code below fits a LASSO model with \\(\\lambda = 10\\). This value of \\(\\lambda\\) is specified in the tuneGrid argument. The alpha = 1 specifies the LASSO method specifically (the glmnet method has other purposes). set.seed(74) lasso_mod_lambda10 &lt;- train( Salary ~ ., data = Hitters, method = &quot;glmnet&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 7), tuneGrid = data.frame(alpha = 1, lambda = 10), metric = &quot;RMSE&quot;, na.action = na.omit ) # Model coefficients for lambda = 10 # The .&#39;s indicate that the coefficient is 0 coefficients(lasso_mod_lambda10$finalModel, 10) How many variables remain in the LASSO model with \\(\\lambda=10\\)? How do their coefficients compare to the corresponding variables in the least squares model? Fit the LASSO using \\(\\lambda=100\\). set.seed(74) lasso_mod_lambda100 &lt;- train( Salary ~ ., data = Hitters, method = &quot;glmnet&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 7), tuneGrid = data.frame(alpha = 1, lambda = 100), metric = &quot;RMSE&quot;, na.action = na.omit ) # Model coefficients for lambda = 100 coefficients(lasso_mod_lambda100$finalModel, 100) How many variables remain in the LASSO model with \\(\\lambda=100\\)? Is this model “bigger” or smaller than the LASSO with \\(\\lambda=10\\)? How do the variables’ coefficients compare to the corresponding variables in the least squares model and the LASSO with \\(\\lambda=10\\)? LASSO for a variety of \\(\\lambda\\) There are infinitely many \\(\\lambda\\) we could use. It would be too tedious to examine these one at a time. The following code fits LASSO models across a grid of \\(\\lambda\\) values and makes a summary plot of the coefficient estimates as a function of \\(\\lambda\\). # Create a grid of lambda values lambdas &lt;- 10^seq(-3, 3, length.out = 100) # Fit LASSO models for all of the lambdas set.seed(74) lasso_mod &lt;- train( Salary ~ ., data = Hitters, method = &quot;glmnet&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 7), tuneGrid = data.frame(alpha = 1, lambda = lambdas), metric = &quot;RMSE&quot;, na.action = na.omit ) # Plot summary of coefficient estimates plot(lasso_mod$finalModel, xvar = &quot;lambda&quot;, label = TRUE, col = rainbow(20)) # What variables do the numbers correspond to? rownames(lasso_mod$finalModel$beta) There’s a lot of information in this plot! Each colored line corresponds to a different predictor. The small number to the left of each line indicates a predictor by its position in rownames(lasso_mod$finalModel$beta). The x-axis reflects the range of different \\(\\lambda\\) values considered in lasso_mod (the lambdas vector that we created). At each \\(\\lambda\\), the y-axis reflects the coefficient estimates for the predictors in the corresponding LASSO model. At each \\(\\lambda\\), the numbers at the top of the plot indicate how many predictors remain in the corresponding model. Very roughly eyeball the coefficient estimates when \\(log(\\lambda) = -2\\). Do they look like they correspond to the coefficient estimates from ordinary least squares in exercise 2? Why do all of the lines head toward y = 0 on the far right of the plot? We can zoom in on the plot by setting the y-axis limits to go from -10 to 10 with ylim as below. Compare the lines for variables 6 and 15. What are variables 6 and 15? Which seems to be a more “important” or “persistent” variable? Does this make sense in context? # Zoom in plot(lasso_mod$finalModel, xvar = &quot;lambda&quot;, label = TRUE, col = rainbow(20), ylim = c(-10,10)) # What is variable 6? rownames(lasso_mod$finalModel$beta)[6] Picking \\(\\lambda\\) In order to pick which \\(\\lambda\\) (hence LASSO model) is “best”, we can compare the 7-fold CV error rate for each model. caret has actually done that for us when it train()ed the model. We can look at a plot of those results: # Plot a summary of the performance of the different models plot(lasso_mod) This figure plots cross-validation estimates of the RMSE (y-axis) versus value of \\(\\lambda\\) (regularization parameter). Comment on the shape of the plot. The RMSE’s go down at the very beginning then start going back up. Why do you think that is? Roughly, what value of \\(\\lambda\\) results in the best model? This plot indicates that we tried many \\(\\lambda\\) values that were pretty bad. (Why?) Let’s fit LASSO models over a better grid of \\(\\lambda\\) values. Modify the previous code to use the following grid and remake lasso_mod and the previous plot: lambdas &lt;- seq(0, 50, length.out = 100) Picking \\(\\lambda\\): accounting for uncertainty Each of the points on the previous plot arose from taking the mean RMSE over 7 cross-validation iterations. Those 7 RMSE estimates have a standard deviation and standard error too. You can use the custom best_lambdas() function to make a plot of estimated test RMSE versus \\(\\lambda\\) that also shows information about the standard errors. In particular, the plot shows points that exactly correspond to the previous plot. The additional lines show 1 standard error above and below the RMSE estimate. In essence, the span of the lines indicates a confidence interval. The best_lambdas() function also prints information about some reasonable choices for good \\(\\lambda\\) values. best_lambdas &lt;- function(model) { # Extract the results table res &lt;- model$results # Extract the K in K-fold CV k &lt;- model$control$number # Compute the standard error (SE) of the RMSE estimate res$rmse_se &lt;- res$RMSESD/sqrt(k) # Which lambda resulted in the lowest RMSE? index_lambda_min &lt;- which.min(res$RMSE) lambda_min &lt;- res$lambda[index_lambda_min] # Compute 1 SE below and above the minimum RMSE res$rmse_lower &lt;- res$RMSE - res$rmse_se res$rmse_upper &lt;- res$RMSE + res$rmse_se rmse_lower &lt;- res$RMSE[index_lambda_min] - res$rmse_se[index_lambda_min] rmse_upper &lt;- res$RMSE[index_lambda_min] + res$rmse_se[index_lambda_min] res$within_1se &lt;- res$RMSE &gt;= rmse_lower &amp; res$RMSE &lt;= rmse_upper index_lambda_1se &lt;- max(which(res$within_1se)) lambda_1se &lt;- res$lambda[index_lambda_1se] p &lt;- ggplot(res, aes(x = lambda, y = RMSE)) + geom_pointrange(aes(ymin = rmse_lower, ymax = rmse_upper)) print(p) output &lt;- res[c(index_lambda_min, index_lambda_1se),c(&quot;lambda&quot;, &quot;RMSE&quot;)] rownames(output) &lt;- c(&quot;lambda_min&quot;, &quot;lambda_1se&quot;) output } lambda_choices &lt;- best_lambdas(lasso_mod) lambda_choices The first row of printed output shows a choice for \\(\\lambda\\) called lambda_min, the \\(\\lambda\\) at which the observed CV error was smallest. The second row shows a choice called lambda_1se, the largest \\(\\lambda\\) for which the corresponding LASSO model has a CV error that’s still within 1 standard error of that for the LASSO using lambda_min. Explain why we might use the LASSO with lambda_1se instead of lambda_min. How does the CV-estimated RMSE of these models compare to that of the original ordinary least squares model in exercise 2? Look at the coefficients of LASSO models corresponding to both choices of \\(\\lambda\\). How do the coefficients differ between lambda_min and lambda_1se? Does one model’s coefficients seem more sensible contextually? The instructor does not have a deep enough understanding of baseball, but you might! # Coefficients for the lambda_min LASSO model coefficients(lasso_mod$finalModel, lambda_choices[&quot;lambda_min&quot;, &quot;lambda&quot;]) # Coefficients for the lambda_1se LASSO model coefficients(lasso_mod$finalModel, lambda_choices[&quot;lambda_1se&quot;, &quot;lambda&quot;]) "],
["knn-bias-variance-tradeoff.html", "Topic 7 KNN &amp; Bias-Variance Tradeoff 7.1 Discussion 7.2 Exercises", " Topic 7 KNN &amp; Bias-Variance Tradeoff 7.1 Discussion So far our models have all looked like: \\[ y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip} + \\varepsilon \\] This is kind of a rigid assumption…can we build more flexible models? Flexibility of a method/model Recall that in regression tasks, the goal is to find a good function \\(f(x_1,\\ldots,x_p)\\) that maps predictors \\(x_1,\\ldots,x_p\\) to responses \\(Y\\) A model has higher flexibility if it is capable of generating more possibilities for \\(f(X)\\). e.g. For linear regression, more predictors makes for more flexible models There are some possibilities for \\(f(x_1,\\ldots,x_p) = \\beta_0 + \\beta_1 x_1\\). There are more possibilities for \\(f(x_1,\\ldots,x_p) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\). There are even more possibilities for \\(f(x_1,\\ldots,x_p) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3\\). Parametric vs. nonparametric methods A way to get more flexible models is with nonparametric approaches. Parametric models have a particular form determined by parameters. e.g. Linear regression models are determined by the \\(\\beta\\)’s. (The \\(\\beta\\) coefficients are parameters.) Nonparametric models try to avoid assuming that the relationship between response and predictors looks a very particular way. e.g. KNN regression tries to predict responses based on cases that have similar predictor values. We’ll also look at generalized additive models and tree methods. The bias-variance tradeoff Who wouldn’t want a flexible method? Isn’t flexibility a good thing? The test error of a model has both a bias and a variance component. Low bias: my model is not overly simplified Low variance: my model would not change very much with new training data When we improve one, we hurt the other… …the hope is that we do this in a way that decreases test error overall Usually a model with a little more bias has way less variance. 7.2 Exercises No R today. The focus of these exercises is entirely on conceptual understanding. Bias-variance tradeoff &lt;-------------------------------------------------------------------&gt; less flexible more flexible Draw the diagram shown above for yourself. Add to it annotations of: (more/less) bias, variance, complexity (Challenge yourself to not look at the video notes.) Where would you put ordinary least squares linear regression with all predictors (OLS), LASSO, stepwise selection, best subset selection, and KNN regression on this diagram? (For some of these methods, it might be best to do part c at the same time.) All of the above methods have tuning parameters which affect the final model that results. (For stepwise, and best subset, the number of variables included in the model can be thought of as the tuning parameter.) Add to the diagram information about low/high values of the tuning parameter. Think back to our LASSO exercises where we plotted test RMSE versus \\(\\lambda\\). This plot had a “U”-shape in which test error started high, decreased, then increased again. How is this related to the idea of “usually a model with a little more bias has way less variance”? Extending KNN Can you think of instances of where KNN algorithms come up in your day to day life? We’ve talked about KNN for regression. How might you adapt the KNN algorithm to work for classification (where we put cases into, say, 3 categories)? Improving KNN Often machine learning algorithms can/should be modified to work well for the context in which you are working. Let’s explore problems with and modifications of KNN. Looking at the following snippet of a housing dataset, compute the Euclidean distance between house 1 and 2 and between 1 and 3. (TRUE is numerically equivalent to 1 and FALSE to 0.) ## square_feet porch ## 1 1000 TRUE ## 2 1000 FALSE ## 3 1050 TRUE Do you find any problems with the distance calculations in part a? What ideas can you come up with to fix this? Write out in detail the steps of the KNN regression algorithm and try to pick out all areas in which a modification to the algorithm could be made. In what situations would the behavior of the original algorithm be undesirable, and how might you modify the algorithm to improve? "],
["splines.html", "Topic 8 Splines 8.1 Discussion 8.2 Exercises", " Topic 8 Splines 8.1 Discussion Goal: model nonlinear trends in data We’ve seen that KNN regression can learn nonlinear functions. We’ll see that splines can do the same. Splines are a means of performing variable transformations Let’s say that \\(y\\) and \\(x\\) are related with a logarithmic trend: \\(y = \\log(x) + \\varepsilon\\). We transform \\(x\\) to make a new variable called \\(x_{\\text{new}}\\), with \\(x_{\\text{new}}=\\log(x)\\). Then the relationship between \\(y\\) and \\(x_{\\text{new}}\\) will be linear. We do the same thing with splines. But with splines, we create multiple transformed variables. How does the ns() function work? If I want to split my quantitative predictor into \\(r\\) regions, I’ll make \\(r-1\\) cuts (\\(r-1\\) knots). Usually the knots are placed at regularly spaced quantiles (e.g. 25, 50, 75 for 3 knots) This number of regions \\(r\\) is called the degrees of freedom (df) parameter. This is a tuning parameter of splines. Using only the quantitative predictor and df as inputs, ns() knows: The exact number of transformation functions to use (this number is equal to df) The exact formulas of the transformation functions (they’re polynomial functions) How does R know the exact formulas of the transformation functions? Calculus and linear algebra Example Suppose we break our predictor variable into 3 regions (2 knots). We want to find coefficients for the following polynomials in the 3 regions: Region 1: \\(a_0 + a_1x + a_2x^2 + a_3x^3\\) Region 2: \\(a_4 + a_5x + a_6x^2 + a_7x^3\\) Region 3: \\(a_8 + a_9x + a_{10}x^2 + a_{11}x^3\\) We need to make sure that, at the knots, the polynomials are continuous and have continuous first and second derivatives. We also make sure that at the minimum and maximum \\(x\\), the function is linear. When you run ns(), R doesn’t give these functions directly but does give a mathematically “nice” representation of those 3 functions: x &lt;- sort(college_subs$PhD) spline_terms &lt;- ns(x, df = 3) matplot(x, spline_terms, type = &quot;l&quot;, lty = &quot;solid&quot;, lwd = 6, xlab = &quot;PhD&quot;, ylab = &quot;Spline function&quot;, cex.axis = 1.5, cex.lab = 1.5) legend(&quot;topleft&quot;, legend = paste(&quot;Spline function&quot;, 1:3), col = 1:3, lty = &quot;solid&quot;, bty = &quot;n&quot;, cex = 1.5) (If you’ve taken linear algebra, this is a basis representation.) To use these transformation functions, we plug in the original PhD’s and get out 3 transformed versions of PhD. You can think of these transformations as corresponding to the polynomials in the 3 regions. Some predictions made from splines Red line: linear fit Blue line: LOESS fit Orange line: spline fit with degrees of freedom (regions) = 4 (3 knots) Green line: spline fit with degrees of freedom (regions) = 20 (19 knots) library(gridExtra) p1 &lt;- ggplot(college_subs, aes(x = PhD, y = Outstate)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE, col = &quot;red&quot;, lwd = 3) + geom_smooth(method = &quot;loess&quot;, se = FALSE, col = &quot;blue&quot;, lwd = 3) p2 &lt;- p1 + geom_smooth(method = &quot;lm&quot;, formula = y ~ splines::ns(x, df = 4), se = FALSE, col = &quot;orange&quot;, lwd = 3) p3 &lt;- p2 + geom_smooth(method = &quot;lm&quot;, formula = y ~ splines::ns(x, df = 20), se = FALSE, col = &quot;limegreen&quot;, lwd = 3) grid.arrange(p1, p2, p3, ncol = 3) 8.2 Exercises You can download a template RMarkdown file to start from here. We’ll explore KNN regression and modeling with splines using the College dataset in the ISLR package (associated with our optional textbook). You’ll need to install the splines package as we’ll be using it subsequently. Our goal will be to build a model that predicts out-of-state tuition (Outstate). # Load packages and data library(dplyr) library(ggplot2) library(caret) library(ISLR) data(College) # Examine the data codebook ?College Get to know the College data Peek at the first few rows. How many colleges are in the dataset? How many possible predictors of Outstate are there? Is Mac one of the colleges in this data? The code to check this quickly is below: &quot;Macalester College&quot; %in% rownames(College) If you are curious about working with strings in R, you might check out the stringr package. Example code is below: library(stringr) rownames(College)[str_detect(rownames(College), &quot;Mac&quot;)] rownames(College)[str_detect(rownames(College), &quot;Olaf&quot;)] We’ll actually work with a subset of the predictors for the rest of the exercises: college_subs &lt;- College %&gt;% mutate(adm_rate = Accept/Apps) %&gt;% select(Outstate, Private, Top10perc, Room.Board, PhD, S.F.Ratio, perc.alumni, Expend, Grad.Rate, adm_rate) Baseline parametric vs. nonparametric (Throughout, include set.seed(2019) in each code chunk just before you use train().) Fit a parametric ordinary least squares (OLS) model of Outstate as a function of all predictors, and use 10-fold cross-validation to estimate the test error of this model. Use the straight average of the RMSE column. We’ll fit nonparametric KNN models to this data and compare performance. The code below fits KNN models for \\(k = 1,6,\\ldots,96\\). (seq(1,100,5) generates a regular sequence from 1 to 100 jumping by 5.) set.seed(2019) knn_mod_allpred &lt;- train( Outstate ~ ., data = college_subs, method = &quot;knn&quot;, tuneGrid = data.frame(k = seq(1,100,5)), trControl = trainControl(method = &quot;cv&quot;, number = 10), metric = &quot;RMSE&quot;, na.action = na.omit ) Models train()ed by caret have several features in common. We can use plot() on the object resulting from train() to plot test RMSE versus the tuning parameter. Comment on the shape of this curve and how it is related to the bias-variance tradeoff. plot(knn_mod_allpred) We can also look at estimated test errors and their associated uncertainty with $results. We can also see the optimal value of the tuning parameter with $bestTune. How does the best KNN model compare to the OLS model in terms of estimated test error? knn_mod_allpred$results knn_mod_allpred$bestTune In machine learning, nonparametric methods tend to suffer from something called the curse of dimensionality. Do some Googling or search our ISLR textbook for this term. In your own words, explain what the curse of dimensionality is and why KNN suffers from it. Considering splines We should first visually explore the data. Make scatterplots of the response versus the following predictors: PhD, S.F.Ratio, perc.alumni. Add a smooth trend line in blue and a linear trend line in red, as below. ggplot(college_subs, aes(x = ???, y = ???)) + geom_point() + geom_smooth(color = &quot;blue&quot;) + geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;) Based on these plots, do you think that a spline model would improve test error? Let’s fit a spline model “by hand”. Nothing for you to modify here, but step through the code below and make sure you understand the logic of what we’re trying to show. library(splines) # Create 4 new spline &quot;variables&quot; (corresponds to 3 knots) spline_terms_phd &lt;- ns(college_subs$PhD, df = 4) spline_terms_phd &lt;- as.data.frame(spline_terms_phd) # Give the variables the names spline1, ..., spline4 colnames(spline_terms_phd) &lt;- paste0(&quot;spline&quot;, 1:4) # Add in a new column for the response variable spline_terms_phd$Outstate &lt;- college_subs$Outstate # Peek at this new &quot;dataset&quot; head(spline_terms_phd) # Fit an OLS model with these spline variables manual_spline_mod &lt;- lm(Outstate ~ spline1+spline2+spline3+spline4, data = spline_terms_phd) # Create a dataset with the original PhD variable and the predictions from the spline model manual_spline_mod_output &lt;- data.frame( PhD = college_subs$PhD, Outstate = fitted.values(manual_spline_mod) ) # Overlay the scatterplot with our spline model&#39;s predictions ggplot(college_subs, aes(x = PhD, y = Outstate)) + geom_point() + geom_smooth(color = &quot;blue&quot;) + geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;) + geom_point(data = manual_spline_mod_output, color = &quot;green&quot;, aes(x = PhD, y = Outstate)) Note: df in the ns() function stands for “degrees of freedom” and is equal to the number of new spline variables created. df is equal to number of knots plus 1. Do splines help? We can actually use ns() to create spline variables automatically when we write model formulas. Use the code below to fit a spline model that uses 3 knots for all of the quantitative predictors. set.seed(2019) spline_mod &lt;- train( Outstate ~ Private+ns(Top10perc, df=4)+ns(Room.Board, df=4)+ns(PhD, df=4)+ns(S.F.Ratio, df=4)+ns(perc.alumni, df=4)+ns(Expend, df=4)+ns(Grad.Rate, df=4)+ns(adm_rate, df=4), data = college_subs, method = &quot;lm&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 10), na.action = na.omit ) Why do we use method = &quot;lm&quot;? Estimate the test RMSE of spline_mod. How does this model compare to the original OLS model? Splines and the bias-variance tradeoff What tuning parameter is associated with splines? Add the spline tuning parameter to your BVT diagram from last time. "],
["local-regression-and-gams.html", "Topic 9 Local Regression and GAMs 9.1 Discussion 9.2 Exercises 9.3 How to choose between methods?!?", " Topic 9 Local Regression and GAMs 9.1 Discussion Local regression Main tool: LOESS = locally estimated scatterplot smoothing Fit local linear regression models, using only a subset of the data How is this different from KNN regression? ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Generalized additive models Instead of assuming linear relationships between predictors and the response… \\[\\text{Outstate} = \\beta_0 + \\beta_1\\text{Private} + \\beta_2\\text{PhD} + \\beta_3\\text{Expend} + \\varepsilon\\] …let’s be more general and say that the relationships can be arbitrary functions: \\[\\text{Outstate} = \\beta_0 + f_1(\\text{Private}) + f_2(\\text{PhD}) + f_3(\\text{Expend}) + \\varepsilon\\] If the functions \\(f_1, f_2, f_3\\) can be described with splines, then the model is just like an ordinary linear regression model and can be fit with least squares. We can also have these functions be described with LOESS functions. 9.2 Exercises Writing pseudocode for LOESS Pseudocode is an algorithm in words. Write pseudocode for the LOESS algorithm assuming that you are supplied the predictor \\(x\\), response \\(y\\), a span of 0.4. Your algorithm will use local linear fits. Assume that ordinary least squares rather than weighted least squares is used. Start from the following: Set up a grid of \\(x\\) values from the minimum to maximum \\(x\\) For each \\(x\\) in the grid: Step 1 Step 2… The LOESS tuning parameter When you use geom_smooth in ggplot the smooth line is drawn by LOESS by default. The main tuning parameter we modify is span. The span gives the percent of data used in the local linear fit. ggplot(College, aes(x = Expend, y = Outstate)) + geom_point() + geom_smooth(span = 0.1, lwd = 3) + labs(title = &quot;Span: 0.1&quot;) ggplot(College, aes(x = Expend, y = Outstate)) + geom_point() + geom_smooth(span = 0.4, lwd = 3) + labs(title = &quot;Span: 0.4&quot;) ggplot(College, aes(x = Expend, y = Outstate)) + geom_point() + geom_smooth(span = 0.8, lwd = 3) + labs(title = &quot;Span: 0.8&quot;) Put span on our bias-variance tradeoff diagram along with labels for bias, variance, complexity, and flexibility. How would you expect a plot of test RMSE versus span to look? Why? GAMs We can fit GAMs in R by installing the gam package. Within caret, we can specify gamLoess for the method. We specify two values for the span tuning parameter: 0.4 and 0.5. We also have to say the degree of the local polynomial that we’re fitting in each small window: degree=1 for a linear fit. library(caret) set.seed(2019) gam_mod &lt;- train( Outstate ~ Private + Room.Board + PhD + perc.alumni + Expend, data = College, method = &quot;gamLoess&quot;, tuneGrid = data.frame(span = c(0.4, 0.5), degree = 1), trControl = trainControl(method = &quot;cv&quot;, number = 10), metric = &quot;RMSE&quot;, na.action = na.omit ) You can plot each LOESS component by calling the plot() function on the finalModel component gam_mod. (This is the model with the best set of tuning parameters.) This will produce a sequence of 5 plots, each illustrating a different smooth \\(\\hat{f}_i(x_i)\\) that describes how out-of-state tuition changes with that predictor, holding constant the other predictors. The dotted lines show 2 standard errors. That is, pick any 2 points on a plot. The difference in \\(y\\) values gives the change in out-of-state tuition associated with the change in \\(x\\) values, holding constant the other predictors. par(mfrow = c(2,3)) # Set up a plot grid with 2 rows and 3 cols plot(gam_mod$finalModel, se = TRUE, col = &quot;red&quot;, lwd = 3) # lwd = line width Pick 1 or 2 of these plots and interpret your findings in context. Anything surprising or interesting? If instead of LOESS representations for the 4 quantitative variables, we wanted spline representations, we can fit a least squares model with spline terms. We can then view the spline components using plot.Gam() from the GAM package. library(gam) # Load for plot.Gam() spline_mod &lt;- lm(Outstate ~ Private + ns(Room.Board, df=4) + ns(PhD, df=4) + ns(perc.alumni, df=4) + ns(Expend, df=4), data = College) par(mfrow = c(2,3)) plot.Gam(spline_mod, se = TRUE, col = &quot;red&quot;, lwd = 3) How do the spline and LOESS components compare? Note: If you want to read about how GAMs are fit, you can read about the backfitting algorithm in ISLR Section 7.7. When we want LOESS representations for the individual predictor functions, backfitting must be performed. Backfitting can sometimes run into computational issues but is fine most of the time. Least squares is more stable overall, and we can use least squares if we want spline representations of the individual predictor functions. The decision making point is whether we want spline or LOESS representations. 9.3 How to choose between methods?!? We’ve covered many tools. Let’s unpack their pros and cons in different settings as well as some ideas that will carry on as we talk about classification. Ordinary linear regression | GAMs and LOESS | | | | KNN &lt;---|-------------------------------------|------------|-------&gt; parametric nonparametric KNN regression I introduced KNN as a means of introducing the idea of nonparametric methods. The idea of nonparametric methods is going to carry through to classification, coming up next! Pros: KNN is a nonparametric method, which might be preferable if we don’t believe that a parametric model, such as a linear regression model (even with spline terms), holds or if we don’t want to assume a parametric model. How to check? Revisit our regression assumptions! Fit a model with the potential predictors, and look at diagnostic plots. Cons: Look back at the video to visualize the functions learned by KNN for different values of K. Even with large K, the function learned looks “blocky” and has sharp angles. Thus, KNN doesn’t look smooth until we increase K a lot, but at that point, we might suffer from too much bias in the bias-variance tradeoff. The curse of dimensionality. This curse may seem kind of odd in that, normally, we want a lot of predictors. But as we’ve seen having a lot of predictors, makes distances between cases potentially quite large, rendering the nearest neighbors to a case actually quite dissimilar. But we can save ourselves in two ways: Much bigger sample size! Sometimes this is feasible. Reduce dimensionality! The technique of dimension reduction is an unsupervised learning technique that combines existing predictors into a smaller set of predictors. We’ll talk about a technique called PCA later in the semester. Splines Pros: Splines pretty nicely cover all sorts of nonlinear trends and are computationally very attractive because spline terms fit exactly into a least squares linear regression framework. Least squares models are very easy to fit computationally. Cons: Splines are still under the umbrella of a parametric model, because we assume the form \\(y = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_p\\). Really, this is a tiny con. Splines are capable of being quite flexible given enough knots. In this sense, splines are pretty nonparametric even though they outwardly take on a parametric-looking form. It is possible to create multidimensional splines by creating interactions between spline terms for different predictors. This suffers from the curse of dimensionality like KNN because we are trying to estimate a wavy surface in a large dimensional (many variable) space where data points will only sparsely cover the many many regions of the space. LOESS Pros: I find LOESS very interpretable. At each test case, a local linear model is fit, which fits well with the idea that if you zoom enough into any function (no matter how curvy), you’ll see a line. LOESS is a popular way to see smooth trends on scatterplots, which is why ggplot uses it in geom_smooth(). Cons: If there are a lot of data points, fitting a LOESS over the entire range of the predictor can be slow because so many local linear regressions must be fit. We can avoid this if we don’t actually need the fit over the entire range. If we want to just make a prediction about the response for a test case, we only need to fit the local regression at the x value for that test case. GAMs Whether the indvidual functions within a GAM are represented with LOESS or splines, the models are still generally called GAMs. But when you describe your model, you should say whether you are using a LOESS or spline representation Pros: GAMs have the interpretability of linear regresson models (relationships for one predictor hold constant the other predictors), and they also flexibly model nonlinearity. Cons: GAM still make the assumption that you add up the individual functions of the predictors (linear combination of the functions). (Why add? Why not a product or quotient?) Still, this additivity allows for its interpretability, which is a definite plus. "],
["logistic-regression.html", "Topic 10 Logistic Regression 10.1 Discussion 10.2 Exercises", " Topic 10 Logistic Regression 10.1 Discussion Linear vs Logistic Regression Linear regression assumptions \\(y\\) is a quantitative response variable with \\[y = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_p + \\varepsilon \\qquad \\text{ where } \\qquad \\varepsilon \\stackrel{ind}{\\sim} N(0, \\sigma^2)\\] Logistic regression assumptions \\(y\\) is a binary (0/1) response variable. Let \\(P(y=1 \\mid x)\\) denote the probability that \\(y=1\\) given (as a function of) values of the predictors \\(x\\). A Bernoulli model is a model for binary responses (a coin flip model). Then \\[y \\stackrel{ind}{\\sim} \\text{Bernoulli}(P(y=1 \\mid x)) \\qquad \\text{ i.e. } y = \\begin{cases} 1 &amp; \\text{ w/ probability } P(y=1 \\mid x) \\\\ 0 &amp; \\text{ w/ probability } 1-P(y=1 \\mid x) \\\\ \\end{cases}\\] where \\[\\log \\left(\\frac{P(y=1 \\mid x)}{1-P(y=1 \\mid x)} \\right) = \\log \\left(\\text{odds}(y=1 \\mid x) \\right) = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_p\\] equivalently \\[P(y=1 \\mid x) = \\frac{e^{\\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_p}}{1 + e^{\\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_p}}\\] Note: in statistics, “log” is the natural log (ln) by default If we wish to use logistic regression for statistical inference: There are link tests to test whether \\(\\log(p/(1-p))\\) is a good function to use on the left to link to the predictors. There is a test for checking whether the linear terms on the right suffice (Box-Tidwell test). We will focus on using logistic regression for prediction purposes. Using a logistic regression model for prediction Given values of predictor variables, the log odds is given directly. Transform to odds by exponentiating Transform to probability with \\(p = \\text{odds}/(1+\\text{odds})\\) Applying a probability threshold gives hard classifications Overall accuracy: out of all cases, fraction of correct classifications Sensitivity: out of the cases that are truly positive, how many of those were correctly classified as positive? Specificity: out of the cases that are truly negative, how many of those were correctly classified as negative? Why look at just one probability threshold? We will look at many thresholds. We’ll explore a new computing tool in R, for loops, to loop over many thresholds. Through this, we’ll build up another evaluation tool: the ROC curve (receiver operating characteristic) and also AUC (area under the ROC curve). 10.2 Exercises You can download a template RMarkdown file to start from here. Machine learning models provide a powerful tool for medical diagnosis. In these exercises, we’ll look at data collected by Dr. Henrique da Mota available on the UCI Machine Learning Repository. We’ll use this data to develop models that can be used to diagnose orthopedic patients’ vertebral columns as “irregular” or “regular”. Irregular vertebral columns either exhibit spondilolysthesis (a condition where one vertebra is displaced relative to another) or disk hernia (a condition in which connective tissue between vertebra is damaged). For each of 309 patients, the dataset contains 6 vertebral measurements (potential predictors) along with the patient’s vertebral class (1 = irregular, 0 = regular). It’s important to keep in mind that each row in the dataset represents a real patient: library(dplyr) vert &lt;- read.csv(&quot;https://www.macalester.edu/~ajohns24/data/vertebral_column.csv&quot;) # Remove an outlier and make the &quot;class&quot; column categorical vert &lt;- vert %&gt;% filter(spondylolisthesis_grade &lt; 300) %&gt;% mutate(class = as.factor(class)) # Check out the first few rows head(vert, 3) ## pelvic_incidence pelvic_tilt lumbar_angle sacral_slope pelvic_radius ## 1 63.03 22.55 39.61 40.48 98.67 ## 2 39.06 10.06 25.02 29.00 114.41 ## 3 68.83 22.22 50.09 46.61 105.99 ## spondylolisthesis_grade class ## 1 -0.25 1 ## 2 4.56 1 ## 3 -3.53 1 # How many patients of each class are there? table(vert$class) ## ## 0 1 ## 100 209 Thinking about assumptions Suppose that we used hypothesis tests to assess logistic regression assumptions and determined that assumptions could be violated. What would be the implications for statistical inference? Specifically what could you say about the performance of 99% confidence intervals? Planning model evaluation You’ll be comparing a few models for predicting vertebral class. Describe how cross-validation can be used to fairly compare models. Outline the procedure and how a final metric is computed. Fitting logistic regression in caret We’ll explore a logistic regression model with all predictors. You can fit this model in caret as below. method = &quot;glm&quot; means “generalized linear model” (GLM). Logistic regression is a linear model (the right hand side). The “generalized” indicates that more types of response variables than just quantitative (for linear regression) can be considered. family = binomial: The Bernoulli model is a model for one trial (one coin flip). The binomial model is a model for multiple trials (multiple coin flips). This is how you communicate that you want logistic regression as opposed to some other GLM. metric = &quot;Accuracy&quot;: as opposed to “RMSE” in the regression setting trControl: Fill this in to use 10-fold CV. set.seed(333) vert_mod &lt;- train( class ~ ., data = vert, method = &quot;glm&quot;, family = binomial, metric = &quot;Accuracy&quot;, trControl = trainControl(???), na.action = na.omit ) Nailing down caret’s evaluation metrics When you print vert_mod or vert_mod$results, the CV-estimated accuracy is reported. Verify this estimate by hand by using the Accuracy column within vert_mod$resample. (To access a column, use $column_name.) caret has a confusionMatrix() function that takes as input predictions from a model and the true classes. It outputs the confusion matrix and a number of accuracy metrics and statistics. We also specify which of “0” or “1” is the “positive” class. There are a lot of metrics computed and reported! We are not going to focus on all of them, but we will try to make sense of many. You’ll check many of these by hand using the confusion matrix at the top. (Don’t worry about Kappa or McNemar for now.) # predict() below uses 0.5 as a probability threshold by default mod_pred &lt;- predict(vert_mod, newdata = vert) # Compute confusion matrix and statistics conf_mat &lt;- confusionMatrix(data = mod_pred, reference = vert$class, positive = &quot;1&quot;) # Print results conf_mat Compute by hand the overall accuracy and check against “Accuracy”. The “95% CI” is a 95% confidence interval for that Accuracy estimate. What information does it give us? What in the world is “No Information Rate” (NIR)? It’s something quite useful actually. Consider the most simplistic classfier in which we always predict the majority class for all cases. What class makes up the majority of the cases? Verify by hand that the fraction of cases in this majority class is the “No Information Rate”. The p-value beneath corresponds to testing the null hypothesis that Accuracy=NIR versus the alternative that Accuracy &gt; NIR. What information does the p-value give us? Compute sensitivity and specificity by hand and check against the output. Positive and negative predictive value “flip” the probability statements of sensitivity and specificity. That is, sensitivity is the probability of classifying a case as positive GIVEN that they are truly positive (\\(P(\\text{classify positive} \\mid \\text{truly positive})\\) in probability notation). Positive predictive value (PPV) flips this to being the probability of a case truly being positive GIVEN that they were predicted to be positive (\\(P(\\text{truly positive} \\mid \\text{classify positive})\\) in probability notation). Why are PPV and NPV useful in practice? If you were a spinal physician, would you use this model in the clinic? Note: If you want to learn more about the metrics that caret outputs, read the documentation by entering ?caret::confusionMatrix in the Console. Varying the probability threshold: for-loops Here, you’ll explore how to write a for-loop to evaluate a logistic regression model at many probability thresholds. Step through the code and comments (comments start with a # pound sign) to understand some general features about looping operations in R. If you ever want to look up the documentation for a function, you can enter ?function_name in the Console. Also fill in the ???’s in the code. # Obtain the actual observed vertebral classes from the data actual_classes &lt;- vert$class # Obtain the predicted probability of vertebral irregularity (Y=1 event) # type=&quot;response&quot; converts log odds, to odds, and finally to a probability pred_probs &lt;- predict(vert_mod$finalModel, newdata = vert, type = &quot;response&quot;) # Create a regularly-spaced sequence of probability thresholds prob_threshs &lt;- seq(0, 1, by = 0.01) # Create empty vectors to store the sensitivity and specificity # for all values of the probability thresholds sens_vec &lt;- rep(0, length(prob_threshs)) spec_vec &lt;- rep(0, length(prob_threshs)) # Loop over all of the thresholds # seq_along() makes a sequence 1,2,...,length of prob_threshs # In each iteration of the loop, the variable i takes these integer values for (i in seq_along(prob_threshs)) { # Get the probability threshold for this iteration thresh &lt;- prob_threshs[i] # If the predicted prob &gt; threshold, predict &quot;1&quot;. # Otherwise, predict &quot;0&quot;. hard_preds &lt;- ifelse(pred_probs &gt; thresh, &quot;1&quot;, &quot;0&quot;) # Create boolean TRUE/FALSE vectors indicating whether the # prediction for each case is a TP, TN, FP, FN. is_tp &lt;- hard_preds==&quot;1&quot; &amp; actual_classes==&quot;1&quot; is_tn &lt;- ??? is_fp &lt;- ??? is_fn &lt;- ??? # Calculate sensitivity and specificity # Hint: sum(is_tp) gives the number of TPs sensitivity &lt;- ??? specificity &lt;- ??? # Store sensitivity specificity in their containers sens_vec[i] &lt;- sensitivity ??? } ROC curves and AUC An ROC curve plots sensitivity on the y-axis versus 1-specificity on the x-axis. In this way, the y=x line provides a convenient reference for a random guessing classifier (flips a coin to make the classification). plot(1-spec_vec, sens_vec) abline(a = 0, b = 1, col = &quot;gray&quot;) What would the ROC curve look like for a model that is uniformly better for every probability threshold? For a model that is uniformly worse for each threshold? What if ROC curves cross each other? A convenient way to quantify differences between models is with the area under the ROC curve, called AUC. The code below shows how you would compute AUC by hand. roc_fun &lt;- approxfun(x = 1-spec_vec, y = sens_vec) integrate(roc_fun, lower = 0, upper = 1) What is the AUC for a perfect classifier? For the random guessing classifier? You might imagine that there is an R package for making ROC curves and calculating AUC. There is! You will explore the pROC package in your homework. "],
["revisiting-old-tools.html", "Topic 11 Revisiting Old Tools 11.1 Discussion 11.2 Exercises", " Topic 11 Revisiting Old Tools 11.1 Discussion Logistic regression is fit with a technique called maximum likelihood. What is this technique? Example: flipping a coin 3 times, unknown probability \\(p\\) of getting heads The data: 2 heads, 1 tail 3 ways that could have happened: T, H, H H, T, H H, H, T Each way has probability \\(p^2(1-p)\\) The probability of seeing my data (as a function of \\(p\\)) is \\(3p^2(1-p)\\) Goal: Find the \\(p\\) that maximizes that probability The 3 doesn’t matter for this, so we usually remove such constant terms. \\(p^2(1-p)\\) is the likelihood function. Imagine (in a non-coin flipping situation) that we wanted \\(p\\) to depend on predictors…can we relate this back to logistic regression? Yes! \\[p = \\frac{e^{\\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_p}}{1 + e^{\\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_p}}\\] The value of the likelihood function at the best \\(p\\) is a number: the likelihood of the data Connecting to stepwise selection For logistic regression, stepwise selection is exactly the same, except that likelihood is used instead of RSS. Cross-validated accuracy is an option too. Connecting to GAMs There can be nonlinear relationships between quantitative predictors and log odds as well. e.g. \\(\\text{log odds(foreclosure)} = \\beta_0 + f_1(\\text{Age}) + f_2(\\text{Price})\\) Build \\(f_1\\) and \\(f_2\\) from LOESS or splines If relationships are truly nonlinear, should help us improve prediction accuracy. 11.2 Exercises Consider how LASSO would be extended to the logistic regression setting. Using the penalized least squares criterion as a reference, how would you write a penalized criterion for logistic regression using the likelihood? On a different note unrelated to likelihood, consider the KNN algorithm for regression. How would you modify the algorithm to… make a hard classification? produce a “soft” classification? A “soft” classification for a test case is an estimated probability of being in each of the \\(K\\) classes. A concrete example to frame your answers: say that for a particular test case, you found its 10 nearest neighbors in the training set. 5 were of Class A, 3 of Class B, and 2 of Class C. "],
["decision-trees.html", "Topic 12 Decision Trees 12.1 Discussion 12.2 Exercises", " Topic 12 Decision Trees 12.1 Discussion Why trees? Logistic regression is a parametric method. Also can only model binary responses If we don’t think that the model form below holds, might prefer a nonparametric method \\[\\text{log odds} = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_p\\] Nonparametric methods KNN classification Tree-based methods Both KNN and tree-based methods can handle 3+ classes. Gini index: choosing the “best” binary split Suppose: response variable \\(y\\) has \\(C\\) different classes the tree has \\(R\\) total regions/nodes (leaves) Measuring the purity of a single node Let \\[p_{rc} = \\text{proportion of cases in region/node $r$ that are of class $c$}\\] From these, we can compute the node/region’s Gini index/impurity: \\[G_r = \\sum_{c=1}^C p_{rc} (1 - p_{rc})\\] The smaller \\(G_r\\), the more pure region \\(r\\) is. \\(G_r = 0\\) if region \\(r\\) is perfectly pure (the cases in the node are all of 1 class). Choosing the binary splits The binary splits in a tree are chosen to minimize the average Gini index across all regions: \\[\\sum_{r=1}^{R} G_r \\cdot \\frac{\\text{# cases in region r}}{\\text{total # cases}}\\] Example 13 cases; 3 classes (A, B, and C) What is the average Gini index for the first split? For the second split? Which split is preferable? 12.2 Exercises You can download a template RMarkdown file to start from here. Why use the Gini index instead of classification accuracy to build the tree? Consider a situation where we have 2 classes (class A and B) with 400 cases in each class. Consider two different splits: Split 1: Node 1 has 300 cases of class A and 100 of class B Node 2 has 100 cases of class A and 300 of class B Split 2: Node 1 has 200 cases of class A and 400 of class B Node 2 has 200 cases of class A and 0 of class B To make a prediction for a node, we predict the majority class for all cases in that node. (e.g. For Split 1, Node 1, we predict A for all cases in that node.) What is the overall accuracy over both Nodes 1 and 2 for Split 1? For Split 2? What is the average Gini index for Split 1? For Split 2? Based on this example, why do you think the Gini index is preferred? Trees can also be used for regression! For classification, we use average Gini index as a measure of node (im)purity for tree building. We can build regression trees using residuals. In particular, we try to find the particular predictor and split point that minimize the following equation: \\[\\sum_{i: x_i \\in R_1} (y_i - \\hat{y}_{R_1})^2 + \\sum_{i: x_i \\in R_2} (y_i - \\hat{y}_{R_2})^2\\] There’s a lot of notation in the above equation, but we can break it down into two sensible pieces. \\(\\hat{y}_{R_1}\\) and \\(\\hat{y}_{R_2}\\) are the predicted values in region(/node/leaf) 1 and region 2. (Regions 1 and 2 are the left and right branches.) These predicted values are the mean of the cases that end up in that region. The left sum is the sum of squared residuals in region 1, and the right sum is the sum of squared residuals in region 2. Explain why minimizing the above equation is sensible for building regression trees. Those greedy, greedy algorithms… It was mentioned in the video that recursive binary splitting (RBS) is a greedy algorithm. What is another technique we’ve learned in class that is greedy? Describe how RBS parallels that other technique. Overfitting and the bias-variance tradeoff Consider the two trees below built from a heart disease dataset. Which tree is likely overfit to the data and why? Number of splits is not something that we can often directly control when we use software to build decision trees, but we can still think of it as a tuning parameter. How is number of splits related to bias and variance? Tree pruning The idea behind tree pruning is to first grow a large tree. (e.g. Grow each region until the leaves have 5 or fewer cases.) Then we prune some branches–that is, we take out some splits. If there was a split, merge it back together. The big tree above was actually pruned to make the smaller tree. Compare the trees and verify for a couple of branches that the changes result from merging nodes back together. This pruning is achieved with something called cost complexity pruning, and the idea is very related to LASSO! The cost complexity criterion for classification trees looks like: \\[\\text{average Gini index over leaves} + \\alpha \\times (\\text{# leaves})\\] Describe how this pruning criterion helps with overfitting by discussing the 2 extremes of \\(\\alpha\\). How can we pick a good value for \\(\\alpha\\)? In the exercises below, we’ll work with some real data and caret to nail down some computing essentials for decision trees. You’ll be looking at high resolution aerial image data to classify different parts of images into different types of urban land cover. Data from the UCI Machine Learning Repository include the observed type of land cover (determined by human visual inspection) and “spectral, size, shape, and texture information” computed from the image. There are 9 types of land uses in the dataset. We’ll focus on distinguishing between asphalt, grass, and tree. library(ggplot2) library(dplyr) land &lt;- read.csv(&quot;https://www.macalester.edu/~ajohns24/data/land_cover.csv&quot;) land &lt;- land %&gt;% filter(class %in% c(&quot;asphalt &quot;,&quot;grass &quot;, &quot;tree &quot;)) %&gt;% mutate(class = droplevels(class)) Before moving forward, install the rpart and rpart.plot packages in the Console. rpart builds decision trees and rpart.plot makes beautiful tree plots. install.packages(c(&quot;rpart&quot;, &quot;rpart.plot&quot;)) There are 147 possible predictors of land use class. We can fit a decision tree using all 147 predictors with the code below. 10-fold CV is used to estimate test accuracy. The tuneLength = 30 means that 30 values of the tree’s tuning parameter are tried. This tuning parameter is called the cp or “Complexity Parameter” and can be viewed similarly to \\(\\alpha\\) in Exercise 5. That is, high values of this parameter are akin to high values of \\(\\alpha\\). library(caret) library(rpart) library(rpart.plot) set.seed(333) tree_mod1 &lt;- train( class ~ ., data = land, method = &quot;rpart&quot;, metric = &quot;Accuracy&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 10), tuneLength = 30 ) Plot test accuracy versus this Complexity Parameter. Roughly, what is the optimal value of this parameter? plot(tree_mod1) Why does the plot look this way? Specifically, write an explanation that discusses the far left of the plot, the middle, and the far right. Discuss tree complexity, bias, and variance in these 3 parts of the plot. Report the exact value for this optimal value with tree_mod1$bestTune, and report the estimated test accuracy of this tree by looking at tree_mod1$results. Plot the best tree with: rpart.plot(tree_mod1$finalModel) Look at page 3 of the rpart.plot package vignette (an example-heavy manual) to understand what the plot components mean. What percent of the cases get split into the region defined by the following condition? NDVI &gt;= -0.01, Bright_100 &lt; 142 What would be your hard prediction of land use class when NDVI = -0.05? What are the soft predictions of a region being asphalt, grass, tree when NDVI = -0.05? Describe a situation in which these soft predictions would be useful. There are more tuning parameters than just the cp Complexity Parameter. You can view many ways to control the building of a tree (essentially encoding the stopping criteria) by looking at the help page for the rpart.control() function by entering ?rpart.control in the Console. We will explore the minbucket parameter in this exercise. Read about the minbucket parameter on the help page. What will happen to the number of splits in the tree as minbucket increases? The code below fits a decision tree using the best cp value identified from cross-validation earlier and uses a minbucket of 7 (the default value). Write a for-loop to plot trees for several minbucket values to check your response in part a. Use values from 10 to 70 counting up by 10. Hint 1: First create the minbucket sequence using the seq() function. Hint 2: You don’t need to create any “empty boxes” to store output because the goal is to just print the trees. Hint 3: The inside of the for-loop will be almost identical to what is already below. tree_mod2 &lt;- rpart( class ~ ., data = land, minbucket = 7, cp = tree_mod1$bestTune$cp ) rpart.plot(tree_mod2) "],
["bagging-and-random-forests.html", "Topic 13 Bagging and Random Forests 13.1 Exercises", " Topic 13 Bagging and Random Forests 13.1 Exercises You can download a template RMarkdown file to start from here. Install the randomForest package in the Console before starting these exercises. We will continue looking at high resolution aerial image data to classify different parts of images into different types of urban land cover. Data from the UCI Machine Learning Repository include the observed type of land cover (determined by human visual inspection) and “spectral, size, shape, and texture information” computed from the image. You will be predicting the class variable which gives the type of urban land cover. # Load required packages library(dplyr) library(rpart) library(randomForest) library(caret) # Read in data land &lt;- read.csv(&quot;https://www.macalester.edu/~ajohns24/data/land_cover.csv&quot;) land &lt;- land %&gt;% filter(class %in% c(&quot;asphalt &quot;,&quot;grass &quot;, &quot;tree &quot;)) %&gt;% mutate(class = droplevels(class)) Fit a random forest with caret with 10-fold CV, and call this object rf_mod_cv. This time the code is not provided. Try to figure out how to construct the train() command by looking back at previous examples from other methods. Some guidance: Visit caret’s (extensive) manual and search for “random forest” in the top right search bar. There will be many results. Find the entry that only has randomForest listed in the “Libraries” column. (The instructor is not familiar with the other packages.) Pay attention to the “method Value” column. Also pay attention to the “Tuning Parameters” column. Answer these next 2 questions in your homework: What is the tuning parameter called? What does this stand for? (To answer the second question, look up the documentation for the randomForest() function by entering ?randomForest in the Console.) Don’t worry about the tuneGrid option for now. If you don’t supply this, caret picks sensible default values for varying the tuning parameter. (In general) Don’t worry about including na.action unless you end up getting an error message that has something to do with NA’s. Now that you have your train() command set up, let’s time how long it takes to execute. Wrap your entire command within the system.time() function as below: system.time({ rf_mod_cv &lt;- train(???) }) The number under elapsed gives the runtime in seconds. Ah! But as discussed in the video, OOB is another option. Figure out how to modify the trControl part of your command to use OOB error estimation (look up the documentation for caret’s trainControl() function), and save this new object as rf_mod_oob. Also time your command with system.time() as above. How does the runtime here compare to using CV? Print rf_mod_oob and rf_mod_cv. (Just type the object names. Running this code displays some summary output for these models that caret has computed.) How do the estimated test accuracies compare between the two test error estimation methods? Now let’s just focus on rf_mod_oob. What value of the tuning parameter corresponds to performing just bagging (bagging but not random forests)? Why? Plot estimated test accuracy versus the tuning parameter. (For any object resulting from caret’s train() function, you can put that object inside plot() to make such a plot. That is, you can just run plot(rf_mod_oob).) The other values of the tuning parameters correspond to random forests. How do random forests compare to bagging? Is this dependent on the tuning parameter? Discuss in terms of the bias-variance tradeoff. Note: You don’t have to include this in your homework, but you could refer back to your test accuracy from a single decision tree from the last set of exercises (where you made tree_mod1 in Exercise 6) and check: do random forests actually improve on the single decision tree? Confusion matrix The OOB principle can be used to generate a test confusion matrix (as opposed to a training confusion matrix). This is displayed when you print rf_mod_oob$finalModel. Rows are predicted classes, and columns are true classes. In parts a and b, we’ll nail down the difference between test and training confusion matrices. For a particular case in the dataset, how do we determine the trees for which the case is OOB? How do we use these trees to make a single prediction for this case? Thus, how is the test confusion matrix constructed? This is contrast to a training confusion matrix where we use all trees to make a prediction for a particular case–even trees that were built on data that included this case. Why is the test confusion matrix preferable to a training confusion matrix? Look at the errors that were made for our land use classification. Why (contextually) do you think some errors are more common than others? Variable importance measures Because bagging and random forests use tons of trees, the nice interpretability of single decision trees is lost. However, we can still get a measure of how important the different variables were in this classification task. For each of the 147 predictors, the code below gives the “total decrease in node impurities (as measured by the Gini index) from splitting on the variable, averaged over all trees” (package documentation). var_imp_rf &lt;- randomForest::importance(rf_mod_oob$finalModel) # Sort by importance with dplyr&#39;s arrange() var_imp_rf &lt;- data.frame(predictor = rownames(var_imp_rf), MeanDecreaseGini = var_imp_rf[,&quot;MeanDecreaseGini&quot;]) %&gt;% arrange(desc(MeanDecreaseGini)) # Top 20 head(var_imp_rf, 20) # Bottom 10 tail(var_imp_rf, 10) Check out the codebook for these variables here. The descriptions of the variables aren’t the greatest…but does this ranking make some contextual sense? It has been found that this random forest measure of variable importance is ok generally but can tend to favor predictors with a lot of unique values. Explain briefly why it makes sense that this can happen by thinking about the recursive binary splitting algorithm for a single tree. Extra! If you want a challenge, try implementing a random forest by yourself using a for-loop. You’ll need a number of pieces of R syntax to get this up and running: Rely on the rpart() function to build the individual trees. You’ll need to supply a formula object for the first part (like class ~ x1+x4+x10). How to do that….? Read on. Look at documentation for the colnames() and setdiff() functions to get a character vector of the predictors. Look at the paste() function for combining entries in a character vector into one string. Look at the as.formula() function. Look at dplyr’s sample_n() function for performing bootstrapping. You’ll want to set.seed()…but inside or outside the for-loop? Note: this is a simpler version of a random forest in which a random subset of the predictors is used for an entire single tree rather than different random subsets at each split. To actually implement the random subset of predictors at each split, we’d need to code decision trees from scratch…which is possible, but much more adventurous! # Use colnames() and setdiff() to get a character vector of the predictors: predictors &lt;- ??? # Total number of trees in the forest num_trees &lt;- 100 # Compute the square root of the number of predictors. # You&#39;ll want to round() this num_pred_per_tree &lt;- ??? # Set up contain for the trees tree_list &lt;- vector(&quot;list&quot;, length = num_trees) # Loop! for (i in seq_len(num_trees)) { land_boot &lt;- sample_n(???) random_predictors &lt;- sample(???) tree_formula &lt;- ??? tree_boot &lt;- rpart(???) tree_list[[i]] &lt;- tree_boot } "],
["midterm-review.html", "Topic 14 Midterm Review 14.1 Organizing information on all methods 14.2 Review of specific methods", " Topic 14 Midterm Review Shared Google doc for midterm review 14.1 Organizing information on all methods For each statistical learning method that we’ve learned about, you might find it useful to organize information on the following aspects: Name of method How does this method work? How to use this method to make predictions? Tuning parameters When would I use this method? (Pros and cons) How does this method work? It is important that you understand how these different methods work from an algorithmic standpoint. If you do, you could implement them yourselves with enough familiarity with a programming language. For this part, write rough pseudocode describing how the method works to build the model. Try to be explicit to the level of “for every … in …, do the following” whenever iteration is required in the algorithm. How to use this method to make predictions? Once we have a trained model, how do we use it to make a prediction for a new test case? For regression tasks, how is a single quantitative prediction made? For classification tasks, how is a single soft or hard prediction made? Tuning parameters Tuning parameters are parameters that we can adjust to tweak these methods. How do adjustments to parameters affect model complexity, bias, variance? When would I use this method? Pros: What nice things does this method allow me to do? Cons: But no method is perfect…do any of the themes below help us think about potential cons of the method? Themes Not all of themes relate to pros/cons, but they are different ways to think about specific methods and general behavior of methods. “Greedy” algorithmic behaivor Penalities Overfitting Bias-variance tradeoff Parametric vs. nonparametric methods Curse of dimensionality Training performance vs. test performance Methods Subset selection Best subset Stepwise selection (forward, backward) LASSO KNN Splines Local regression (LOESS) GAMs Logistic regression Decision trees Bagged trees Random forests 14.2 Review of specific methods Below is a collection of assorted review ideas and practice questions (not necessarily exhaustive). 14.2.1 Subset selection Questions: For a dataset with 5 variables, describe in detail the forward and backward stepwise selection process. Exactly how many models are fit and compared at each stage? What about with best subset selection? 14.2.2 LASSO Questions: Compile a list of all the instances where we have seen penalty terms so far. Describe a common underlying framework for thinking about all of these instances. 14.2.3 Splines Review ideas: A natural cubic spline is a function that consists of piecewise polynomials stitched together across multiple subregions of a predictor. These subregions are formed by cutting the span of the predictor at break points called knots. The piecewise polynomials are continuous at the knots and have continuous first and second derivatives at the knots. The spline function is linear in the region lower than the minimum value of the predictor variable and linear in the region higher than the amx If you choose to break a predictor by choosing \\(K\\) knots, \\(K+1\\) regions are formed, which is the degrees of freedom (df) tuning parameter for splines. The nice thing about a spline is that you can represent this spline function with a weighted sum of simpler polynomial functions. e.g. We desire the relationship between \\(y\\) and \\(x\\) to be modeled with a spline with 1 knot at \\(x = k\\). I can create a spline function to represent this relationship by adding and subtracting together a certain amount of \\(f_1(x) = x\\), \\(f_2(x) = x^2\\), \\(f_3(x) = x^3\\), and \\(f_4(x) = (x-k)^3_+\\). The + in \\((x-k)^3_+\\) denotes the positive part. That is, \\((x-k)^3_+\\) equals \\((x-k)^3\\) when \\(x &gt; k\\) and equals 0 otherwise. If you are curious about the math behind this look at the “Splines” chapter in the Appendix. These simpler polynomial functions thus define variable transformations of the predictor. These transformed versions of the predictors can be treated as variables like any other predictors to be included in an ordinary regression model. 14.2.4 Generalzed additive models (GAMs) Review ideas: Instead of assuming linear relationships between predictors and the response… \\[\\text{Outstate} = \\beta_0 + \\beta_1\\text{PhD} + \\beta_2\\text{Expend} + \\varepsilon\\] …let’s be more general and say that the relationships can be arbitrary functions: \\[\\text{Outstate} = \\beta_0 + f_1(\\text{PhD}) + f_2(\\text{Expend}) + \\varepsilon\\] If the functions \\(f_1, f_2\\) can be described with splines, then the model is just like an ordinary linear regression model and can be fit with least squares. Let’s say that we want 2 knots for each of PhD and Expend. 2 knots = 3 regions = 3 degrees of freedom (df) Then \\(f_1\\) can be represented mathematically with a weighted sum of 3 simpler polynomials \\(a_1(\\text{PhD})\\), \\(a_2(\\text{PhD})\\), and \\(a_3(\\text{PhD})\\). These simpler polynomials define 3 transformed versions of the original PhD variable. And \\(f_2\\) can be represented mathematically with a weighted sum of 3 simpler polynomials \\(b_1(\\text{Expend})\\), \\(b_2(\\text{Expend})\\), and \\(b_3(\\text{Expend})\\). These simpler polynomials define 3 transformed versions of the original Expend variable. We can also have these functions be described with LOESS functions. \\(f_1\\) can be represented by a function drawn from local linear regressions. \\(f_2\\) can be represented by a function drawn from local linear regressions. Note that regardless of how \\(f_1, f_2, \\ldots\\) are built, we interpret them as how the response changes with that predictor, holding constant the other predictors. 14.2.5 KNN and decision trees Questions: KNN and decision trees are both nonparametric methods for regression and classification. Contrast how these models are different from parametric linear and logistic regression models. KNN and decision trees are both nonparametric methods, but differ in the form that they assume for the relationship between the response and the predictors. Describe this and draw a 2-dimensional (2 predictor) example to highlight the difference. In our ISLR book: exercises 4 and 5 in section 8.4 14.2.6 Logistic regression Questions: In our ISLR book: exercises 6 and 9 in section 4.7 Can the “No Information Rate” be computed with 3 or more classes? If so, how? Can sensitivity, specificity, positive predictive value, and negative predictive value be defined with 3 or more classes? If so, how? Can ROC curves, AUC be computed with 3 or more classes? If so, how? 14.2.7 Bagging and random forests Questions: We’ve discussed bootstrap aggregating (bagging) in the context of decision trees. Could bagging apply to linear or logistic regression? How? Come up with a specific numerical example with a small dataset and small number of trees that fully shows how out of bag (OOB) test error is computed. 14.2.8 Test set performance Questions: We have mostly focused on test RMSE (regression) and test overall accuracy (classification). We have many metrics for evaluating classification models that are useful in different ways/in different contexts. There are test versions of these metrics. For example, there are notions of test sensitivity, test specificity, test AUC, test confusion matrix. How are these computed with cross-validation or an out-of-bag (OOB) procedure? "],
["cross-validation-1.html", "A Cross-Validation A.1 Objects A.2 Subsetting A.3 Writing R functions A.4 for-loops and control flow A.5 Building our cross-validation function! A.6 Aside: apply() functions", " A Cross-Validation If you’re keen on learning more about R programming and want to try implementing the methods we’ve talked about in class, you’re in the right place! You’ll learn the programming tools needed to implement cross-validation here. The goal will be to write an R function that performs cross-validation for ordinary least squares linear regression models. Along the way, you’ll learn about objects, subsetting operations, for-loops, and writing functions in R. A.1 Objects Read the Vectors section of the free online Advanced R book by Hadley Wickham. A.2 Subsetting Read the Subsetting section of Advanced R. A.3 Writing R functions Read the Functions chapter of R for Data Science by Garrett Grolemund and Hadley Wickham. A.4 for-loops and control flow Read the Iteration chapter of R for Data Science. Also read the Control flow chapter of Advanced R to learn about if-statements. A.5 Building our cross-validation function! Work through the steps below to build up our cross-validation function. Step 1: Create the skeleton body for a function called cross_validation that takes the following input arguments: data: the training dataset (a data.frame object) formula: the model formula for the linear regression model (e.g. resp~x1+x2). This is a special type of R object called (reasonably) a formula object. k: the k in k-fold cross-validation response: a character object giving the name of the response variable Step 2: Devise a way to randomly split the data into \\(k\\) folds. There are many ways to potentially do this, but here’s an idea that I have in mind: Randomly reorder the data. Then add a new “variable” column that repeats 1,2,3,…k,1,2,3,…,k until the last row. If you want to try my idea, look at the help pages for the the sample_n() function in the dplyr package and the rep() function. (You can enter ?dplyr::sample_n and ?rep into the Console.) You’ll need to be comfortable subsetting data.frame’s. Use either the earlier reading, or look at the filter() function available in dplyr. If you come up with another idea and want help implementing it, feel free to ask! Step 3: Set up an output container object to hold the evaluation metric computed in each iteration of cross-validation. Also write the skeleton of a for-loop. Step 4: Write the for-loop body to perform the steps needed to estimate the test error in this iteration. Feel free to use whatever evaluation metric you desire. It may be helpful to write a function for computing that evaluation metric. If you want to add more capability to your function, try the following for some extra challenge: Remove the response argument. Try to extract it from the formula. To help with this, look into the as.character() function, and the str_split() function in the stringr package. Add an argument called metric that will be a character object specifying what evaluation metric to use. You should create functions for each of the metrics that you’ll allow the user to specify. Allow the k argument to be a character object where the user inputs “loocv” instead of a number. Your function should still support numerical k though. A.6 Aside: apply() functions R provides a family of apply() functions (apply(), lapply(), sapply(), tapply(), mapply()) that do similar things to for-loops but tackle specialized tasks. The main feature that is different between these functions and for-loops is that these functions create the output objects from looping automatically. In contrast, for-loops require you to set up a vector container beforehand to store the outputs being created in the loop. You can learn more about the apply() functions here. It is a common misconception in the R community that these functions are faster than for-loops. This isn’t true. People often like apply() functions for their readability, but everyone has their personal preferences. You can see more of the discussion in this issue of R News and on this StackOverflow thread. "],
["splines-2.html", "B Splines B.1 Exercise B.2 Debriefing", " B Splines Disclaimer: These exercises are by no means required or knowledge that I expect you to be responsible for. But if you want to dig deeper into the math of splines, try these out. Background: A cubic spline is a function that consists of piecewise polynomials stitched together continuously over multiple regions. The spline must also have continuous first and second derivatives at the knots. The exercise below draws from Exercise 1 in Chapter 7 of ISLR. Through it, you’ll see how a spline function can be modeled via linear regression by transforming the predictor variable multiple times. B.1 Exercise We’ll be in a situation where there is one knot at \\(x = k\\). Consider the function \\[f(x) = \\beta_0 + \\beta_1x + \\beta_2 x^2 + \\beta_3 x^3 + \\beta_4 (x-k)^3_+\\] where the + in \\((x-k)^3_+\\) denotes the positive part. That is, \\((x-k)^3_+\\) equals \\((x-k)^3\\) when \\(x &gt; k\\) and equals 0 otherwise. Overall goal: Show that a function of the form above is a cubic spline. That is, we must show that: \\(f(x)\\) consists of two piecewise cubic polynomials: \\(f_1(x)\\) in region 1 where \\(x \\leq k\\) and \\(f_2(x)\\) in region 2 where \\(x &gt; k\\) \\(f(x)\\) is continuous at \\(x = k\\) The first derivative \\(f&#39;(x)\\) is continuous at \\(x = k\\) The second derivative \\(f&#39;&#39;(x)\\) is continuous at \\(x = k\\) Goal 1: Show that \\(f(x)\\) consists of two piecewise cubic polynomials. Show that in region 1, we can write the region-specific polynomial as a cubic. That is, \\(f_1(x)\\) should look like \\(f_1(x) = a_1 + b_1x + c_1x^2 + d_1x^3\\) for some values of \\(a_1, b_1, c_1, d_1\\) related to \\(\\beta_0, \\beta_1, \\beta_2, \\beta_3, \\beta_4\\) and possibly also \\(k\\). Show that in region 2, we can write the region-specific polynomial as a cubic. That is, \\(f_2(x)\\) should look like \\(f_2(x) = a_2 + b_2x + c_2x^2 + d_2x^3\\) for some values of \\(a_2, b_2, c_2, d_2\\) related to \\(\\beta_0, \\beta_1, \\beta_2, \\beta_3, \\beta_4\\) and possibly also \\(k\\). Goal 2: Show that \\(f(x)\\) is continuous at \\(x = k\\). Goal 3: Show that \\(f&#39;(x)\\) is continuous at \\(x = k\\). Goal 4: Show that \\(f&#39;&#39;(x)\\) is continuous at \\(x = k\\). B.2 Debriefing Great! Now that we’ve shown that the function above gives the general form of a spline, we see that we go back to a least squares linear regression framework. Our response \\(y\\) is modeled as \\[y = f(x) + \\varepsilon\\] We see from the form of \\(f(x)\\) that modeling \\(y\\) as a nonlinear function of \\(x\\) just involves “spline-ifying” \\(x\\). We do that by transforming \\(x\\) 4 times: \\(x\\) raised to the power 1 (associated with coefficient \\(\\beta_1\\)) \\(x\\) raised to the power 2 (associated with coefficient \\(\\beta_2\\)) \\(x\\) raised to the power 3 (associated with coefficient \\(\\beta_3\\)) \\(x\\) minus the knot location \\(k\\), raised to the power 3, and finally take the positive part (associated with coefficient \\(\\beta_4\\)) Note: The 4 transformation functions that we’ve looked at here are 4 possible ones. A drawback you may notice is that if \\(x\\) is large, using these transformations involves cubing large numbers, which can be numerically unstable. There are other sets of 4 transformations that can be combined as above to make \\(f(x)\\) a cubic spline overall. (In linear algebra terms, there are other bases of functions that still span the vector space of piecewise polynomials that are continuous and have continuous first and second derivatives at the knots. What has been represented above is a linear combination of basis functions.) In particular, when you use functions like ns() in R, R uses a different set of transformation functions, which are the colored functions like below plotted in the video and in the course manual. library(splines) set.seed(57) n &lt;- 1000 x &lt;- seq(0, 2*pi, length.out = n) y &lt;- sin(x) + rnorm(n, mean = 0, sd = 0.5) spline_terms_df2 &lt;- ns(x, df = 2) spline_terms_df4 &lt;- ns(x, df = 4) spline_terms_df8 &lt;- ns(x, df = 8) par(mfrow = c(1,4)) plot(x, y, main = &quot;y = sin(x)&quot;) matplot(spline_terms_df2, type = &quot;l&quot;, xlab = &quot;x&quot;, ylab = &quot;Transformed x&quot;, main = &quot;df = 2&quot;) matplot(spline_terms_df4, type = &quot;l&quot;, xlab = &quot;x&quot;, ylab = &quot;Transformed x&quot;, main = &quot;df = 4&quot;) matplot(spline_terms_df8, type = &quot;l&quot;, xlab = &quot;x&quot;, ylab = &quot;Transformed x&quot;, main = &quot;df = 8&quot;) The details of the particular algorithm that R uses involves a lot of tedious recursive formulas, which, frankly, the instructor finds confusing and completely unilluminating, which is why we won’t go down that path. If you are interested in looking at this, search for “B-spline algorithm” (B-spline stands for “basis spline”). "]
]
