[
["index.html", "MATH 253: Machine Learning Preface", " MATH 253: Machine Learning Preface Image source This is the class manual for Machine Learning (MATH 253) at Macalester College. The content here was made by Leslie Myint and draws upon our class textbook, An Introduction to Statistical Learning with Applications in R, and on materials prepared by Alicia Johnson. This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],
["schedule.html", "Schedule Tentative overall schedule Week 2: 1/28 - 2/1 Week 3: 2/4 - 2/8", " Schedule Topics for each class day, as well as links to the pre-class videos and slides, are listed below. Before each class, watch the pre-class video and answer the corresponding comprehension questions on Moodle. Tentative overall schedule Regression tasks: Weeks 2-5 Topics: model evaluation, model building, nonparametric methods, tools for modeling nonlinearity Classification tasks: Weeks 6-9 Topics: model evaluation, logistic regression, discriminant analysis, tree-based methods, support vector methods Unsupervised learning: Weeks 10-11 Topics: principal components analysis/regression, clustering Other topics + project work time: Weeks 12-14 Suggested topic: deep learning Week 2: 1/28 - 2/1 Monday: Assumptions of linear regression (Video, Slides) Related ISLR reading: Section 2.1 gives some general background that is not particular to linear regression assumptions but sets up some key foundational ideas. Friday: Model evaluation metrics for regression (Video, Slides) Related ISLR reading: Sections 2.2.1-2.2.2 talk about MSE, training and test data, test error, cross-validation, and the bias-variance tradeoff. The bias-variance tradeoff will come up a little later in class, but feel free to preview the ideas now. Week 3: 2/4 - 2/8 Monday: Cross-validation (Video, Slides) Related ISLR reading: Section 5.1 is devoted to cross-validation. See also Sections 2.2.1-2.2.2. "],
["motivation-and-review.html", "Topic 1 Motivation and Review 1.1 Activity: motivating main ideas 1.2 Review exercises", " Topic 1 Motivation and Review 1.1 Activity: motivating main ideas In each of the following situations, there is some behind-the-scenes code that performs an analysis and generates some output plots. Brainstorm what research question(s) are trying to be answered in each of the situations by looking at the first few rows of data and the plots. Situation A We have 10,000 observations on the following variables: ID: Identification Income: Income in $10,000’s Limit: Credit limit Rating: Credit rating Cards: Number of credit cards Age: Age in years Education: Number of years of education Gender: A factor with levels Male and Female Student: A factor with levels No and Yes indicating whether the individual was a student Married: A factor with levels No and Yes indicating whether the individual was married Ethnicity: A factor with levels African American, Asian, and Caucasian indicating the individual’s ethnicity Balance: Average credit card balance in $. ## Look at the first few rows head(Credit) ## ID Income Limit Rating Cards Age Education Gender Student Married ## 1 1 14.891 3606 283 2 34 11 Male No Yes ## 2 2 106.025 6645 483 3 82 15 Female Yes Yes ## 3 3 104.593 7075 514 4 71 11 Male No No ## 4 4 148.924 9504 681 3 36 11 Female No No ## 5 5 55.882 4897 357 2 68 16 Male No Yes ## 6 6 80.180 8047 569 4 77 10 Male No No ## Ethnicity Balance ## 1 Caucasian 333 ## 2 Asian 903 ## 3 Asian 580 ## 4 Asian 964 ## 5 Caucasian 331 ## 6 Caucasian 1151 Situation B We have 400 observations of the following variables: admit: binary variable; 0 = rejected, 1 = admitted gre: applicant’s GRE score GPA: applicant’s undergraduate GPA rank: A “prestige” ranking of the applicant’s undergraduate institution. 1 to 4 going from least to most prestigious head(grad) ## admit gre gpa rank ## 1 No 380 3.61 3 ## 2 Yes 660 3.67 3 ## 3 Yes 800 4.00 1 ## 4 Yes 640 3.19 4 ## 5 No 520 2.93 4 ## 6 Yes 760 3.00 2 Situation C We have 85 different Halloween candies and measured the following variables: competitorname: The name of the Halloween candy. chocolate: Does it contain chocolate? fruity: Is it fruit flavored? caramel: Is there caramel in the candy? peanutyalmondy: Does it contain peanuts, peanut butter or almonds? nougat: Does it contain nougat? crispedricewafer: Does it contain crisped rice, wafers, or a cookie component? hard: Is it a hard candy? bar: Is it a candy bar? pluribus: Is it one of many candies in a bag or box? sugarpercent: The percentile of sugar it falls under within the dataset. pricepercent: The unit price percentile compared to the rest of the dataset. winpercent: The overall win percentage according to 269,000 matchups. head(candy_rankings) ## competitorname chocolate fruity caramel peanutyalmondy nougat ## 1 100 Grand TRUE FALSE TRUE FALSE FALSE ## 2 3 Musketeers TRUE FALSE FALSE FALSE TRUE ## 3 One dime FALSE FALSE FALSE FALSE FALSE ## 4 One quarter FALSE FALSE FALSE FALSE FALSE ## 5 Air Heads FALSE TRUE FALSE FALSE FALSE ## 6 Almond Joy TRUE FALSE FALSE TRUE FALSE ## crispedricewafer hard bar pluribus sugarpercent pricepercent ## 1 TRUE FALSE TRUE FALSE 0.732 0.860 ## 2 FALSE FALSE TRUE FALSE 0.604 0.511 ## 3 FALSE FALSE FALSE FALSE 0.011 0.116 ## 4 FALSE FALSE FALSE FALSE 0.011 0.511 ## 5 FALSE FALSE FALSE FALSE 0.906 0.511 ## 6 FALSE FALSE TRUE FALSE 0.465 0.767 ## winpercent ## 1 66.97173 ## 2 67.60294 ## 3 32.26109 ## 4 46.11650 ## 5 52.34146 ## 6 50.34755 1.2 Review exercises The following exercises are meant to help you remember key concepts from Math 155 and help you get used to working with R again. All code is provided, but make sure that you understand the general structure so that you could write your own code in the future, with the aid of a reference sheet. (Suggestion: Make a code reference sheet for yourself!) So that you don’t have to copy and paste the code, download and work from the template file here. This is an RMarkdown file that you can open in RStudio. You won’t need to write new code, but you may find it helpful to add notes and comments. (Note: When you go to save this file to your computer, your browser may append a “.txt” to the end of the filename. Delete this extra file extension, and select “All Files” from the “Format” dropdown menu at the bottom of the Save As box.) You will submit your answers to these questions for Homework 1 (on Moodle). Data context: We have data on over a thousand homes in upstate New York. This data contains information on house price as well as several other physical characteristics of the house. library(ggplot2) library(dplyr) homes &lt;- read.delim(&quot;http://sites.williams.edu/rdeveaux/files/2014/09/Saratoga.txt&quot;) Before getting started on the exercises, take a minute to familiarize yourself with the data structure: # Look at the first 6 rows. What are the cases? What are the variables? head(homes) # Obtain the dimensions of the data. How many cases? How many variables? dim(homes) # Look at just the variable names colnames(homes) Visualizing the response variable Construct a visualization of house price: # Histogram ggplot(homes, aes(x = Price)) + geom_histogram() # Density plot ggplot(homes, aes(x = Price)) + geom_density() Question: How would you describe the shape of the distribution? Left-skewed Right-skewed A simple linear regression model We want to explore the relationship between house price and square footage (Living.Area variable): Price = \\(f\\)(Living.Area) + \\(\\varepsilon\\) = \\(\\beta_0\\) + \\(\\beta_1\\)Living.Area + \\(\\varepsilon\\) We can visualize the relationship with a scatterplot with an overlaid estimated linear trend line (geom_smooth(method = &quot;lm&quot;)): ggplot(homes, aes(x = Living.Area, y = Price)) + geom_point() + geom_smooth(method = &quot;lm&quot;) In R we can obtain an equation for the model line above. This equation is an estimate for the population model: Price = \\(\\hat{f}\\)(Living.Area) + \\(\\varepsilon\\) = \\(\\hat\\beta_0\\) + \\(\\hat\\beta_1\\)Living.Area + \\(\\varepsilon\\) # Fit the model mod1 &lt;- lm(Price ~ Living.Area, data = homes) # Print the summarized output from the model summary(mod1) Question: Which of the following is the correct estimated model formula? Price = 113.123 + 13439.394 Living.Area + \\(\\varepsilon\\) Price = 13439.394 + 113.123 Living.Area + \\(\\varepsilon\\) Predictions Consider a house that has a square footage of 1000 square feet. Use mod1 to predict the price of this house. (Round to the nearest dollar.) Residuals A particular house in the dataset has a square footage of 1000 and was priced at $100,000. Using your prediction from the previous exercise (rounded to the nearest dollar), calculate the residual (true value - predicted value) for this house. Adding a categorical variable In addition to Living.Area let’s consider the Fuel.Type variable which has 3 categories: fuel types 2, 3, or 4. We can visualize how fuel type relates to house price with these plots: # Adding fuel as a color to the existing scatterplot ggplot(homes, aes(x = Living.Area, y = Price, color = factor(Fuel.Type))) + geom_point(alpha = 0.2) + geom_smooth(method = &quot;lm&quot;) # Visualizing price and fuel alone ggplot(homes, aes(x = factor(Fuel.Type), y = Price)) + geom_boxplot() We can model this with the multiple linear regression model (“multiple” = multiple predictor variables): Price = \\(f\\)(Living.Area, Fuel.Type) + \\(\\varepsilon\\) Price = \\(\\beta_0\\) + \\(\\beta_1\\)Living.Area + \\(\\beta_2\\)Fuel.Type2 + \\(\\beta_3\\)Fuel.Type3 + \\(\\varepsilon\\) Note that Fuel.Type2 and Fuel.Type3 are indicator variables where, for example, Fuel.Type2 equals 1 if the house uses fuel type 2 and 0 otherwise. To fit this model in R: mod2 &lt;- lm(Price ~ Living.Area + factor(Fuel.Type), data = homes) summary(mod2) (Note: enclosing Fuel.Type within factor() forces R to treat it as a categorical variable. By default, Fuel.Type consists of the integers 2, 3, and 4.) Question: Using this model, what price would you predict for a house that uses fuel type 2 and has a square footage of 1000? (Round to the nearest dollar.) Interpreting coefficients in a multivariate model What is the interpretation of the factor(Fuel.Type)3 coefficient in mod2? The average price for a house that uses fuel type 3. The average price for a house that uses fuel type 3, holding constant square footage. The difference in average price for a house that uses fuel type 3 compared to a house that uses fuel type 2 The difference in average price for a house that uses fuel type 3 compared to a house that uses fuel type 2, holding constant square footage. Interpreting coefficients in a multivariate model What is the interpretation of the Living.Area coefficient in mod2? The average increase in price for each extra square foot The average increase in price for each extra square foot, holding constant fuel type The average increase in price for each extra square foot, only for houses that use fuel type 2 The average increase in price for each extra square foot, only for houses that use fuel type 3 The average increase in price for each extra square foot, only for houses that use fuel type 4 Statistical inference: confidence intervals In mod2 we see that the estimate of \\(\\beta_1\\) is \\(\\hat\\beta_1 = 110.231\\) and has a standard error of \\(SE(\\hat\\beta_1) = 2.784\\). Which of the following provides an approximate 95% confidence interval for \\(\\beta_1\\)? \\(110.231 \\pm 2.784 = (107.447, 113.015)\\) \\(110.231 \\pm 2\\times 2.784 = (104.663, 115.799)\\) \\(110.231 \\pm 3\\times 2.784 = (101.879, 118.583)\\) Confidence interval interpretation How can we interpret the 95% confidence interval? There’s a 95% chance that \\(\\beta_1\\) is in this interval. There’s a 95% chance that our sample would produce a 95% confidence interval that covers \\(\\beta_1\\). Confidence interval interpretation Notice that 0 is not in the 95% confidence interval for \\(\\beta_1\\). What does this tell us? We have significant evidence that house price increases as square footage increases, holding constant fuel type. We do not have significant evidence that house price increases as square footage increases, holding constant fuel type. Statistical inference: p-values The p-value in the Living.Area row corresponds to the test that: \\(H_0: \\beta_1 = 0\\) \\(H_a: \\beta_1 \\neq 0\\) What can we conclude from the p-value? We have significant evidence that house price increases as square footage increases, holding constant fuel type. We do not have significant evidence that house price increases as square footage increases, holding constant fuel type. Interpreting p-values How can we interpret the p-value? There’s a tiny chance that there’s no relationship between house price and square footage holding constant fuel type (\\(\\beta_1=0\\)). There’s a tiny chance that there’s any relationship between house price and square footage holding constant fuel type (\\(\\beta_1 \\neq 0\\)). If in fact only there were no relationship between house price and square footage holding constant fuel type, there’s a tiny chance we’d have gotten this sample of data in which there were such a strong positive relationship. Thinking about fireplaces Let’s think about both square footage (Living.Area) and about whether or not a house has any fireplaces. Below, we create a binary variable called AnyFireplaces that is TRUE if the number of fireplaces is greater than 0 and that is FALSE otherwise. homes &lt;- homes %&gt;% mutate(AnyFireplaces = Fireplaces &gt; 0) We can visualize both of these predictors as follows: ggplot(homes, aes(x = Living.Area, y = Price, color = AnyFireplaces)) + geom_point(alpha = 0.2) + geom_smooth(method = &quot;lm&quot;) Consider two models that use AnyFireplaces: mod3 &lt;- lm(Price ~ Living.Area + AnyFireplaces, data = homes) summary(mod3) mod4 &lt;- lm(Price ~ Living.Area * AnyFireplaces, data = homes) summary(mod4) Question: Consider the research question: “Is a square foot worth the same amount in a home with a fireplace as in a home without a fireplace?” Which of mod3 and mod4 can answer this question? mod3 mod4 Consider the research question: “How much is a square foot worth, holding fixed whether or not a house has a fireplace?” Which of mod3 and mod4 can answer this question? mod3 mod4 Which of mod3 and mod4 would be called an interaction model? mod3 mod4 Summarizing interaction models Using the interaction model chosen in the previous exercise, which of the following represents the relationship between Price and Living.Area for homes without fireplaces? Price = (40901.294-37610.413) + 92.364 Living.Area Price = (40901.294-37610.413) + (92.364+26.852) Living.Area Price = 40901.294 + (92.364+26.852) Living.Area Price = 40901.294 + 92.364 Living.Area Summarizing interaction models Which of the following represents the relationship between Price and Living.Area for homes with fireplaces? Price = (40901.294-37610.413) + 92.364 Living.Area Price = (40901.294-37610.413) + (92.364+26.852) Living.Area Price = 40901.294 + (92.364+26.852) Living.Area Price = 40901.294 + 92.364 Living.Area Inference in interaction models At a significance level of 0.01, what can we conclude about the relationship between price and square footage in homes without fireplaces and in homes with fireplaces in the general population? A square foot in a home with a fireplace is worth more than a square foot in a home without a fireplace. We don’t have evidence to say that a square foot in a home with a fireplace is worth more than a square foot in a home without a fireplace. Inference in interaction models What could we say about the 99% confidence interval for the interaction coefficient? It lies completely above 0. It lies completely below 0. It contains 0. "],
["regression-assumptions.html", "Topic 2 Regression Assumptions 2.1 Discussion 2.2 Exercises", " Topic 2 Regression Assumptions 2.1 Discussion Regression tasks In regression tasks, our goal is to build a model (a function) \\(f\\) that predicts the quantitative response values well: \\[y = f(x) + \\varepsilon\\] \\(y\\): quantitative response variable \\(x = (x_1, x_2, \\ldots, x_p)\\): are \\(p\\) explanatory variables, predictors, features \\(f(x)\\) is a function that captures the underlying trend/relationship between the response and the predictors \\(\\varepsilon\\) is random unexplainable error/noise The linear regression model you learned about in MATH 155 is a special case of \\(f\\): \\[ \\begin{align*} y &amp;= f(x) + \\varepsilon \\\\ &amp;= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p + \\varepsilon \\end{align*} \\] Evaluating regression models Should meet assumptions required for statistical inference Should explain a substantial proportion of the variation in the response Should produce accurate predictions Linear regression assumptions \\[\\varepsilon \\stackrel{ind}{\\sim} N(0, \\sigma^2)\\] Assumptions are to ensure that statistical inference procedures (p-values, confidence intervals) “work as advertised”: The process of creating a 95% CI is a procedure (add and subtract about 2 standard errors from the estimate). It “works as advertised” if in 95% of possible samples it creates intervals that contain the true population value. (The other 5% of samples are unlucky ones.) It does not “work as advertised” if only 90% of possible samples result in intervals (95% CIs) that contain the true population value. 2.2 Exercises You can download a template RMarkdown file to start from here. We will continue looking at the New York housing data. Each case (row) in the dataset is a house in New York, and on each house, we have information on several variables. We’ll focus on the response variable Price (in dollars) and a single predictor variable Age (in years). # Load required packages library(ggplot2) library(dplyr) # Read in the data homes &lt;- read.delim(&quot;http://sites.williams.edu/rdeveaux/files/2014/09/Saratoga.txt&quot;) # Look at the first 6 rows head(homes) ## Price Lot.Size Waterfront Age Land.Value New.Construct Central.Air ## 1 132500 0.09 0 42 50000 0 0 ## 2 181115 0.92 0 0 22300 0 0 ## 3 109000 0.19 0 133 7300 0 0 ## 4 155000 0.41 0 13 18700 0 0 ## 5 86060 0.11 0 0 15000 1 1 ## 6 120000 0.68 0 31 14000 0 0 ## Fuel.Type Heat.Type Sewer.Type Living.Area Pct.College Bedrooms ## 1 3 4 2 906 35 2 ## 2 2 3 2 1953 51 3 ## 3 2 3 3 1944 51 4 ## 4 2 2 2 1944 51 3 ## 5 2 2 3 840 51 2 ## 6 2 2 2 1152 22 4 ## Fireplaces Bathrooms Rooms ## 1 1 1.0 5 ## 2 0 2.5 6 ## 3 1 1.0 8 ## 4 1 1.5 5 ## 5 0 1.0 3 ## 6 1 1.0 8 We will pretend that the homes dataset contains the full population of New York houses. Let’s draw a random sample of 500 houses from the “population”. We can do this with the sample_n() function available in the dplyr package: # Randomly sample 500 homes homes_sample &lt;- sample_n(homes, size = 500) Checking the independence assumption In thinking about what the cases are, do you think the independence assumption holds? A first try at a model Visualize the relationship between house price and house age with a scatterplot and smooth trend line. How would you describe the overall shape of the trend? Is it linear? ggplot(homes_sample, aes(x = ???, y = ???)) + geom_???() + geom_smooth() Using our sample (homes_sample), fit a linear regression model where Price is predicted by Age: # Fit the model mod1 &lt;- lm(Price ~ Age, data = homes_sample) # Display the summary table summary(mod1) Check the trend and homoskedasticity assumptions by plotting the residuals versus the fitted (predicted) values. The points should be evenly scattered around the y = 0 line. Do you think these assumptions are met? # Put the residuals and predicted values into a dataset mod1_output &lt;- data.frame( residual = residuals(mod1), predicted = fitted.values(mod1) ) # Plot ggplot(mod1_output, aes(x = ???, y = ???)) + geom_point() + geom_smooth(color = &quot;blue&quot;, lwd = 3) + # Add a smooth trend geom_hline(yintercept = 0, color = &quot;red&quot;) # Add the y = 0 line Check the normality assumption by making a QQ-plot of the residuals. In a QQ-plot, each residual (y-axis) is plotted against its theoretical corresponding value from a standard normal distribution (\\(N(0,1^2)\\)) on the x-axis. That is, the first quartile of the residuals is plotted against the first quartile of \\(N(0,1^2)\\), the median of the residuals is plotted against the median of \\(N(0,1^2)\\), and so on. If the residuals follow a normal distribution, then the points should fall on a line. Do you think the normality assumption holds? ggplot(mod1_output, aes(sample = residual)) + geom_qq() + geom_qq_line() A second model The diagnostic plots we made above suggest that key assumptions are not being met. Let’s explore how transforming variables can help us meet those assumptions. One of the most common variable transformations that can help fix an unmet homoskedasticity assumption is a logarithmic transformation of the response variable. We will also try to better model the nonlinear shape of the Price vs. Age trend by using a logarithmic transformation. The mutate() function in the dplyr package allows us to create these new transformed variables: # log() = natural logarithm # The %&gt;% is a &quot;pipe&quot;: it takes the dataset to the left and provides it as input to the code that follows homes_sample &lt;- homes_sample %&gt;% mutate( log_price = log(Price), log_age = log(Age + 1) # Some Age&#39;s are 0, so add 1 to prevent log(0), which is undefined ) Fit a linear regression model where log_price is predicted by log_age and obtain the residuals and fitted values: mod2 &lt;- lm(???, data = homes_sample) mod2_output &lt;- data.frame( residual = residuals(???), predicted = fitted.values(???) ) Check the trend, homoskedasticity, and normality assumptions for mod2. Do these assumptions seem to hold better for mod1 or mod2? Implications for statistical inference Since we have the entire population of New York homes, we can investigate whether or not confidence intervals “work as advertised” for the two models we investigated. Display the 95% confidence intervals for the coefficients in mod1 and mod2 with the confint() function: confint(mod1) confint(mod2) By fitting the Price ~ Age model in the full population, we know that the true population value of the Age coefficient is -636.2551305. And by fitting the log_price ~ log_age model in the full population, we know that the true population value of the log_age coefficient is -0.134966. Does the 95% confidence interval “work as advertised” in this case? But what we did in part a just looked at one sample. If the 95% CI truly were working as advertised, 95% of samples would create 95% CIs that contain the true population value. We can run some simulations and see how 95% CIs “work” in 1000 different samples. We find that for mod1, 95% CIs contain the true value of the Age coefficient 899 times. We find that for mod2, 95% CIs contain the true value of the log_age coefficient 964 times. With regards to statistical inference, what can you conclude about assumption violations and fixing those violations? "],
["accuracy-metrics-for-regression.html", "Topic 3 Accuracy Metrics for Regression 3.1 Discussion 3.2 Exercises", " Topic 3 Accuracy Metrics for Regression 3.1 Discussion Evaluating regression models Should meet assumptions required for statistical inference Should explain a substantial proportion of the variation in the response Should produce accurate predictions For both of these points, we can look at residuals. Sum of squared residuals \\[RSS = \\sum_{i=1}^n (y_i - \\hat{y_i})^2 = (y_1 - \\hat{y_1})^2 + (y_2 - \\hat{y_2})^2 + \\cdots + (y_n - \\hat{y_n})^2\\] The sum (and mean) of the residuals is always zero when an intercept is included in the linear regression model -&gt; add up the squared residuals Not very interpretable Due to missing values in predictors, sample size can vary from analysis to analysis (hard to compare RSS) Mean squared error \\[MSE = \\frac{RSS}{n} = \\sum_{i=1}^n (y_i - \\hat{y_i})^2\\] More interpretable than RSS: on average how far are our predictions from the true values (in squared distance)? R-squared Define the total sum of squares (\\(TSS\\)) as the sum of squared deviations of each response \\(y_i\\) from the mean response \\(\\bar{y}\\): \\[TSS = \\sum_{i=1}^n (y_i - \\bar{y})^2\\] \\[R^2 = 1-\\frac{RSS}{TSS} = \\frac{\\text{Var(fitted)}}{\\text{Var(response)}}\\] Most interpretable: the proportion of variation in the response that is explained by the model Problems with R-squared and MSE R-squared automatically increases with added predictors (even useless ones) MSE automatically decreases with added predictors (even useless ones) Example below: dataset with 20 cases. Random numbers are used as predictors. Alternative metrics: Instead of R-squared, use adjusted R-squared Instead of MSE, we’ll use cross-validation (coming up next) Overfitting The example above is a demonstration of overfitting. With more and more predictors, greater chance that some are useless. Including useless predictors in a model is like reading too much into the noise. With overfitting, models don’t tend to generalize well. 3.2 Exercises You’ll be working a dataset containing physical measurements on 80 adult males. These measurements include body fat percentage estimates as well as body circumference measurements. fatBrozek: Percent body fat using Brozek’s equation: 457/Density - 414.2 fatSiri: Percent body fat using Siri’s equation: 495/Density - 450 density: Density determined from underwater weighing (gm/cm^3). age: Age (years) weight: Weight (lbs) height: Height (inches) neck: Neck circumference (cm) chest: Chest circumference (cm) abdomen: Abdomen circumference (cm) hip: Hip circumference (cm) thigh: Thigh circumference (cm) knee: Knee circumference (cm) ankle: Ankle circumference (cm) biceps: Biceps (extended) circumference (cm) forearm: Forearm circumference (cm) wrist: Wrist circumference (cm) It takes a lot of effort to estimate body fat percentage accurately through underwater weighing. The goal is to build the best predictive model for fatSiri using just circumference measurements, which are more easily attainable. (Don’t use fatBrozek or density as predictors.) library(ggplot2) library(dplyr) bodyfat &lt;- read.csv(&quot;https://www.dropbox.com/s/js2gxnazybokbzh/bodyfat_train.csv?dl=1&quot;) # Remove the fatBrozek and density variables bodyfat &lt;- bodyfat %&gt;% select(-fatBrozek, -density) Using tools from Math 155 and 253 (e.g. exploratory plots, p-values, confidence intervals, adjusted R-squared), experiment with different models to try to build the best predictive model possible. What are the adjusted R-squared and MSE for this model? Code notes: if you want to extract the adjusted R-squared from a fitted model, you can use the following. your_model &lt;- lm(fatSiri ~ age, data = bodyfat) summary(your_model)$adj.r.squared And if you want to compute MSE, you can use the function below: mse &lt;- function(mod) { mean(residuals(mod)^2) } mse(your_model) Now that you’ve selected your best model, deploy it in the real world by applying it to a new set of 172 adult males. The predict() function allows you to supply a fitted model and a new dataset of predictors (the newdata argument). bodyfat_test &lt;- read.csv(&quot;https://www.dropbox.com/s/7gizws208u0oywq/bodyfat_test.csv?dl=1&quot;) # Predict test_predictions &lt;- predict(your_model, newdata = bodyfat_test) # Compute MSE # The $ extracts a particular column from a dataset mean((bodyfat_test$fatSiri - test_predictions)^2) Thinking about main themes How did your MSE on the original dataset of 80 males compare to the MSE on the new data of 172 males? What conclusions can you draw from this exploration in relation to overfitting? Thinking more about overfitting Do you think that a model with more predictors or less predictors is more prone to overfitting? Why? The method used to find the coefficients in linear regression is called the least squares method. We find coefficients \\(\\hat{\\beta}_1, \\hat{\\beta}_2, \\ldots, \\hat{\\beta}_p\\) that minimize the sum of squared residuals \\(RSS\\). Given your answer in part a, can you think of a way to modify the least squares criterion to penalize weak predictors being included in a model? That is, can you brainstorm a possible penalty to add below? Least squares criterion: find \\(\\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p\\) that minimize \\(RSS\\) Penalized least squares criterion: find \\(\\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p\\) that minimize \\(RSS + \\text{penalty}\\) Suggestion: Draw inspiration from the “penalty” term in the adjusted R-squared formula from the video. Extra! If you have time and are interested in learning about writing R functions, try the following. Using the mse() function above as a guide, write a function that compute the MSE of a model on new data. What inputs do you need? These must be supplied as arguments to the function. These are given in the parentheses. You can take multiple intermediate steps within the function. This is often recommended for multi-step tasks because it makes the code easier to read. Annotate the steps of your function with comments. (Start a comment line with a #.) "]
]
