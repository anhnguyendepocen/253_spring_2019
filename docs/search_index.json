[
["index.html", "MATH 253: Machine Learning Preface", " MATH 253: Machine Learning Preface Image source This is the class manual for Machine Learning (MATH 253) at Macalester College. The content here was made by Leslie Myint and draws upon our class textbook, An Introduction to Statistical Learning with Applications in R, and on materials prepared by Alicia Johnson. This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],
["schedule.html", "Schedule Tentative overall schedule Week 2: 1/28 - 2/1 Week 3: 2/4 - 2/8 Week 4: 2/11 - 2/15 Week 5: 2/18 - 2/22", " Schedule Topics for each class day, as well as links to the pre-class videos and slides, are listed below. Before each class, watch the pre-class video and answer the corresponding comprehension questions on Moodle. Tentative overall schedule Regression tasks: Weeks 2-5 Topics: model evaluation, model building, nonparametric methods, tools for modeling nonlinearity Classification tasks: Weeks 6-9 Topics: model evaluation, logistic regression, discriminant analysis, tree-based methods, support vector methods Unsupervised learning: Weeks 10-11 Topics: principal components analysis/regression, clustering Other topics + project work time: Weeks 12-14 Suggested topic: deep learning Week 2: 1/28 - 2/1 Monday: Assumptions of linear regression (Video, Slides) Related ISLR reading: Section 2.1 gives some general background that is not particular to linear regression assumptions but sets up some key foundational ideas. Friday: Model evaluation metrics for regression (Video, Slides) Related ISLR reading: Sections 2.2.1-2.2.2 talk about MSE, training and test data, test error, cross-validation, and the bias-variance tradeoff. The bias-variance tradeoff will come up a little later in class, but feel free to preview the ideas now. Week 3: 2/4 - 2/8 Monday: Cross-validation (Video, Slides) Related ISLR reading: Section 5.1 is devoted to cross-validation. See also Sections 2.2.1-2.2.2. Wednesday: We’ll finish reviewing cross validation, then talk briefly about variable selection methods for building models (Video, Slides) Related ISLR reading: Section 6.1 Friday: Quiz 1. Remaining time will be left for working on homework. Week 4: 2/11 - 2/15 Monday: Shrinkage/regularization methods for model building (Video, Slides) Related ISLR reading: Section 6.2 Wednesday: Continue discussing shrinkage methods Friday: K-nearest neighbors regression and the bias-variance tradeoff (Video, Slides) Related ISLR reading: Section 2.2.2 for the bias-variance tradeoff and Section 3.5 for K-nearest neighbors regression Week 5: 2/18 - 2/22 "],
["motivation-and-review.html", "Topic 1 Motivation and Review 1.1 Activity: motivating main ideas 1.2 Review exercises", " Topic 1 Motivation and Review 1.1 Activity: motivating main ideas In each of the following situations, there is some behind-the-scenes code that performs an analysis and generates some output plots. Brainstorm what research question(s) are trying to be answered in each of the situations by looking at the first few rows of data and the plots. Situation A We have 10,000 observations on the following variables: ID: Identification Income: Income in $10,000’s Limit: Credit limit Rating: Credit rating Cards: Number of credit cards Age: Age in years Education: Number of years of education Gender: A factor with levels Male and Female Student: A factor with levels No and Yes indicating whether the individual was a student Married: A factor with levels No and Yes indicating whether the individual was married Ethnicity: A factor with levels African American, Asian, and Caucasian indicating the individual’s ethnicity Balance: Average credit card balance in $. ## Look at the first few rows head(Credit) ## ID Income Limit Rating Cards Age Education Gender Student Married ## 1 1 14.891 3606 283 2 34 11 Male No Yes ## 2 2 106.025 6645 483 3 82 15 Female Yes Yes ## 3 3 104.593 7075 514 4 71 11 Male No No ## 4 4 148.924 9504 681 3 36 11 Female No No ## 5 5 55.882 4897 357 2 68 16 Male No Yes ## 6 6 80.180 8047 569 4 77 10 Male No No ## Ethnicity Balance ## 1 Caucasian 333 ## 2 Asian 903 ## 3 Asian 580 ## 4 Asian 964 ## 5 Caucasian 331 ## 6 Caucasian 1151 Situation B We have 400 observations of the following variables: admit: binary variable; 0 = rejected, 1 = admitted gre: applicant’s GRE score GPA: applicant’s undergraduate GPA rank: A “prestige” ranking of the applicant’s undergraduate institution. 1 to 4 going from least to most prestigious head(grad) ## admit gre gpa rank ## 1 No 380 3.61 3 ## 2 Yes 660 3.67 3 ## 3 Yes 800 4.00 1 ## 4 Yes 640 3.19 4 ## 5 No 520 2.93 4 ## 6 Yes 760 3.00 2 Situation C We have 85 different Halloween candies and measured the following variables: competitorname: The name of the Halloween candy. chocolate: Does it contain chocolate? fruity: Is it fruit flavored? caramel: Is there caramel in the candy? peanutyalmondy: Does it contain peanuts, peanut butter or almonds? nougat: Does it contain nougat? crispedricewafer: Does it contain crisped rice, wafers, or a cookie component? hard: Is it a hard candy? bar: Is it a candy bar? pluribus: Is it one of many candies in a bag or box? sugarpercent: The percentile of sugar it falls under within the dataset. pricepercent: The unit price percentile compared to the rest of the dataset. winpercent: The overall win percentage according to 269,000 matchups. head(candy_rankings) ## competitorname chocolate fruity caramel peanutyalmondy nougat ## 1 100 Grand TRUE FALSE TRUE FALSE FALSE ## 2 3 Musketeers TRUE FALSE FALSE FALSE TRUE ## 3 One dime FALSE FALSE FALSE FALSE FALSE ## 4 One quarter FALSE FALSE FALSE FALSE FALSE ## 5 Air Heads FALSE TRUE FALSE FALSE FALSE ## 6 Almond Joy TRUE FALSE FALSE TRUE FALSE ## crispedricewafer hard bar pluribus sugarpercent pricepercent ## 1 TRUE FALSE TRUE FALSE 0.732 0.860 ## 2 FALSE FALSE TRUE FALSE 0.604 0.511 ## 3 FALSE FALSE FALSE FALSE 0.011 0.116 ## 4 FALSE FALSE FALSE FALSE 0.011 0.511 ## 5 FALSE FALSE FALSE FALSE 0.906 0.511 ## 6 FALSE FALSE TRUE FALSE 0.465 0.767 ## winpercent ## 1 66.97173 ## 2 67.60294 ## 3 32.26109 ## 4 46.11650 ## 5 52.34146 ## 6 50.34755 1.2 Review exercises The following exercises are meant to help you remember key concepts from Math 155 and help you get used to working with R again. All code is provided, but make sure that you understand the general structure so that you could write your own code in the future, with the aid of a reference sheet. (Suggestion: Make a code reference sheet for yourself!) So that you don’t have to copy and paste the code, download and work from the template file here. This is an RMarkdown file that you can open in RStudio. You won’t need to write new code, but you may find it helpful to add notes and comments. (Note: When you go to save this file to your computer, your browser may append a “.txt” to the end of the filename. Delete this extra file extension, and select “All Files” from the “Format” dropdown menu at the bottom of the Save As box.) You will submit your answers to these questions for Homework 1 (on Moodle). Data context: We have data on over a thousand homes in upstate New York. This data contains information on house price as well as several other physical characteristics of the house. library(ggplot2) library(dplyr) homes &lt;- read.delim(&quot;http://sites.williams.edu/rdeveaux/files/2014/09/Saratoga.txt&quot;) Before getting started on the exercises, take a minute to familiarize yourself with the data structure: # Look at the first 6 rows. What are the cases? What are the variables? head(homes) # Obtain the dimensions of the data. How many cases? How many variables? dim(homes) # Look at just the variable names colnames(homes) Visualizing the response variable Construct a visualization of house price: # Histogram ggplot(homes, aes(x = Price)) + geom_histogram() # Density plot ggplot(homes, aes(x = Price)) + geom_density() Question: How would you describe the shape of the distribution? Left-skewed Right-skewed A simple linear regression model We want to explore the relationship between house price and square footage (Living.Area variable): Price = \\(f\\)(Living.Area) + \\(\\varepsilon\\) = \\(\\beta_0\\) + \\(\\beta_1\\)Living.Area + \\(\\varepsilon\\) We can visualize the relationship with a scatterplot with an overlaid estimated linear trend line (geom_smooth(method = &quot;lm&quot;)): ggplot(homes, aes(x = Living.Area, y = Price)) + geom_point() + geom_smooth(method = &quot;lm&quot;) In R we can obtain an equation for the model line above. This equation is an estimate for the population model: Price = \\(\\hat{f}\\)(Living.Area) + \\(\\varepsilon\\) = \\(\\hat\\beta_0\\) + \\(\\hat\\beta_1\\)Living.Area + \\(\\varepsilon\\) # Fit the model mod1 &lt;- lm(Price ~ Living.Area, data = homes) # Print the summarized output from the model summary(mod1) Question: Which of the following is the correct estimated model formula? Price = 113.123 + 13439.394 Living.Area + \\(\\varepsilon\\) Price = 13439.394 + 113.123 Living.Area + \\(\\varepsilon\\) Predictions Consider a house that has a square footage of 1000 square feet. Use mod1 to predict the price of this house. (Round to the nearest dollar.) Residuals A particular house in the dataset has a square footage of 1000 and was priced at $100,000. Using your prediction from the previous exercise (rounded to the nearest dollar), calculate the residual (true value - predicted value) for this house. Adding a categorical variable In addition to Living.Area let’s consider the Fuel.Type variable which has 3 categories: fuel types 2, 3, or 4. We can visualize how fuel type relates to house price with these plots: # Adding fuel as a color to the existing scatterplot ggplot(homes, aes(x = Living.Area, y = Price, color = factor(Fuel.Type))) + geom_point(alpha = 0.2) + geom_smooth(method = &quot;lm&quot;) # Visualizing price and fuel alone ggplot(homes, aes(x = factor(Fuel.Type), y = Price)) + geom_boxplot() We can model this with the multiple linear regression model (“multiple” = multiple predictor variables): Price = \\(f\\)(Living.Area, Fuel.Type) + \\(\\varepsilon\\) Price = \\(\\beta_0\\) + \\(\\beta_1\\)Living.Area + \\(\\beta_2\\)Fuel.Type2 + \\(\\beta_3\\)Fuel.Type3 + \\(\\varepsilon\\) Note that Fuel.Type2 and Fuel.Type3 are indicator variables where, for example, Fuel.Type2 equals 1 if the house uses fuel type 2 and 0 otherwise. To fit this model in R: mod2 &lt;- lm(Price ~ Living.Area + factor(Fuel.Type), data = homes) summary(mod2) (Note: enclosing Fuel.Type within factor() forces R to treat it as a categorical variable. By default, Fuel.Type consists of the integers 2, 3, and 4.) Question: Using this model, what price would you predict for a house that uses fuel type 2 and has a square footage of 1000? (Round to the nearest dollar.) Interpreting coefficients in a multivariate model What is the interpretation of the factor(Fuel.Type)3 coefficient in mod2? The average price for a house that uses fuel type 3. The average price for a house that uses fuel type 3, holding constant square footage. The difference in average price for a house that uses fuel type 3 compared to a house that uses fuel type 2 The difference in average price for a house that uses fuel type 3 compared to a house that uses fuel type 2, holding constant square footage. Interpreting coefficients in a multivariate model What is the interpretation of the Living.Area coefficient in mod2? The average increase in price for each extra square foot The average increase in price for each extra square foot, holding constant fuel type The average increase in price for each extra square foot, only for houses that use fuel type 2 The average increase in price for each extra square foot, only for houses that use fuel type 3 The average increase in price for each extra square foot, only for houses that use fuel type 4 Statistical inference: confidence intervals In mod2 we see that the estimate of \\(\\beta_1\\) is \\(\\hat\\beta_1 = 110.231\\) and has a standard error of \\(SE(\\hat\\beta_1) = 2.784\\). Which of the following provides an approximate 95% confidence interval for \\(\\beta_1\\)? \\(110.231 \\pm 2.784 = (107.447, 113.015)\\) \\(110.231 \\pm 2\\times 2.784 = (104.663, 115.799)\\) \\(110.231 \\pm 3\\times 2.784 = (101.879, 118.583)\\) Confidence interval interpretation How can we interpret the 95% confidence interval? There’s a 95% chance that \\(\\beta_1\\) is in this interval. There’s a 95% chance that our sample would produce a 95% confidence interval that covers \\(\\beta_1\\). Confidence interval interpretation Notice that 0 is not in the 95% confidence interval for \\(\\beta_1\\). What does this tell us? We have significant evidence that house price increases as square footage increases, holding constant fuel type. We do not have significant evidence that house price increases as square footage increases, holding constant fuel type. Statistical inference: p-values The p-value in the Living.Area row corresponds to the test that: \\(H_0: \\beta_1 = 0\\) \\(H_a: \\beta_1 \\neq 0\\) What can we conclude from the p-value? We have significant evidence that house price increases as square footage increases, holding constant fuel type. We do not have significant evidence that house price increases as square footage increases, holding constant fuel type. Interpreting p-values How can we interpret the p-value? There’s a tiny chance that there’s no relationship between house price and square footage holding constant fuel type (\\(\\beta_1=0\\)). There’s a tiny chance that there’s any relationship between house price and square footage holding constant fuel type (\\(\\beta_1 \\neq 0\\)). If in fact only there were no relationship between house price and square footage holding constant fuel type, there’s a tiny chance we’d have gotten this sample of data in which there were such a strong positive relationship. Thinking about fireplaces Let’s think about both square footage (Living.Area) and about whether or not a house has any fireplaces. Below, we create a binary variable called AnyFireplaces that is TRUE if the number of fireplaces is greater than 0 and that is FALSE otherwise. homes &lt;- homes %&gt;% mutate(AnyFireplaces = Fireplaces &gt; 0) We can visualize both of these predictors as follows: ggplot(homes, aes(x = Living.Area, y = Price, color = AnyFireplaces)) + geom_point(alpha = 0.2) + geom_smooth(method = &quot;lm&quot;) Consider two models that use AnyFireplaces: mod3 &lt;- lm(Price ~ Living.Area + AnyFireplaces, data = homes) summary(mod3) mod4 &lt;- lm(Price ~ Living.Area * AnyFireplaces, data = homes) summary(mod4) Question: Consider the research question: “Is a square foot worth the same amount in a home with a fireplace as in a home without a fireplace?” Which of mod3 and mod4 can answer this question? mod3 mod4 Consider the research question: “How much is a square foot worth, holding fixed whether or not a house has a fireplace?” Which of mod3 and mod4 can answer this question? mod3 mod4 Which of mod3 and mod4 would be called an interaction model? mod3 mod4 Summarizing interaction models Using the interaction model chosen in the previous exercise, which of the following represents the relationship between Price and Living.Area for homes without fireplaces? Price = (40901.294-37610.413) + 92.364 Living.Area Price = (40901.294-37610.413) + (92.364+26.852) Living.Area Price = 40901.294 + (92.364+26.852) Living.Area Price = 40901.294 + 92.364 Living.Area Summarizing interaction models Which of the following represents the relationship between Price and Living.Area for homes with fireplaces? Price = (40901.294-37610.413) + 92.364 Living.Area Price = (40901.294-37610.413) + (92.364+26.852) Living.Area Price = 40901.294 + (92.364+26.852) Living.Area Price = 40901.294 + 92.364 Living.Area Inference in interaction models At a significance level of 0.01, what can we conclude about the relationship between price and square footage in homes without fireplaces and in homes with fireplaces in the general population? A square foot in a home with a fireplace is worth more than a square foot in a home without a fireplace. We don’t have evidence to say that a square foot in a home with a fireplace is worth more than a square foot in a home without a fireplace. Inference in interaction models What could we say about the 99% confidence interval for the interaction coefficient? It lies completely above 0. It lies completely below 0. It contains 0. "],
["regression-assumptions.html", "Topic 2 Regression Assumptions 2.1 Discussion 2.2 Exercises", " Topic 2 Regression Assumptions 2.1 Discussion Regression tasks In regression tasks, our goal is to build a model (a function) \\(f\\) that predicts the quantitative response values well: \\[y = f(x) + \\varepsilon\\] \\(y\\): quantitative response variable \\(x = (x_1, x_2, \\ldots, x_p)\\): are \\(p\\) explanatory variables, predictors, features \\(f(x)\\) is a function that captures the underlying trend/relationship between the response and the predictors \\(\\varepsilon\\) is random unexplainable error/noise The linear regression model you learned about in MATH 155 is a special case of \\(f\\): \\[ \\begin{align*} y &amp;= f(x) + \\varepsilon \\\\ &amp;= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p + \\varepsilon \\end{align*} \\] Evaluating regression models Should meet assumptions required for statistical inference Should explain a substantial proportion of the variation in the response Should produce accurate predictions Linear regression assumptions \\[\\varepsilon \\stackrel{ind}{\\sim} N(0, \\sigma^2)\\] Assumptions are to ensure that statistical inference procedures (p-values, confidence intervals) “work as advertised”: The process of creating a 95% CI is a procedure (add and subtract about 2 standard errors from the estimate). It “works as advertised” if in 95% of possible samples it creates intervals that contain the true population value. (The other 5% of samples are unlucky ones.) It does not “work as advertised” if only 90% of possible samples result in intervals (95% CIs) that contain the true population value. 2.2 Exercises You can download a template RMarkdown file to start from here. We will continue looking at the New York housing data. Each case (row) in the dataset is a house in New York, and on each house, we have information on several variables. We’ll focus on the response variable Price (in dollars) and a single predictor variable Age (in years). # Load required packages library(ggplot2) library(dplyr) # Read in the data homes &lt;- read.delim(&quot;http://sites.williams.edu/rdeveaux/files/2014/09/Saratoga.txt&quot;) # Look at the first 6 rows head(homes) ## Price Lot.Size Waterfront Age Land.Value New.Construct Central.Air ## 1 132500 0.09 0 42 50000 0 0 ## 2 181115 0.92 0 0 22300 0 0 ## 3 109000 0.19 0 133 7300 0 0 ## 4 155000 0.41 0 13 18700 0 0 ## 5 86060 0.11 0 0 15000 1 1 ## 6 120000 0.68 0 31 14000 0 0 ## Fuel.Type Heat.Type Sewer.Type Living.Area Pct.College Bedrooms ## 1 3 4 2 906 35 2 ## 2 2 3 2 1953 51 3 ## 3 2 3 3 1944 51 4 ## 4 2 2 2 1944 51 3 ## 5 2 2 3 840 51 2 ## 6 2 2 2 1152 22 4 ## Fireplaces Bathrooms Rooms ## 1 1 1.0 5 ## 2 0 2.5 6 ## 3 1 1.0 8 ## 4 1 1.5 5 ## 5 0 1.0 3 ## 6 1 1.0 8 We will pretend that the homes dataset contains the full population of New York houses. Let’s draw a random sample of 500 houses from the “population”. We can do this with the sample_n() function available in the dplyr package: # Randomly sample 500 homes homes_sample &lt;- sample_n(homes, size = 500) Checking the independence assumption In thinking about what the cases are, do you think the independence assumption holds? A first try at a model Visualize the relationship between house price and house age with a scatterplot and smooth trend line. How would you describe the overall shape of the trend? Is it linear? ggplot(homes_sample, aes(x = ???, y = ???)) + geom_???() + geom_smooth() Using our sample (homes_sample), fit a linear regression model where Price is predicted by Age: # Fit the model mod1 &lt;- lm(Price ~ Age, data = homes_sample) # Display the summary table summary(mod1) Check the trend and homoskedasticity assumptions by plotting the residuals versus the fitted (predicted) values. The points should be evenly scattered around the y = 0 line. Do you think these assumptions are met? # Put the residuals and predicted values into a dataset mod1_output &lt;- data.frame( residual = residuals(mod1), predicted = fitted.values(mod1) ) # Plot ggplot(mod1_output, aes(x = ???, y = ???)) + geom_point() + geom_smooth(color = &quot;blue&quot;, lwd = 3) + # Add a smooth trend geom_hline(yintercept = 0, color = &quot;red&quot;) # Add the y = 0 line Check the normality assumption by making a QQ-plot of the residuals. In a QQ-plot, each residual (y-axis) is plotted against its theoretical corresponding value from a standard normal distribution (\\(N(0,1^2)\\)) on the x-axis. That is, the first quartile of the residuals is plotted against the first quartile of \\(N(0,1^2)\\), the median of the residuals is plotted against the median of \\(N(0,1^2)\\), and so on. If the residuals follow a normal distribution, then the points should fall on a line. Do you think the normality assumption holds? ggplot(mod1_output, aes(sample = residual)) + geom_qq() + geom_qq_line() A second model The diagnostic plots we made above suggest that key assumptions are not being met. Let’s explore how transforming variables can help us meet those assumptions. One of the most common variable transformations that can help fix an unmet homoskedasticity assumption is a logarithmic transformation of the response variable. We will also try to better model the nonlinear shape of the Price vs. Age trend by using a logarithmic transformation. The mutate() function in the dplyr package allows us to create these new transformed variables: # log() = natural logarithm # The %&gt;% is a &quot;pipe&quot;: it takes the dataset to the left and provides it as input to the code that follows homes_sample &lt;- homes_sample %&gt;% mutate( log_price = log(Price), log_age = log(Age + 1) # Some Age&#39;s are 0, so add 1 to prevent log(0), which is undefined ) Fit a linear regression model where log_price is predicted by log_age and obtain the residuals and fitted values: mod2 &lt;- lm(???, data = homes_sample) mod2_output &lt;- data.frame( residual = residuals(???), predicted = fitted.values(???) ) Check the trend, homoskedasticity, and normality assumptions for mod2. Do these assumptions seem to hold better for mod1 or mod2? Implications for statistical inference Since we have the entire population of New York homes, we can investigate whether or not confidence intervals “work as advertised” for the two models we investigated. Display the 95% confidence intervals for the coefficients in mod1 and mod2 with the confint() function: confint(mod1) confint(mod2) By fitting the Price ~ Age model in the full population, we know that the true population value of the Age coefficient is -636.2551305. And by fitting the log_price ~ log_age model in the full population, we know that the true population value of the log_age coefficient is -0.134966. Does the 95% confidence interval “work as advertised” in this case? But what we did in part a just looked at one sample. If the 95% CI truly were working as advertised, 95% of samples would create 95% CIs that contain the true population value. We can run some simulations and see how 95% CIs “work” in 1000 different samples. We find that for mod1, 95% CIs contain the true value of the Age coefficient 899 times. We find that for mod2, 95% CIs contain the true value of the log_age coefficient 964 times. With regards to statistical inference, what can you conclude about assumption violations and fixing those violations? "],
["accuracy-metrics-for-regression.html", "Topic 3 Accuracy Metrics for Regression 3.1 Discussion 3.2 Exercises", " Topic 3 Accuracy Metrics for Regression 3.1 Discussion Evaluating regression models Should meet assumptions required for statistical inference Should explain a substantial proportion of the variation in the response Should produce accurate predictions For both of these points, we can look at residuals. Sum of squared residuals \\[RSS = \\sum_{i=1}^n (y_i - \\hat{y_i})^2 = (y_1 - \\hat{y}_1)^2 + (y_2 - \\hat{y}_2)^2 + \\cdots + (y_n - \\hat{y}_n)^2\\] The sum (and mean) of the residuals is always zero when an intercept is included in the linear regression model -&gt; add up the squared residuals Not very interpretable Due to missing values in predictors, sample size can vary from analysis to analysis (hard to compare RSS) Mean squared error \\[MSE = \\frac{RSS}{n} = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\] More interpretable than RSS: on average how far are our predictions from the true values (in squared distance)? Interpretation downside: the units are squared units Square root of MSE (RMSE = root mean squared error) is often used: \\(RMSE = \\sqrt{MSE}\\) It’s tempting to try to interpret RMSE as the average distance of our predictions from the true values because the units align with the response variable, but it’s not technically quite right due to the square root. Mean absolute error \\[MAE = \\frac{1}{n}\\sum_{i=1}^n \\|y_i - \\hat{y}_i\\|\\] Where \\(\\|y_i - \\hat{y}_i\\|\\) indicates the absolute value of the residual Very interpretable: on average how far are our predictions from the true values R-squared Define the total sum of squares (\\(TSS\\)) as the sum of squared deviations of each response \\(y_i\\) from the mean response \\(\\bar{y}\\): \\[TSS = \\sum_{i=1}^n (y_i - \\bar{y})^2\\] \\[R^2 = 1-\\frac{RSS}{TSS} = \\frac{\\text{Var(fitted)}}{\\text{Var(response)}}\\] Very interpretable: the proportion of variation in the response that is explained by the model Problems with R-squared and MSE R-squared automatically increases with added predictors (even useless ones) MSE automatically decreases with added predictors (even useless ones) Example below: dataset with 20 cases. Random numbers are used as predictors. Alternative metrics: Instead of R-squared, use adjusted R-squared Instead of MSE, we’ll use cross-validation (coming up next) Overfitting The example above is a demonstration of overfitting. With more and more predictors, greater chance that some are useless. Including useless predictors in a model is like reading too much into the noise. With overfitting, models don’t tend to generalize well. 3.2 Exercises You can download a template RMarkdown file to start from here. You’ll be working a dataset containing physical measurements on 80 adult males. These measurements include body fat percentage estimates as well as body circumference measurements. fatBrozek: Percent body fat using Brozek’s equation: 457/Density - 414.2 fatSiri: Percent body fat using Siri’s equation: 495/Density - 450 density: Density determined from underwater weighing (gm/cm^3). age: Age (years) weight: Weight (lbs) height: Height (inches) neck: Neck circumference (cm) chest: Chest circumference (cm) abdomen: Abdomen circumference (cm) hip: Hip circumference (cm) thigh: Thigh circumference (cm) knee: Knee circumference (cm) ankle: Ankle circumference (cm) biceps: Biceps (extended) circumference (cm) forearm: Forearm circumference (cm) wrist: Wrist circumference (cm) It takes a lot of effort to estimate body fat percentage accurately through underwater weighing. The goal is to build the best predictive model for fatSiri using just circumference measurements, which are more easily attainable. (Don’t use fatBrozek or density as predictors.) library(ggplot2) library(dplyr) bodyfat &lt;- read.csv(&quot;https://www.dropbox.com/s/js2gxnazybokbzh/bodyfat_train.csv?dl=1&quot;) # Remove the fatBrozek and density variables bodyfat &lt;- bodyfat %&gt;% select(-fatBrozek, -density) Using tools from Math 155 and 253 (e.g. exploratory plots, p-values, confidence intervals, adjusted R-squared), experiment with different models to try to build the best predictive model possible. What are the adjusted R-squared and MSE for this model? Code notes: if you want to extract the adjusted R-squared from a fitted model, you can use the following. your_model &lt;- lm(fatSiri ~ age, data = bodyfat) summary(your_model)$adj.r.squared And if you want to compute MSE, you can use the function below: mse &lt;- function(mod) { mean(residuals(mod)^2) } mse(your_model) Now that you’ve selected your best model, deploy it in the real world by applying it to a new set of 172 adult males. The predict() function allows you to supply a fitted model and a new dataset of predictors (the newdata argument). bodyfat_test &lt;- read.csv(&quot;https://www.dropbox.com/s/7gizws208u0oywq/bodyfat_test.csv?dl=1&quot;) # Predict test_predictions &lt;- predict(your_model, newdata = bodyfat_test) # Compute MSE # The $ extracts a particular column from a dataset mean((bodyfat_test$fatSiri - test_predictions)^2) Thinking about main themes How did your MSE on the original dataset of 80 males compare to the MSE on the new data of 172 males? What conclusions can you draw from this exploration in relation to overfitting? Thinking more about overfitting Do you think that a model with more predictors or less predictors is more prone to overfitting? Why? The method used to find the coefficients in linear regression is called the least squares method. We find coefficients \\(\\hat{\\beta}_1, \\hat{\\beta}_2, \\ldots, \\hat{\\beta}_p\\) that minimize the sum of squared residuals \\(RSS\\). Given your answer in part a, can you think of a way to modify the least squares criterion to penalize weak predictors being included in a model? That is, can you brainstorm a possible penalty to add below? Least squares criterion: find \\(\\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p\\) that minimize \\(RSS\\) Penalized least squares criterion: find \\(\\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p\\) that minimize \\(RSS + \\text{penalty}\\) Suggestion: Draw inspiration from the “penalty” term in the adjusted R-squared formula from the video. Extra! If you have time and are interested in learning about writing R functions, try the following. Using the mse() function above as a guide, write a function that compute the MSE of a model on new data. What inputs do you need? These must be supplied as arguments to the function. These are given in the parentheses. You can take multiple intermediate steps within the function. This is often recommended for multi-step tasks because it makes the code easier to read. Annotate the steps of your function with comments. (Start a comment line with a #.) "],
["cross-validation.html", "Topic 4 Cross-Validation 4.1 Discussion 4.2 Exercises", " Topic 4 Cross-Validation 4.1 Discussion Overfitting From ISLR, page 32: When a given method yields a small training MSE but a large test MSE, we are said to be overfitting the data. This happens because our statistical learning procedure is working too hard to find patterns in the training data, and may be picking up some patterns that are just caused by random chance rather than by true [trends]. When we overfit the training data, the test MSE will be very large because the supposed patterns that the method found in the training data simply don’t exist in the test data. Note that regardless of whether or not overfitting has occurred, we almost always expect the training MSE to be smaller than the test MSE because most statistical learning methods either directly or indirectly seek to minimize the training MSE. Overfitting refers specifically to the case in which a less flexible model would have yielded a smaller test MSE. “Flexibility” in linear regression refers to the number of coefficients More coefficients (more predictors) = more flexible Fewer coefficients (fewer predictors) = less flexible How do we prevent overfitting? Want an accuracy metric that allows us to choose which of several models will be most accurate on new data Adjusted R-squared is a good idea but restricted to linear regression Cross-validation is a much more general technique that allows estimation of the true error rate on new data (the test error) Use specific statistical learning methods suited to discourage including weak/useless predictors Shrinkage methods (coming soon!) 4.2 Exercises Goals For what purposes would you use cross-validation? How is cross-validation useful for preventing overfitting? You can download a template RMarkdown file to start from here. You’ll continue working on the body fat dataset from last time. The following variables were recorded. fatBrozek: Percent body fat using Brozek’s equation: 457/Density - 414.2 fatSiri: Percent body fat using Siri’s equation: 495/Density - 450 density: Density determined from underwater weighing (gm/cm^3). age: Age (years) weight: Weight (lbs) height: Height (inches) neck: Neck circumference (cm) chest: Chest circumference (cm) abdomen: Abdomen circumference (cm) hip: Hip circumference (cm) thigh: Thigh circumference (cm) knee: Knee circumference (cm) ankle: Ankle circumference (cm) biceps: Biceps (extended) circumference (cm) forearm: Forearm circumference (cm) wrist: Wrist circumference (cm) The focus is on predicting body fat percentage using Siri’s equation (fatSiri) from easily measured variables: age, weight, height, and circumferences. library(ggplot2) library(dplyr) bodyfat_train &lt;- read.csv(&quot;https://www.dropbox.com/s/js2gxnazybokbzh/bodyfat_train.csv?dl=1&quot;) # Remove the fatBrozek and density variables bodyfat_train &lt;- bodyfat_train %&gt;% select(-fatBrozek, -density) 4 models Consider the 4 models below: mod1 &lt;- lm(fatSiri ~ age+weight+neck+abdomen+thigh+forearm, data = bodyfat_train) mod2 &lt;- lm(fatSiri ~ age+weight+neck+abdomen+thigh+biceps+forearm, data = bodyfat_train) mod3 &lt;- lm(fatSiri ~ age+weight+neck+chest+abdomen+hip+thigh+biceps+forearm, data = bodyfat_train) mod4 &lt;- lm(fatSiri ~ ., data = bodyfat_train) # The . means all predictors Before looking at the summary tables, predict: Which model will have the highest R squared? Which model will have the lowest training MSE? Find/compute the R squared and MSE for the 4 models to check your answers in part a. Which model do you think will perform worst on new test data? Why? We’ll use the caret package to perform cross-validation (and to run many different machine learning methods throughout the course). The caret package is a great resource for the machine learning community because it aggregates machine learning methods written by tons of different authors in tons of different R packages into one single package. The advantage is that instead of learning 10 different styles of code for 10 different machine learning methods, we can use fairly similar code throughout the course. Install the caret package by running install.packages(&quot;caret&quot;) in the Console. Cross-validation with the caret package Use the code below to perform 10-fold cross validation for mod1 to estimate the test MSE (\\(\\text{CV}_{(10)}\\)). # Load the package library(caret) # Set up what type of cross-validation is desired train_ctrl_cv10 &lt;- trainControl(method = &quot;cv&quot;, number = 10) # To ensure the same results each time the code is run set.seed(253) # Fit (train) the model as written in mod1 # Also supply information about the type of CV desired for evaluating the model with the trControl argument # na.action = na.pass prevents errors if the data has missing values mod1_cv10 &lt;- train( fatSiri ~ age+weight+neck+abdomen+thigh+forearm, data = bodyfat_train, method = &quot;lm&quot;, trControl = train_ctrl_cv10, na.action = na.pass ) # The $ extracts components of an object # Peek at the &quot;resample&quot; part of mod1_cv10 - what info does it contain? mod1_cv10$resample # Estimate of test MSE # RMSE = square root of MSE mean(mod1_cv10$resample$RMSE^2) Adapt the code above to perform: 10-fold CV for model 2 LOOCV for model 1 In doing so, look carefully at the structure of the code. What parts need to be repeated? What parts don’t? (Hint: nrow(dataset) gives the number of cases in a dataset.) Looking at the evaluation metrics A completed table of evaluation metrics is below. Which model performed the best on the training data? Which model performed best on the test set? Which model would be preferred using \\(\\text{CV}_{(10)}\\) or LOOCV estimates of the test error? How is cross-validation helping us avoid overfitting? Model \\(R^2\\) Training MSE \\(\\text{CV}_{(10)}\\) LOOCV Test set MSE mod1 0.8103 14.52153 17.21062 18.16816 23.92333 mod2 0.8146 14.18762 19.64114 19.29848 23.90547 mod3 0.816 14.08022 21.24115 20.28958 23.63958 mod4 0.8162 14.06917 21.88440 21.26073 24.65370 Practical issues: choosing \\(k\\) What do you think are the pros/cons of low vs. high \\(k\\)? If possible, it is advisable to choose \\(k\\) to be a divisor of the sample size. Why do you think that is? Extra! Writing R functions If you’re interested in learning about writing R functions, look at the following function that the instructor used to fill out the above evaluation metrics table. bodyfat_test &lt;- read.csv(&quot;https://www.dropbox.com/s/7gizws208u0oywq/bodyfat_test.csv?dl=1&quot;) evaluate_model &lt;- function(formula) { train_ctrl_cv10 &lt;- trainControl(method = &quot;cv&quot;, number = 10) train_ctrl_loocv &lt;- trainControl(method = &quot;cv&quot;, number = nrow(bodyfat_train)) mod_cv10 &lt;- train(formula, data = bodyfat_train, method = &quot;lm&quot;, trControl = train_ctrl_cv10, na.action = na.pass) mod_loocv &lt;- train(formula, data = bodyfat_train, method = &quot;lm&quot;, trControl = train_ctrl_loocv, na.action = na.pass) model_predictions &lt;- predict(mod_cv10, newdata = bodyfat_test) test_mse &lt;- mean((bodyfat_test$fatSiri - model_predictions)^2) c( cv10 = mean(mod_cv10$resample$RMSE^2), loocv = mean(mod_loocv$resample$RMSE^2), test = test_mse ) } set.seed(253) evaluate_model(fatSiri ~ age+weight+neck+abdomen+thigh+forearm) evaluate_model(fatSiri ~ age+weight+neck+abdomen+thigh+biceps+forearm) evaluate_model(fatSiri ~ age+weight+neck+chest+abdomen+hip+thigh+biceps+forearm) evaluate_model(fatSiri ~ .) Step through each line and see if you can understand the structure. How would you modify the function to work on arbitrary data? How would you have to change the function arguments (within the parentheses on the first line)? How would you modify the function to allow the user to choose \\(k\\)? "],
["subset-selection.html", "Topic 5 Subset Selection 5.1 Discussion 5.2 Exercises", " Topic 5 Subset Selection 5.1 Discussion Subset selection methods Automated methods that take different strategies for exploring subsets of the predictors Stepwise selection methods: add or remove variables one at a time Best subset selection: brute force method that tries all possible subsets of predictors 5.2 Exercises You can download a template RMarkdown file to start from here. We’ll continue using the body fat dataset to explore subset selection methods. library(ggplot2) library(dplyr) bodyfat &lt;- read.csv(&quot;http://www.macalester.edu/~ajohns24/data/bodyfatsub.csv&quot;) # Take out the redundant Density and HeightFt variables bodyfat &lt;- bodyfat %&gt;% select(-Density, -HeightFt) Backward stepwise selection: by hand In the backward stepwise procedure, we start with the full model, full_model, with all predictors: full_model &lt;- lm(BodyFat ~ Age + Weight + Height + Neck + Chest + Abdomen + Hip + Thigh + Knee + Ankle + Biceps + Forearm + Wrist, data = bodyfat) Then… Identify which predictor contributes the least to the model. One approach is to identify the least significant predictor. Fit a new model which eliminates this predictor. Identify the least significant predictor in this model. Fit a new model which eliminates this predictor. Repeat 1 more time to get the hang of it. Interpreting the results Examine which predictors remain after the previous exercise. Are you surprised that, for example, Wrist is still in the model but Weight is not? Does this mean that Wrist is a better predictor of body fat percentage than Weight is? Forward selection is another stepwise technique. Can you guess how this differs from backward selection? Best subset selection is another subset selection technique that looks at every possible subset of predictors, fits all of these models, and picks the best one. From the perspective of computational time, why is this not a preferable approach? Planning backward selection using CV Using p-values to perform backward selection by hand is convenient but not the most direct way to target predictive accuracy. Outline the steps that you would take to use cross-validation to perform backward selection. (Write an algorithm in words.) Backward stepwise selection in caret We can use the caret package to perform backward stepwise selection with cross-validation as shown below. (Note: CV is only used to pick among the best 1, 2, 3, …, and 13 variable models. To find the best 1, 2, 3, …, and 13 variable models, training MSE is used. caret uses training MSE because within a subset size, all models have the same number of coefficients, which makes both ordinary R-squared and training MSE ok.) Just focus on the structure of the code and how different parts of the output are used. Is there a use case that you are interested in but don’t see below? Feel free to ask the instructor about it! library(caret) # Set up cross-validation information train_ctrl_cv10 &lt;- trainControl(method = &quot;cv&quot;, number = 10) # Perform backward stepwise selection # The first time you run this, you&#39;ll be prompted to install the &quot;leaps&quot; package set.seed(253) back_step &lt;- train( BodyFat ~ ., data = bodyfat, method = &quot;leapBackward&quot;, tuneGrid = data.frame(nvmax = 1:13), trControl = train_ctrl_cv10, metric = &quot;RMSE&quot; ) # Look at accuracy/error metrics for the different subset sizes back_step$results # What tuning parameter gave the best performance? # i.e. What subset size gave the best model? back_step$bestTune # Obtain the coefficients for the best model coefficients(back_step$finalModel, id = back_step$bestTune$nvmax) # Use the best model to make predictions on new data predict(back_step, newdata = bodyfat) Some notes about the code: The BodyFat ~ . formula tells R that BodyFat is the response and that all predictors (specified with the .) will be considered. The tuneGrid argument allows us to input tuning parameters into the fitting process. The tuning parameters here are the number of variables included in the models (nvmax). This can vary from 1 to 13 (the maximum number of predictors possible). The metric argument indicates how the best of the 1-variable, 2-variable, etc. models will be chosen. We’ll use RMSE (root mean squared error). When you look at back_step$results, you’ll see a matrix of output. The rows correspond to the different subset sizes. For each subset size you’ll see the RMSE, \\(R^2\\), and MAE accuracy/error metrics. Recall that these are estimates that arise by taking the mean of the values given in the 10 CV iterations. The 10 values from the 10 iterations also have a standard deviation. These are reported in the last 3 columns. What use might the standard deviation have in picking a final model? "],
["shrinkageregularization.html", "Topic 6 Shrinkage/Regularization 6.1 Discussion 6.2 Exercises", " Topic 6 Shrinkage/Regularization 6.1 Discussion Exploring doing LASSO by hand I took a subset of the Hitters dataset to focus on only the Walks and Assists variables. I made this penalized_rss() function to compute the penalized sum of squared residuals given a value for the lambda penalty and guesses for beta1 (for Walks) and for beta2 (Assists). penalized_rss &lt;- function(lambda, beta1, beta2) { # Predict salary using beta1, beta2 and predictor info pred &lt;- hitters_subs$Walks*beta1 + hitters_subs$Assists*beta2 # Compute residuals resid &lt;- hitters_subs$Salary - pred # Compute RSS rss &lt;- sum(resid^2, na.rm = TRUE) # Compute penalized RSS prss &lt;- rss + lambda*(abs(beta1) + abs(beta2)) prss } I wanted to compute the penalized RSS for many different combinations of lambda, beta1, and beta2. Here are some of those combinations: ## lambda beta1 beta2 ## 1e+03 1.4 7.2 ## 1e+02 3.2 4.4 ## 0e+00 2.0 2.0 ## 1e+05 1.0 7.8 ## 1e+03 1.6 5.2 ## 1e+02 2.8 0.4 I then actually computed the penalized RSS for these combinations using the penalized_rss() function. ## lambda beta1 beta2 pen_rss ## 1e+03 1.4 7.2 53150907 ## 1e+02 3.2 4.4 52964196 ## 0e+00 2.0 2.0 53098255 ## 1e+05 1.0 7.8 54062874 ## 1e+03 1.6 5.2 53133494 ## 1e+02 2.8 0.4 53024238 For each lambda value that I tried, I manually fit a LASSO model by finding the beta1 and beta2 values that minimized the penalized RSS. These are shown here: ## # A tibble: 6 x 4 ## lambda beta1 beta2 pen_rss ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 10 9.8 52264853. ## 2 100 10 9.6 52266813. ## 3 1000 10 8 52283711. ## 4 10000 10 0 52392761. ## 5 100000 10 0 53292761. ## 6 1000000 0 0 53319113. Thought exercise: How do the results above demonstrate the shrinkage property of LASSO? 6.2 Exercises You can download a template RMarkdown file to start from here. We’ll explore LASSO modeling using the Hitters dataset in the ISLR package (associated with the optional textbook). You’ll need to install the ISLR package in the Console first. You should also install the glmnet package as we’ll be using it subsequently for fitting LASSO models. install.packages(c(&quot;ISLR&quot;, &quot;glmnet&quot;)) # Load the data library(ISLR) data(Hitters) # Examine the data codebook ?Hitters The Hitters dataset contains a number of stats on major league baseball players in 1987. Our goal will be to build a regression model that predicts player Salary. Get to know the Hitters data Peek at the first few rows. How many players are in the dataset? How many possible predictors of salary are there? Developing some intuition A natural model to start with is one with all possible predictors. The following model is fit with ordinary (not penalized) least squares: least_squares_mod &lt;- lm(Salary ~ ., data = Hitters) coefficients(least_squares_mod) Use caret to perform 7-fold cross-validation to estimate the test error of this model. Use the straight average of the RMSE column instead of squaring the values first. (Why 7? Think about the number of cases in the folds.) How do you think the estimated test error would change with fewer predictors? Briefly describe how the output of a stepwise selection procedure could help you choose a smaller model (a model with fewer predictors). This model fit with ordinary least squares corresponds to a special case of penalized least squares. What is the value of \\(\\lambda\\) in this special case? As \\(\\lambda\\) increases, what would you expect to happen to the number of predictors that remain in the model? LASSO for specific \\(\\lambda\\) The code below fits a LASSO model with \\(\\lambda = 10\\). This value of \\(\\lambda\\) is specified in the tuneGrid argument. The alpha = 1 specifies the LASSO method specifically (the glmnet method has other purposes). set.seed(74) lasso_mod_lambda10 &lt;- train( Salary ~ ., data = Hitters, method = &quot;glmnet&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 7), tuneGrid = data.frame(alpha = 1, lambda = 10), metric = &quot;RMSE&quot;, na.action = na.omit ) # Model coefficients for lambda = 10 # The .&#39;s indicate that the coefficient is 0 coefficients(lasso_mod_lambda10$finalModel, 10) How many variables remain in the LASSO model with \\(\\lambda=10\\)? How do their coefficients compare to the corresponding variables in the least squares model? Fit the LASSO using \\(\\lambda=100\\). set.seed(74) lasso_mod_lambda100 &lt;- train( Salary ~ ., data = Hitters, method = &quot;glmnet&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 7), tuneGrid = data.frame(alpha = 1, lambda = 100), metric = &quot;RMSE&quot;, na.action = na.omit ) # Model coefficients for lambda = 100 coefficients(lasso_mod_lambda100$finalModel, 100) How many variables remain in the LASSO model with \\(\\lambda=100\\)? Is this model “bigger” or smaller than the LASSO with \\(\\lambda=10\\)? How do the variables’ coefficients compare to the corresponding variables in the least squares model and the LASSO with \\(\\lambda=10\\)? LASSO for a variety of \\(\\lambda\\) There are infinitely many \\(\\lambda\\) we could use. It would be too tedious to examine these one at a time. The following code fits LASSO models across a grid of \\(\\lambda\\) values and makes a summary plot of the coefficient estimates as a function of \\(\\lambda\\). # Create a grid of lambda values lambdas &lt;- 10^seq(-3, 3, length.out = 100) # Fit LASSO models for all of the lambdas set.seed(74) lasso_mod &lt;- train( Salary ~ ., data = Hitters, method = &quot;glmnet&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 7), tuneGrid = data.frame(alpha = 1, lambda = lambdas), metric = &quot;RMSE&quot;, na.action = na.omit ) # Plot summary of coefficient estimates plot(lasso_mod$finalModel, xvar = &quot;lambda&quot;, label = TRUE, col = rainbow(20)) # What variables do the numbers correspond to? rownames(lasso_mod$finalModel$beta) There’s a lot of information in this plot! Each colored line corresponds to a different predictor. The small number to the left of each line indicates a predictor by its position in rownames(lasso_mod$finalModel$beta). The x-axis reflects the range of different \\(\\lambda\\) values considered in lasso_mod (the lambdas vector that we created). At each \\(\\lambda\\), the y-axis reflects the coefficient estimates for the predictors in the corresponding LASSO model. At each \\(\\lambda\\), the numbers at the top of the plot indicate how many predictors remain in the corresponding model. Very roughly eyeball the coefficient estimates when \\(log(\\lambda) = -2\\). Do they look like they correspond to the coefficient estimates from ordinary least squares in exercise 2? Why do all of the lines head toward y = 0 on the far right of the plot? We can zoom in on the plot by setting the y-axis limits to go from -10 to 10 with ylim as below. Compare the lines for variables 6 and 15. What are variables 6 and 15? Which seems to be a more “important” or “persistent” variable? Does this make sense in context? # Zoom in plot(lasso_mod$finalModel, xvar = &quot;lambda&quot;, label = TRUE, col = rainbow(20), ylim = c(-10,10)) # What is variable 6? rownames(lasso_mod$finalModel$beta)[6] Picking \\(\\lambda\\) In order to pick which \\(\\lambda\\) (hence LASSO model) is “best”, we can compare the 7-fold CV error rate for each model. caret has actually done that for us when it train()ed the model. We can look at a plot of those results: # Plot a summary of the performance of the different models plot(lasso_mod) This figure plots cross-validation estimates of the RMSE (y-axis) versus value of \\(\\lambda\\) (regularization parameter). Comment on the shape of the plot. The RMSE’s go down at the very beginning then start going back up. Why do you think that is? Roughly, what value of \\(\\lambda\\) results in the best model? This plot indicates that we tried many \\(\\lambda\\) values that were pretty bad. (Why?) Let’s fit LASSO models over a better grid of \\(\\lambda\\) values. Modify the previous code to use the following grid and remake lasso_mod and the previous plot: lambdas &lt;- seq(0, 50, length.out = 100) Picking \\(\\lambda\\): accounting for uncertainty Each of the points on the previous plot arose from taking the mean RMSE over 7 cross-validation iterations. Those 7 RMSE estimates have a standard deviation and standard error too. You can use the custom best_lambdas() function to make a plot of estimated test RMSE versus \\(\\lambda\\) that also shows information about the standard errors. In particular, the plot shows points that exactly correspond to the previous plot. The additional lines show 1 standard error above and below the RMSE estimate. In essence, the span of the lines indicates a confidence interval. The best_lambdas() function also prints information about some reasonable choices for good \\(\\lambda\\) values. best_lambdas &lt;- function(model) { # Extract the results table res &lt;- model$results # Extract the K in K-fold CV k &lt;- model$control$number # Compute the standard error (SE) of the RMSE estimate res$rmse_se &lt;- res$RMSESD/sqrt(k) # Which lambda resulted in the lowest RMSE? index_lambda_min &lt;- which.min(res$RMSE) lambda_min &lt;- res$lambda[index_lambda_min] # Compute 1 SE below and above the minimum RMSE res$rmse_lower &lt;- res$RMSE - res$rmse_se res$rmse_upper &lt;- res$RMSE + res$rmse_se rmse_lower &lt;- res$RMSE[index_lambda_min] - res$rmse_se[index_lambda_min] rmse_upper &lt;- res$RMSE[index_lambda_min] + res$rmse_se[index_lambda_min] res$within_1se &lt;- res$RMSE &gt;= rmse_lower &amp; res$RMSE &lt;= rmse_upper index_lambda_1se &lt;- max(which(res$within_1se)) lambda_1se &lt;- res$lambda[index_lambda_1se] p &lt;- ggplot(res, aes(x = lambda, y = RMSE)) + geom_pointrange(aes(ymin = rmse_lower, ymax = rmse_upper)) print(p) output &lt;- res[c(index_lambda_min, index_lambda_1se),c(&quot;lambda&quot;, &quot;RMSE&quot;)] rownames(output) &lt;- c(&quot;lambda_min&quot;, &quot;lambda_1se&quot;) output } lambda_choices &lt;- best_lambdas(lasso_mod) lambda_choices The first row of printed output shows a choice for \\(\\lambda\\) called lambda_min, the \\(\\lambda\\) at which the observed CV error was smallest. The second row shows a choice called lambda_1se, the largest \\(\\lambda\\) for which the corresponding LASSO model has a CV error that’s still within 1 standard error of that for the LASSO using lambda_min. Explain why we might use the LASSO with lambda_1se instead of lambda_min. How does the CV-estimated RMSE of these models compare to that of the original ordinary least squares model in exercise 2? Look at the coefficients of LASSO models corresponding to both choices of \\(\\lambda\\). How do the coefficients differ between lambda_min and lambda_1se? Does one model’s coefficients seem more sensible contextually? The instructor does not have a deep enough understanding of baseball, but you might! # Coefficients for the lambda_min LASSO model coefficients(lasso_mod$finalModel, lambda_choices[&quot;lambda_min&quot;, &quot;lambda&quot;]) # Coefficients for the lambda_1se LASSO model coefficients(lasso_mod$finalModel, lambda_choices[&quot;lambda_1se&quot;, &quot;lambda&quot;]) "],
["cross-validation-1.html", "A Cross-Validation A.1 Objects A.2 Subsetting A.3 Writing R functions A.4 for-loops and control flow A.5 Building our cross-validation function! A.6 Aside: apply() functions", " A Cross-Validation If you’re keen on learning more about R programming and want to try implementing the methods we’ve talked about in class, you’re in the right place! You’ll learn the programming tools needed to implement cross-validation here. The goal will be to write an R function that performs cross-validation for ordinary least squares linear regression models. Along the way, you’ll learn about objects, subsetting operations, for-loops, and writing functions in R. A.1 Objects Read the Vectors section of the free online Advanced R book by Hadley Wickham. A.2 Subsetting Read the Subsetting section of Advanced R. A.3 Writing R functions Read the Functions chapter of R for Data Science by Garrett Grolemund and Hadley Wickham. A.4 for-loops and control flow Read the Iteration chapter of R for Data Science. Also read the Control flow chapter of Advanced R to learn about if-statements. A.5 Building our cross-validation function! Work through the steps below to build up our cross-validation function. Step 1: Create the skeleton body for a function called cross_validation that takes the following input arguments: data: the training dataset (a data.frame object) formula: the model formula for the linear regression model (e.g. resp~x1+x2). This is a special type of R object called (reasonably) a formula object. k: the k in k-fold cross-validation response: a character object giving the name of the response variable Step 2: Devise a way to randomly split the data into \\(k\\) folds. There are many ways to potentially do this, but here’s an idea that I have in mind: Randomly reorder the data. Then add a new “variable” column that repeats 1,2,3,…k,1,2,3,…,k until the last row. If you want to try my idea, look at the help pages for the the sample_n() function in the dplyr package and the rep() function. (You can enter ?dplyr::sample_n and ?rep into the Console.) You’ll need to be comfortable subsetting data.frame’s. Use either the earlier reading, or look at the filter() function available in dplyr. If you come up with another idea and want help implementing it, feel free to ask! Step 3: Set up an output container object to hold the evaluation metric computed in each iteration of cross-validation. Also write the skeleton of a for-loop. Step 4: Write the for-loop body to perform the steps needed to estimate the test error in this iteration. Feel free to use whatever evaluation metric you desire. It may be helpful to write a function for computing that evaluation metric. If you want to add more capability to your function, try the following for some extra challenge: Remove the response argument. Try to extract it from the formula. To help with this, look into the as.character() function, and the str_split() function in the stringr package. Add an argument called metric that will be a character object specifying what evaluation metric to use. You should create functions for each of the metrics that you’ll allow the user to specify. Allow the k argument to be a character object where the user inputs “loocv” instead of a number. Your function should still support numerical k though. A.6 Aside: apply() functions R provides a family of apply() functions (apply(), lapply(), sapply(), tapply(), mapply()) that do similar things to for-loops but tackle specialized tasks. The main feature that is different between these functions and for-loops is that these functions create the output objects from looping automatically. In contrast, for-loops require you to set up a vector container beforehand to store the outputs being created in the loop. You can learn more about the apply() functions here. It is a common misconception in the R community that these functions are faster than for-loops. This isn’t true. People often like apply() functions for their readability, but everyone has their personal preferences. You can see more of the discussion in this issue of R News and on this StackOverflow thread. "]
]
