<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Topic 9 Local Regression and GAMs | MATH 253: Machine Learning</title>
  <meta name="description" content="This is the class activity manual for Math 253 at Macalester College.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Topic 9 Local Regression and GAMs | MATH 253: Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is the class activity manual for Math 253 at Macalester College." />
  <meta name="github-repo" content="lmyint/253_spring_2019" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Topic 9 Local Regression and GAMs | MATH 253: Machine Learning" />
  
  <meta name="twitter:description" content="This is the class activity manual for Math 253 at Macalester College." />
  




  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="splines.html">
<link rel="next" href="logistic-regression.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #2a211c; color: #bdae9d; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; background-color: #2a211c; color: #bdae9d; border-right: 1px solid #bdae9d; }
td.sourceCode { padding-left: 5px; }
pre, code { color: #bdae9d; background-color: #2a211c; }
code > span.kw { color: #43a8ed; font-weight: bold; } /* Keyword */
code > span.dt { text-decoration: underline; } /* DataType */
code > span.dv { color: #44aa43; } /* DecVal */
code > span.bn { color: #44aa43; } /* BaseN */
code > span.fl { color: #44aa43; } /* Float */
code > span.ch { color: #049b0a; } /* Char */
code > span.st { color: #049b0a; } /* String */
code > span.co { color: #0066ff; font-style: italic; } /* Comment */
code > span.al { color: #ffff00; } /* Alert */
code > span.fu { color: #ff9358; font-weight: bold; } /* Function */
code > span.er { color: #ffff00; font-weight: bold; } /* Error */
code > span.wa { color: #ffff00; font-weight: bold; } /* Warning */
code > span.cn { } /* Constant */
code > span.sc { color: #049b0a; } /* SpecialChar */
code > span.vs { color: #049b0a; } /* VerbatimString */
code > span.ss { color: #049b0a; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { } /* Variable */
code > span.cf { color: #43a8ed; font-weight: bold; } /* ControlFlow */
code > span.op { } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { font-weight: bold; } /* Preprocessor */
code > span.at { } /* Attribute */
code > span.do { color: #0066ff; font-style: italic; } /* Documentation */
code > span.an { color: #0066ff; font-weight: bold; font-style: italic; } /* Annotation */
code > span.co { color: #0066ff; font-weight: bold; font-style: italic; } /* Comment */
code > span.in { color: #0066ff; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href = "./">MATH 253: Machine Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="schedule.html"><a href="schedule.html"><i class="fa fa-check"></i>Schedule</a><ul>
<li class="chapter" data-level="" data-path="schedule.html"><a href="schedule.html#tentative-overall-schedule"><i class="fa fa-check"></i>Tentative overall schedule</a></li>
<li class="chapter" data-level="" data-path="schedule.html"><a href="schedule.html#week-2-128---21"><i class="fa fa-check"></i>Week 2: 1/28 - 2/1</a></li>
<li class="chapter" data-level="" data-path="schedule.html"><a href="schedule.html#week-3-24---28"><i class="fa fa-check"></i>Week 3: 2/4 - 2/8</a></li>
<li class="chapter" data-level="" data-path="schedule.html"><a href="schedule.html#week-4-211---215"><i class="fa fa-check"></i>Week 4: 2/11 - 2/15</a></li>
<li class="chapter" data-level="" data-path="schedule.html"><a href="schedule.html#week-5-218---222"><i class="fa fa-check"></i>Week 5: 2/18 - 2/22</a></li>
<li class="chapter" data-level="" data-path="schedule.html"><a href="schedule.html#week-6-225---31"><i class="fa fa-check"></i>Week 6: 2/25 - 3/1</a></li>
<li class="chapter" data-level="" data-path="schedule.html"><a href="schedule.html#week-7-34---38"><i class="fa fa-check"></i>Week 7: 3/4 - 3/8</a></li>
<li class="chapter" data-level="" data-path="schedule.html"><a href="schedule.html#week-8-311---315"><i class="fa fa-check"></i>Week 8: 3/11 - 3/15</a></li>
<li class="chapter" data-level="" data-path="schedule.html"><a href="schedule.html#week-9-325---329"><i class="fa fa-check"></i>Week 9: 3/25 - 3/29</a></li>
<li class="chapter" data-level="" data-path="schedule.html"><a href="schedule.html#week-10-41---45"><i class="fa fa-check"></i>Week 10: 4/1 - 4/5</a></li>
<li class="chapter" data-level="" data-path="schedule.html"><a href="schedule.html#week-11-48---412"><i class="fa fa-check"></i>Week 11: 4/8 - 4/12</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="ml-and-society.html"><a href="ml-and-society.html"><i class="fa fa-check"></i>ML and Society</a></li>
<li class="part"><span><b>I Regression: Model Evaluation</b></span></li>
<li class="chapter" data-level="1" data-path="motivation-and-review.html"><a href="motivation-and-review.html"><i class="fa fa-check"></i><b>1</b> Motivation and Review</a><ul>
<li class="chapter" data-level="1.1" data-path="motivation-and-review.html"><a href="motivation-and-review.html#activity-motivating-main-ideas"><i class="fa fa-check"></i><b>1.1</b> Activity: motivating main ideas</a><ul>
<li class="chapter" data-level="" data-path="motivation-and-review.html"><a href="motivation-and-review.html#situation-a"><i class="fa fa-check"></i>Situation A</a></li>
<li class="chapter" data-level="" data-path="motivation-and-review.html"><a href="motivation-and-review.html#situation-b"><i class="fa fa-check"></i>Situation B</a></li>
<li class="chapter" data-level="" data-path="motivation-and-review.html"><a href="motivation-and-review.html#situation-c"><i class="fa fa-check"></i>Situation C</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="motivation-and-review.html"><a href="motivation-and-review.html#review-exercises"><i class="fa fa-check"></i><b>1.2</b> Review exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="regression-assumptions.html"><a href="regression-assumptions.html"><i class="fa fa-check"></i><b>2</b> Regression Assumptions</a><ul>
<li class="chapter" data-level="2.1" data-path="regression-assumptions.html"><a href="regression-assumptions.html#discussion"><i class="fa fa-check"></i><b>2.1</b> Discussion</a></li>
<li class="chapter" data-level="2.2" data-path="regression-assumptions.html"><a href="regression-assumptions.html#exercises"><i class="fa fa-check"></i><b>2.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="accuracy-metrics-for-regression.html"><a href="accuracy-metrics-for-regression.html"><i class="fa fa-check"></i><b>3</b> Accuracy Metrics for Regression</a><ul>
<li class="chapter" data-level="3.1" data-path="accuracy-metrics-for-regression.html"><a href="accuracy-metrics-for-regression.html#discussion-1"><i class="fa fa-check"></i><b>3.1</b> Discussion</a></li>
<li class="chapter" data-level="3.2" data-path="accuracy-metrics-for-regression.html"><a href="accuracy-metrics-for-regression.html#exercises-1"><i class="fa fa-check"></i><b>3.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="cross-validation.html"><a href="cross-validation.html"><i class="fa fa-check"></i><b>4</b> Cross-Validation</a><ul>
<li class="chapter" data-level="4.1" data-path="cross-validation.html"><a href="cross-validation.html#discussion-2"><i class="fa fa-check"></i><b>4.1</b> Discussion</a></li>
<li class="chapter" data-level="4.2" data-path="cross-validation.html"><a href="cross-validation.html#exercises-2"><i class="fa fa-check"></i><b>4.2</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>II Regression: Model Building</b></span></li>
<li class="chapter" data-level="5" data-path="subset-selection.html"><a href="subset-selection.html"><i class="fa fa-check"></i><b>5</b> Subset Selection</a><ul>
<li class="chapter" data-level="5.1" data-path="subset-selection.html"><a href="subset-selection.html#discussion-3"><i class="fa fa-check"></i><b>5.1</b> Discussion</a></li>
<li class="chapter" data-level="5.2" data-path="subset-selection.html"><a href="subset-selection.html#exercises-3"><i class="fa fa-check"></i><b>5.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="shrinkageregularization.html"><a href="shrinkageregularization.html"><i class="fa fa-check"></i><b>6</b> Shrinkage/Regularization</a><ul>
<li class="chapter" data-level="6.1" data-path="shrinkageregularization.html"><a href="shrinkageregularization.html#discussion-4"><i class="fa fa-check"></i><b>6.1</b> Discussion</a></li>
<li class="chapter" data-level="6.2" data-path="shrinkageregularization.html"><a href="shrinkageregularization.html#exercises-4"><i class="fa fa-check"></i><b>6.2</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>III Regression: More Flexibility</b></span></li>
<li class="chapter" data-level="7" data-path="knn-bias-variance-tradeoff.html"><a href="knn-bias-variance-tradeoff.html"><i class="fa fa-check"></i><b>7</b> KNN &amp; Bias-Variance Tradeoff</a><ul>
<li class="chapter" data-level="7.1" data-path="knn-bias-variance-tradeoff.html"><a href="knn-bias-variance-tradeoff.html#discussion-5"><i class="fa fa-check"></i><b>7.1</b> Discussion</a></li>
<li class="chapter" data-level="7.2" data-path="knn-bias-variance-tradeoff.html"><a href="knn-bias-variance-tradeoff.html#exercises-5"><i class="fa fa-check"></i><b>7.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="splines.html"><a href="splines.html"><i class="fa fa-check"></i><b>8</b> Splines</a><ul>
<li class="chapter" data-level="8.1" data-path="splines.html"><a href="splines.html#discussion-6"><i class="fa fa-check"></i><b>8.1</b> Discussion</a></li>
<li class="chapter" data-level="8.2" data-path="splines.html"><a href="splines.html#exercises-6"><i class="fa fa-check"></i><b>8.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="local-regression-and-gams.html"><a href="local-regression-and-gams.html"><i class="fa fa-check"></i><b>9</b> Local Regression and GAMs</a><ul>
<li class="chapter" data-level="9.1" data-path="local-regression-and-gams.html"><a href="local-regression-and-gams.html#discussion-7"><i class="fa fa-check"></i><b>9.1</b> Discussion</a></li>
<li class="chapter" data-level="9.2" data-path="local-regression-and-gams.html"><a href="local-regression-and-gams.html#exercises-7"><i class="fa fa-check"></i><b>9.2</b> Exercises</a></li>
<li class="chapter" data-level="9.3" data-path="local-regression-and-gams.html"><a href="local-regression-and-gams.html#how-to-choose-between-methods"><i class="fa fa-check"></i><b>9.3</b> How to choose between methods?!?</a></li>
</ul></li>
<li class="part"><span><b>IV Classification</b></span></li>
<li class="chapter" data-level="10" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>10</b> Logistic Regression</a><ul>
<li class="chapter" data-level="10.1" data-path="logistic-regression.html"><a href="logistic-regression.html#discussion-8"><i class="fa fa-check"></i><b>10.1</b> Discussion</a></li>
<li class="chapter" data-level="10.2" data-path="logistic-regression.html"><a href="logistic-regression.html#exercises-8"><i class="fa fa-check"></i><b>10.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="revisiting-old-tools.html"><a href="revisiting-old-tools.html"><i class="fa fa-check"></i><b>11</b> Revisiting Old Tools</a><ul>
<li class="chapter" data-level="11.1" data-path="revisiting-old-tools.html"><a href="revisiting-old-tools.html#discussion-9"><i class="fa fa-check"></i><b>11.1</b> Discussion</a></li>
<li class="chapter" data-level="11.2" data-path="revisiting-old-tools.html"><a href="revisiting-old-tools.html#exercises-9"><i class="fa fa-check"></i><b>11.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>12</b> Decision Trees</a><ul>
<li class="chapter" data-level="12.1" data-path="decision-trees.html"><a href="decision-trees.html#discussion-10"><i class="fa fa-check"></i><b>12.1</b> Discussion</a></li>
<li class="chapter" data-level="12.2" data-path="decision-trees.html"><a href="decision-trees.html#exercises-10"><i class="fa fa-check"></i><b>12.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="bagging-and-random-forests.html"><a href="bagging-and-random-forests.html"><i class="fa fa-check"></i><b>13</b> Bagging and Random Forests</a><ul>
<li class="chapter" data-level="13.1" data-path="bagging-and-random-forests.html"><a href="bagging-and-random-forests.html#exercises-11"><i class="fa fa-check"></i><b>13.1</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="support-vector-machines-part-1.html"><a href="support-vector-machines-part-1.html"><i class="fa fa-check"></i><b>14</b> Support Vector Machines (Part 1)</a><ul>
<li class="chapter" data-level="14.1" data-path="support-vector-machines-part-1.html"><a href="support-vector-machines-part-1.html#exercises-12"><i class="fa fa-check"></i><b>14.1</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="support-vector-machines-part-2.html"><a href="support-vector-machines-part-2.html"><i class="fa fa-check"></i><b>15</b> Support Vector Machines (Part 2)</a><ul>
<li class="chapter" data-level="15.1" data-path="support-vector-machines-part-2.html"><a href="support-vector-machines-part-2.html#exercises-13"><i class="fa fa-check"></i><b>15.1</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>V Unsupervised Learning</b></span></li>
<li class="chapter" data-level="16" data-path="k-means-clustering.html"><a href="k-means-clustering.html"><i class="fa fa-check"></i><b>16</b> K-Means Clustering</a><ul>
<li class="chapter" data-level="16.1" data-path="k-means-clustering.html"><a href="k-means-clustering.html#discussion-11"><i class="fa fa-check"></i><b>16.1</b> Discussion</a></li>
<li class="chapter" data-level="16.2" data-path="k-means-clustering.html"><a href="k-means-clustering.html#exercises-14"><i class="fa fa-check"></i><b>16.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html"><i class="fa fa-check"></i><b>17</b> Hierarchical Clustering</a><ul>
<li class="chapter" data-level="17.1" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#exercises-15"><i class="fa fa-check"></i><b>17.1</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>Review</b></span></li>
<li class="chapter" data-level="" data-path="midterm-review.html"><a href="midterm-review.html"><i class="fa fa-check"></i>Midterm Review</a><ul>
<li class="chapter" data-level="" data-path="midterm-review.html"><a href="midterm-review.html#organizing-information-on-all-methods"><i class="fa fa-check"></i>Organizing information on all methods</a></li>
<li class="chapter" data-level="" data-path="midterm-review.html"><a href="midterm-review.html#review-of-specific-methods"><i class="fa fa-check"></i>Review of specific methods</a><ul>
<li class="chapter" data-level="" data-path="midterm-review.html"><a href="midterm-review.html#subset-selection-1"><i class="fa fa-check"></i>Subset selection</a></li>
<li class="chapter" data-level="" data-path="midterm-review.html"><a href="midterm-review.html#lasso"><i class="fa fa-check"></i>LASSO</a></li>
<li class="chapter" data-level="" data-path="midterm-review.html"><a href="midterm-review.html#splines-1"><i class="fa fa-check"></i>Splines</a></li>
<li class="chapter" data-level="" data-path="midterm-review.html"><a href="midterm-review.html#generalzed-additive-models-gams"><i class="fa fa-check"></i>Generalzed additive models (GAMs)</a></li>
<li class="chapter" data-level="" data-path="midterm-review.html"><a href="midterm-review.html#knn-and-decision-trees"><i class="fa fa-check"></i>KNN and decision trees</a></li>
<li class="chapter" data-level="" data-path="midterm-review.html"><a href="midterm-review.html#logistic-regression-1"><i class="fa fa-check"></i>Logistic regression</a></li>
<li class="chapter" data-level="" data-path="midterm-review.html"><a href="midterm-review.html#bagging-and-random-forests-1"><i class="fa fa-check"></i>Bagging and random forests</a></li>
<li class="chapter" data-level="" data-path="midterm-review.html"><a href="midterm-review.html#test-set-performance"><i class="fa fa-check"></i>Test set performance</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Final Project</b></span></li>
<li class="chapter" data-level="" data-path="final-project-instructions.html"><a href="final-project-instructions.html"><i class="fa fa-check"></i>Final Project Instructions</a><ul>
<li class="chapter" data-level="" data-path="final-project-instructions.html"><a href="final-project-instructions.html#overview"><i class="fa fa-check"></i>Overview</a></li>
<li class="chapter" data-level="" data-path="final-project-instructions.html"><a href="final-project-instructions.html#initial-check-in"><i class="fa fa-check"></i>Initial check-in</a></li>
<li class="chapter" data-level="" data-path="final-project-instructions.html"><a href="final-project-instructions.html#final-report"><i class="fa fa-check"></i>Final report</a></li>
<li class="chapter" data-level="" data-path="final-project-instructions.html"><a href="final-project-instructions.html#final-presentation"><i class="fa fa-check"></i>Final presentation</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="cross-validation-1.html"><a href="cross-validation-1.html"><i class="fa fa-check"></i><b>A</b> Cross-Validation</a><ul>
<li class="chapter" data-level="A.1" data-path="cross-validation-1.html"><a href="cross-validation-1.html#objects"><i class="fa fa-check"></i><b>A.1</b> Objects</a></li>
<li class="chapter" data-level="A.2" data-path="cross-validation-1.html"><a href="cross-validation-1.html#subsetting"><i class="fa fa-check"></i><b>A.2</b> Subsetting</a></li>
<li class="chapter" data-level="A.3" data-path="cross-validation-1.html"><a href="cross-validation-1.html#writing-r-functions"><i class="fa fa-check"></i><b>A.3</b> Writing R functions</a></li>
<li class="chapter" data-level="A.4" data-path="cross-validation-1.html"><a href="cross-validation-1.html#for-loops-and-control-flow"><i class="fa fa-check"></i><b>A.4</b> <code>for</code>-loops and control flow</a></li>
<li class="chapter" data-level="A.5" data-path="cross-validation-1.html"><a href="cross-validation-1.html#building-our-cross-validation-function"><i class="fa fa-check"></i><b>A.5</b> Building our cross-validation function!</a></li>
<li class="chapter" data-level="A.6" data-path="cross-validation-1.html"><a href="cross-validation-1.html#aside-apply-functions"><i class="fa fa-check"></i><b>A.6</b> Aside: <code>apply()</code> functions</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="splines-2.html"><a href="splines-2.html"><i class="fa fa-check"></i><b>B</b> Splines</a><ul>
<li class="chapter" data-level="B.1" data-path="splines-2.html"><a href="splines-2.html#exercise"><i class="fa fa-check"></i><b>B.1</b> Exercise</a></li>
<li class="chapter" data-level="B.2" data-path="splines-2.html"><a href="splines-2.html#debriefing"><i class="fa fa-check"></i><b>B.2</b> Debriefing</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MATH 253: Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="local-regression-and-gams" class="section level1">
<h1><span class="header-section-number">Topic 9</span> Local Regression and GAMs</h1>
<div id="discussion-7" class="section level2">
<h2><span class="header-section-number">9.1</span> Discussion</h2>
<p><strong>Local regression</strong></p>
<ul>
<li>Main tool: LOESS = locally estimated scatterplot smoothing</li>
<li>Fit local linear regression models, using only a subset of the data</li>
<li>How is this different from KNN regression?</li>
</ul>
<pre><code>## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39;
## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39;</code></pre>
<p><img src="math253_manual_files/figure-html/unnamed-chunk-80-1.png" width="960" style="display: block; margin: auto;" /></p>
<p><br> <br> <br></p>
<p><strong>Generalized additive models</strong></p>
<p>Instead of assuming <strong>linear</strong> relationships between predictors and the response…</p>
<p><span class="math display">\[\text{Outstate} = \beta_0 + \beta_1\text{Private} + \beta_2\text{PhD} + \beta_3\text{Expend} + \varepsilon\]</span></p>
<p>…let’s be more general and say that the relationships can be arbitrary functions:</p>
<p><span class="math display">\[\text{Outstate} = \beta_0 + f_1(\text{Private}) + f_2(\text{PhD}) + f_3(\text{Expend}) + \varepsilon\]</span></p>
<ul>
<li>If the functions <span class="math inline">\(f_1, f_2, f_3\)</span> can be described with splines, then the model is just like an ordinary linear regression model and can be fit with least squares.</li>
<li>We can also have these functions be described with LOESS functions.</li>
</ul>
<p><br> <br> <br></p>
</div>
<div id="exercises-7" class="section level2">
<h2><span class="header-section-number">9.2</span> Exercises</h2>
<ol style="list-style-type: decimal">
<li><strong>Writing pseudocode for LOESS</strong><br />
Pseudocode is an algorithm in words. Write pseudocode for the LOESS algorithm assuming that you are supplied the predictor <span class="math inline">\(x\)</span>, response <span class="math inline">\(y\)</span>, a span of 0.4. Your algorithm will use local <em>linear</em> fits. Assume that ordinary least squares rather than weighted least squares is used.<br />
Start from the following:
<ul>
<li>Set up a grid of <span class="math inline">\(x\)</span> values from the minimum to maximum <span class="math inline">\(x\)</span></li>
<li>For each <span class="math inline">\(x\)</span> in the grid:
<ul>
<li>Step 1</li>
<li>Step 2…</li>
</ul></li>
</ul></li>
</ol>
<p><br> <br> <br></p>
<ol start="2" style="list-style-type: decimal">
<li><p><strong>The LOESS tuning parameter</strong><br />
When you use <code>geom_smooth</code> in <code>ggplot</code> the smooth line is drawn by LOESS by default. The main tuning parameter we modify is <code>span</code>. The <code>span</code> gives the percent of data used in the local linear fit.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(College, <span class="kw">aes</span>(<span class="dt">x =</span> Expend, <span class="dt">y =</span> Outstate)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_smooth</span>(<span class="dt">span =</span> <span class="fl">0.1</span>, <span class="dt">lwd =</span> <span class="dv">3</span>) <span class="op">+</span><span class="st"> </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Span: 0.1&quot;</span>)
<span class="kw">ggplot</span>(College, <span class="kw">aes</span>(<span class="dt">x =</span> Expend, <span class="dt">y =</span> Outstate)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_smooth</span>(<span class="dt">span =</span> <span class="fl">0.4</span>, <span class="dt">lwd =</span> <span class="dv">3</span>) <span class="op">+</span><span class="st"> </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Span: 0.4&quot;</span>)
<span class="kw">ggplot</span>(College, <span class="kw">aes</span>(<span class="dt">x =</span> Expend, <span class="dt">y =</span> Outstate)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_smooth</span>(<span class="dt">span =</span> <span class="fl">0.8</span>, <span class="dt">lwd =</span> <span class="dv">3</span>) <span class="op">+</span><span class="st"> </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Span: 0.8&quot;</span>)</code></pre></div>
<img src="math253_manual_files/figure-html/unnamed-chunk-82-1.png" width="960" style="display: block; margin: auto;" />
<ol style="list-style-type: lower-alpha">
<li>Put <code>span</code> on our bias-variance tradeoff diagram along with labels for bias, variance, complexity, and flexibility.</li>
<li>How would you expect a plot of test RMSE versus <code>span</code> to look? Why?</li>
</ol></li>
</ol>
<p><br> <br> <br></p>
<ol start="3" style="list-style-type: decimal">
<li><p><strong>GAMs</strong><br />
We can fit GAMs in R by installing the <code>gam</code> package. Within <code>caret</code>, we can specify <code>gamLoess</code> for the method. We specify two values for the <code>span</code> tuning parameter: 0.4 and 0.5. We also have to say the degree of the local polynomial that we’re fitting in each small window: degree=1 for a linear fit.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caret)
<span class="kw">set.seed</span>(<span class="dv">2019</span>)
gam_mod &lt;-<span class="st"> </span><span class="kw">train</span>(
    Outstate <span class="op">~</span><span class="st"> </span>Private <span class="op">+</span><span class="st"> </span>Room.Board <span class="op">+</span><span class="st"> </span>PhD <span class="op">+</span><span class="st"> </span>perc.alumni <span class="op">+</span><span class="st"> </span>Expend,
    <span class="dt">data =</span> College,
    <span class="dt">method =</span> <span class="st">&quot;gamLoess&quot;</span>,
    <span class="dt">tuneGrid =</span> <span class="kw">data.frame</span>(<span class="dt">span =</span> <span class="kw">c</span>(<span class="fl">0.4</span>, <span class="fl">0.5</span>), <span class="dt">degree =</span> <span class="dv">1</span>),
    <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">10</span>),
    <span class="dt">metric =</span> <span class="st">&quot;RMSE&quot;</span>,
    <span class="dt">na.action =</span> na.omit
)</code></pre></div>
<p>You can plot each LOESS component by calling the <code>plot()</code> function on the <code>finalModel</code> component <code>gam_mod</code>. (This is the model with the best set of tuning parameters.) This will produce a sequence of 5 plots, each illustrating a different smooth <span class="math inline">\(\hat{f}_i(x_i)\)</span> that describes how out-of-state tuition changes with that predictor, <em>holding constant</em> the other predictors. The dotted lines show 2 standard errors.<br />
That is, pick any 2 points on a plot. The difference in <span class="math inline">\(y\)</span> values gives the change in out-of-state tuition associated with the change in <span class="math inline">\(x\)</span> values, holding constant the other predictors.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">3</span>)) <span class="co"># Set up a plot grid with 2 rows and 3 cols</span>
<span class="kw">plot</span>(gam_mod<span class="op">$</span>finalModel, <span class="dt">se =</span> <span class="ot">TRUE</span>, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">lwd =</span> <span class="dv">3</span>) <span class="co"># lwd = line width</span></code></pre></div>
<p><img src="math253_manual_files/figure-html/unnamed-chunk-84-1.png" width="960" style="display: block; margin: auto;" /></p>
<ol style="list-style-type: lower-alpha">
<li>Pick 1 or 2 of these plots and interpret your findings in context. Anything surprising or interesting?</li>
<li><p>If instead of LOESS representations for the 4 quantitative variables, we wanted spline representations, we can fit a least squares model with spline terms. We can then view the spline components using <code>plot.Gam()</code> from the <code>GAM</code> package.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(gam) <span class="co"># Load for plot.Gam()</span>
spline_mod &lt;-<span class="st"> </span><span class="kw">lm</span>(Outstate <span class="op">~</span><span class="st"> </span>Private <span class="op">+</span><span class="st"> </span><span class="kw">ns</span>(Room.Board, <span class="dt">df=</span><span class="dv">4</span>) <span class="op">+</span><span class="st"> </span><span class="kw">ns</span>(PhD, <span class="dt">df=</span><span class="dv">4</span>) <span class="op">+</span><span class="st"> </span><span class="kw">ns</span>(perc.alumni, <span class="dt">df=</span><span class="dv">4</span>) <span class="op">+</span><span class="st"> </span><span class="kw">ns</span>(Expend, <span class="dt">df=</span><span class="dv">4</span>), <span class="dt">data =</span> College)
<span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">3</span>))
<span class="kw">plot.Gam</span>(spline_mod, <span class="dt">se =</span> <span class="ot">TRUE</span>, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">lwd =</span> <span class="dv">3</span>)</code></pre></div>
<p><img src="math253_manual_files/figure-html/unnamed-chunk-85-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>How do the spline and LOESS components compare?</p></li>
</ol></li>
</ol>
<p><br> <br> <br> <br></p>
<p><strong>Note:</strong> If you want to read about how GAMs are fit, you can read about the <strong>backfitting algorithm</strong> in ISLR Section 7.7. When we want LOESS representations for the individual predictor functions, backfitting must be performed. Backfitting can sometimes run into computational issues but is fine most of the time. Least squares is more stable overall, and we can use least squares if we want spline representations of the individual predictor functions. The decision making point is whether we want spline or LOESS representations.</p>
</div>
<div id="how-to-choose-between-methods" class="section level2">
<h2><span class="header-section-number">9.3</span> How to choose between methods?!?</h2>
<p>We’ve covered many tools. Let’s unpack their pros and cons in different settings as well as some ideas that will carry on as we talk about classification.</p>
<pre><code>Ordinary linear regression
    |                                   GAMs and LOESS
    |                                     |   
    |                                     |           KNN
&lt;---|-------------------------------------|------------|-------&gt;
parametric                                          nonparametric</code></pre>
<p><strong>KNN regression</strong></p>
<ul>
<li>I introduced KNN as a means of introducing the idea of nonparametric methods. The idea of nonparametric methods is going to carry through to classification, coming up next!</li>
<li>Pros:
<ul>
<li>KNN is a nonparametric method, which might be preferable if we don’t believe that a parametric model, such as a linear regression model (even with spline terms), holds or if we don’t want to assume a parametric model. How to check? Revisit our regression assumptions! Fit a model with the potential predictors, and look at diagnostic plots.</li>
</ul></li>
<li>Cons:
<ul>
<li>Look back at the video to visualize the functions learned by KNN for different values of K. Even with large K, the function learned looks “blocky” and has sharp angles. Thus, KNN doesn’t look smooth until we increase K a lot, but at that point, we might suffer from too much bias in the bias-variance tradeoff.</li>
<li>The curse of dimensionality. This curse may seem kind of odd in that, normally, we want a lot of predictors. But as we’ve seen having a lot of predictors, makes distances between cases potentially quite large, rendering the nearest neighbors to a case actually quite dissimilar. But we can save ourselves in two ways:
<ul>
<li>Much bigger sample size! Sometimes this is feasible.</li>
<li>Reduce dimensionality! The technique of <strong>dimension reduction</strong> is an unsupervised learning technique that combines existing predictors into a smaller set of predictors. We’ll talk about a technique called PCA later in the semester.</li>
</ul></li>
</ul></li>
</ul>
<p><strong>Splines</strong></p>
<ul>
<li>Pros:
<ul>
<li>Splines pretty nicely cover all sorts of nonlinear trends and are computationally very attractive because spline terms fit exactly into a least squares linear regression framework. Least squares models are very easy to fit computationally.</li>
</ul></li>
<li>Cons:
<ul>
<li>Splines are still under the umbrella of a parametric model, because we assume the form <span class="math inline">\(y = \beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p\)</span>. Really, this is a tiny con. Splines are capable of being quite flexible given enough knots. In this sense, splines are pretty nonparametric even though they outwardly take on a parametric-looking form.</li>
<li>It is possible to create multidimensional splines by creating interactions between spline terms for different predictors. This suffers from the curse of dimensionality like KNN because we are trying to estimate a wavy surface in a large dimensional (many variable) space where data points will only sparsely cover the many many regions of the space.</li>
</ul></li>
</ul>
<p><strong>LOESS</strong></p>
<ul>
<li>Pros:
<ul>
<li>I find LOESS very interpretable. At each test case, a local linear model is fit, which fits well with the idea that if you zoom enough into any function (no matter how curvy), you’ll see a line.</li>
<li>LOESS is a popular way to see smooth trends on scatterplots, which is why <code>ggplot</code> uses it in <code>geom_smooth()</code>.</li>
</ul></li>
<li>Cons:
<ul>
<li>If there are a lot of data points, fitting a LOESS over the entire range of the predictor can be slow because so many local linear regressions must be fit.<br />
We can avoid this if we don’t actually need the fit over the entire range. If we want to just make a prediction about the response for a test case, we only need to fit the local regression at the x value for that test case.</li>
</ul></li>
</ul>
<p><strong>GAMs</strong></p>
<ul>
<li>Whether the indvidual functions within a GAM are represented with LOESS or splines, the models are still generally called GAMs. But when you describe your model, you should say whether you are using a LOESS or spline representation</li>
<li>Pros:
<ul>
<li>GAMs have the interpretability of linear regresson models (relationships for one predictor hold constant the other predictors), and they also flexibly model nonlinearity.</li>
</ul></li>
<li>Cons:
<ul>
<li>GAM still make the assumption that you add up the individual functions of the predictors (linear combination of the functions). (Why add? Why not a product or quotient?) Still, this additivity allows for its interpretability, which is a definite plus.</li>
</ul></li>
</ul>

</div>
</div>



            </section>

          </div>
        </div>
      </div>
<a href="splines.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="logistic-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
