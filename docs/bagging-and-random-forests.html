<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Topic 13 Bagging and Random Forests | MATH 253: Machine Learning</title>
  <meta name="description" content="This is the class activity manual for Math 253 at Macalester College.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Topic 13 Bagging and Random Forests | MATH 253: Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is the class activity manual for Math 253 at Macalester College." />
  <meta name="github-repo" content="lmyint/253_spring_2019" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Topic 13 Bagging and Random Forests | MATH 253: Machine Learning" />
  
  <meta name="twitter:description" content="This is the class activity manual for Math 253 at Macalester College." />
  




  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="decision-trees.html">
<link rel="next" href="support-vector-machines-part-1.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #2a211c; color: #bdae9d; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; background-color: #2a211c; color: #bdae9d; border-right: 1px solid #bdae9d; }
td.sourceCode { padding-left: 5px; }
pre, code { color: #bdae9d; background-color: #2a211c; }
code > span.kw { color: #43a8ed; font-weight: bold; } /* Keyword */
code > span.dt { text-decoration: underline; } /* DataType */
code > span.dv { color: #44aa43; } /* DecVal */
code > span.bn { color: #44aa43; } /* BaseN */
code > span.fl { color: #44aa43; } /* Float */
code > span.ch { color: #049b0a; } /* Char */
code > span.st { color: #049b0a; } /* String */
code > span.co { color: #0066ff; font-style: italic; } /* Comment */
code > span.al { color: #ffff00; } /* Alert */
code > span.fu { color: #ff9358; font-weight: bold; } /* Function */
code > span.er { color: #ffff00; font-weight: bold; } /* Error */
code > span.wa { color: #ffff00; font-weight: bold; } /* Warning */
code > span.cn { } /* Constant */
code > span.sc { color: #049b0a; } /* SpecialChar */
code > span.vs { color: #049b0a; } /* VerbatimString */
code > span.ss { color: #049b0a; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { } /* Variable */
code > span.cf { color: #43a8ed; font-weight: bold; } /* ControlFlow */
code > span.op { } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { font-weight: bold; } /* Preprocessor */
code > span.at { } /* Attribute */
code > span.do { color: #0066ff; font-style: italic; } /* Documentation */
code > span.an { color: #0066ff; font-weight: bold; font-style: italic; } /* Annotation */
code > span.co { color: #0066ff; font-weight: bold; font-style: italic; } /* Comment */
code > span.in { color: #0066ff; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href = "./">MATH 253: Machine Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="schedule.html"><a href="schedule.html"><i class="fa fa-check"></i>Schedule</a><ul>
<li class="chapter" data-level="" data-path="schedule.html"><a href="schedule.html#tentative-overall-schedule"><i class="fa fa-check"></i>Tentative overall schedule</a></li>
<li class="chapter" data-level="" data-path="schedule.html"><a href="schedule.html#week-2-128---21"><i class="fa fa-check"></i>Week 2: 1/28 - 2/1</a></li>
<li class="chapter" data-level="" data-path="schedule.html"><a href="schedule.html#week-3-24---28"><i class="fa fa-check"></i>Week 3: 2/4 - 2/8</a></li>
<li class="chapter" data-level="" data-path="schedule.html"><a href="schedule.html#week-4-211---215"><i class="fa fa-check"></i>Week 4: 2/11 - 2/15</a></li>
<li class="chapter" data-level="" data-path="schedule.html"><a href="schedule.html#week-5-218---222"><i class="fa fa-check"></i>Week 5: 2/18 - 2/22</a></li>
<li class="chapter" data-level="" data-path="schedule.html"><a href="schedule.html#week-6-225---31"><i class="fa fa-check"></i>Week 6: 2/25 - 3/1</a></li>
<li class="chapter" data-level="" data-path="schedule.html"><a href="schedule.html#week-7-34---38"><i class="fa fa-check"></i>Week 7: 3/4 - 3/8</a></li>
<li class="chapter" data-level="" data-path="schedule.html"><a href="schedule.html#week-8-311---315"><i class="fa fa-check"></i>Week 8: 3/11 - 3/15</a></li>
<li class="chapter" data-level="" data-path="schedule.html"><a href="schedule.html#week-9-325---329"><i class="fa fa-check"></i>Week 9: 3/25 - 3/29</a></li>
<li class="chapter" data-level="" data-path="schedule.html"><a href="schedule.html#week-10-41---45"><i class="fa fa-check"></i>Week 10: 4/1 - 4/5</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="ml-and-society.html"><a href="ml-and-society.html"><i class="fa fa-check"></i>ML and Society</a></li>
<li class="part"><span><b>I Regression: Model Evaluation</b></span></li>
<li class="chapter" data-level="1" data-path="motivation-and-review.html"><a href="motivation-and-review.html"><i class="fa fa-check"></i><b>1</b> Motivation and Review</a><ul>
<li class="chapter" data-level="1.1" data-path="motivation-and-review.html"><a href="motivation-and-review.html#activity-motivating-main-ideas"><i class="fa fa-check"></i><b>1.1</b> Activity: motivating main ideas</a><ul>
<li class="chapter" data-level="" data-path="motivation-and-review.html"><a href="motivation-and-review.html#situation-a"><i class="fa fa-check"></i>Situation A</a></li>
<li class="chapter" data-level="" data-path="motivation-and-review.html"><a href="motivation-and-review.html#situation-b"><i class="fa fa-check"></i>Situation B</a></li>
<li class="chapter" data-level="" data-path="motivation-and-review.html"><a href="motivation-and-review.html#situation-c"><i class="fa fa-check"></i>Situation C</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="motivation-and-review.html"><a href="motivation-and-review.html#review-exercises"><i class="fa fa-check"></i><b>1.2</b> Review exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="regression-assumptions.html"><a href="regression-assumptions.html"><i class="fa fa-check"></i><b>2</b> Regression Assumptions</a><ul>
<li class="chapter" data-level="2.1" data-path="regression-assumptions.html"><a href="regression-assumptions.html#discussion"><i class="fa fa-check"></i><b>2.1</b> Discussion</a></li>
<li class="chapter" data-level="2.2" data-path="regression-assumptions.html"><a href="regression-assumptions.html#exercises"><i class="fa fa-check"></i><b>2.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="accuracy-metrics-for-regression.html"><a href="accuracy-metrics-for-regression.html"><i class="fa fa-check"></i><b>3</b> Accuracy Metrics for Regression</a><ul>
<li class="chapter" data-level="3.1" data-path="accuracy-metrics-for-regression.html"><a href="accuracy-metrics-for-regression.html#discussion-1"><i class="fa fa-check"></i><b>3.1</b> Discussion</a></li>
<li class="chapter" data-level="3.2" data-path="accuracy-metrics-for-regression.html"><a href="accuracy-metrics-for-regression.html#exercises-1"><i class="fa fa-check"></i><b>3.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="cross-validation.html"><a href="cross-validation.html"><i class="fa fa-check"></i><b>4</b> Cross-Validation</a><ul>
<li class="chapter" data-level="4.1" data-path="cross-validation.html"><a href="cross-validation.html#discussion-2"><i class="fa fa-check"></i><b>4.1</b> Discussion</a></li>
<li class="chapter" data-level="4.2" data-path="cross-validation.html"><a href="cross-validation.html#exercises-2"><i class="fa fa-check"></i><b>4.2</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>II Regression: Model Building</b></span></li>
<li class="chapter" data-level="5" data-path="subset-selection.html"><a href="subset-selection.html"><i class="fa fa-check"></i><b>5</b> Subset Selection</a><ul>
<li class="chapter" data-level="5.1" data-path="subset-selection.html"><a href="subset-selection.html#discussion-3"><i class="fa fa-check"></i><b>5.1</b> Discussion</a></li>
<li class="chapter" data-level="5.2" data-path="subset-selection.html"><a href="subset-selection.html#exercises-3"><i class="fa fa-check"></i><b>5.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="shrinkageregularization.html"><a href="shrinkageregularization.html"><i class="fa fa-check"></i><b>6</b> Shrinkage/Regularization</a><ul>
<li class="chapter" data-level="6.1" data-path="shrinkageregularization.html"><a href="shrinkageregularization.html#discussion-4"><i class="fa fa-check"></i><b>6.1</b> Discussion</a></li>
<li class="chapter" data-level="6.2" data-path="shrinkageregularization.html"><a href="shrinkageregularization.html#exercises-4"><i class="fa fa-check"></i><b>6.2</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>III Regression: More Flexibility</b></span></li>
<li class="chapter" data-level="7" data-path="knn-bias-variance-tradeoff.html"><a href="knn-bias-variance-tradeoff.html"><i class="fa fa-check"></i><b>7</b> KNN &amp; Bias-Variance Tradeoff</a><ul>
<li class="chapter" data-level="7.1" data-path="knn-bias-variance-tradeoff.html"><a href="knn-bias-variance-tradeoff.html#discussion-5"><i class="fa fa-check"></i><b>7.1</b> Discussion</a></li>
<li class="chapter" data-level="7.2" data-path="knn-bias-variance-tradeoff.html"><a href="knn-bias-variance-tradeoff.html#exercises-5"><i class="fa fa-check"></i><b>7.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="splines.html"><a href="splines.html"><i class="fa fa-check"></i><b>8</b> Splines</a><ul>
<li class="chapter" data-level="8.1" data-path="splines.html"><a href="splines.html#discussion-6"><i class="fa fa-check"></i><b>8.1</b> Discussion</a></li>
<li class="chapter" data-level="8.2" data-path="splines.html"><a href="splines.html#exercises-6"><i class="fa fa-check"></i><b>8.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="local-regression-and-gams.html"><a href="local-regression-and-gams.html"><i class="fa fa-check"></i><b>9</b> Local Regression and GAMs</a><ul>
<li class="chapter" data-level="9.1" data-path="local-regression-and-gams.html"><a href="local-regression-and-gams.html#discussion-7"><i class="fa fa-check"></i><b>9.1</b> Discussion</a></li>
<li class="chapter" data-level="9.2" data-path="local-regression-and-gams.html"><a href="local-regression-and-gams.html#exercises-7"><i class="fa fa-check"></i><b>9.2</b> Exercises</a></li>
<li class="chapter" data-level="9.3" data-path="local-regression-and-gams.html"><a href="local-regression-and-gams.html#how-to-choose-between-methods"><i class="fa fa-check"></i><b>9.3</b> How to choose between methods?!?</a></li>
</ul></li>
<li class="part"><span><b>IV Classification</b></span></li>
<li class="chapter" data-level="10" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>10</b> Logistic Regression</a><ul>
<li class="chapter" data-level="10.1" data-path="logistic-regression.html"><a href="logistic-regression.html#discussion-8"><i class="fa fa-check"></i><b>10.1</b> Discussion</a></li>
<li class="chapter" data-level="10.2" data-path="logistic-regression.html"><a href="logistic-regression.html#exercises-8"><i class="fa fa-check"></i><b>10.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="revisiting-old-tools.html"><a href="revisiting-old-tools.html"><i class="fa fa-check"></i><b>11</b> Revisiting Old Tools</a><ul>
<li class="chapter" data-level="11.1" data-path="revisiting-old-tools.html"><a href="revisiting-old-tools.html#discussion-9"><i class="fa fa-check"></i><b>11.1</b> Discussion</a></li>
<li class="chapter" data-level="11.2" data-path="revisiting-old-tools.html"><a href="revisiting-old-tools.html#exercises-9"><i class="fa fa-check"></i><b>11.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>12</b> Decision Trees</a><ul>
<li class="chapter" data-level="12.1" data-path="decision-trees.html"><a href="decision-trees.html#discussion-10"><i class="fa fa-check"></i><b>12.1</b> Discussion</a></li>
<li class="chapter" data-level="12.2" data-path="decision-trees.html"><a href="decision-trees.html#exercises-10"><i class="fa fa-check"></i><b>12.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="bagging-and-random-forests.html"><a href="bagging-and-random-forests.html"><i class="fa fa-check"></i><b>13</b> Bagging and Random Forests</a><ul>
<li class="chapter" data-level="13.1" data-path="bagging-and-random-forests.html"><a href="bagging-and-random-forests.html#exercises-11"><i class="fa fa-check"></i><b>13.1</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="support-vector-machines-part-1.html"><a href="support-vector-machines-part-1.html"><i class="fa fa-check"></i><b>14</b> Support Vector Machines (Part 1)</a><ul>
<li class="chapter" data-level="14.1" data-path="support-vector-machines-part-1.html"><a href="support-vector-machines-part-1.html#exercises-12"><i class="fa fa-check"></i><b>14.1</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>Review</b></span></li>
<li class="chapter" data-level="" data-path="midterm-review.html"><a href="midterm-review.html"><i class="fa fa-check"></i>Midterm Review</a><ul>
<li class="chapter" data-level="" data-path="midterm-review.html"><a href="midterm-review.html#organizing-information-on-all-methods"><i class="fa fa-check"></i>Organizing information on all methods</a></li>
<li class="chapter" data-level="" data-path="midterm-review.html"><a href="midterm-review.html#review-of-specific-methods"><i class="fa fa-check"></i>Review of specific methods</a><ul>
<li class="chapter" data-level="" data-path="midterm-review.html"><a href="midterm-review.html#subset-selection-1"><i class="fa fa-check"></i>Subset selection</a></li>
<li class="chapter" data-level="" data-path="midterm-review.html"><a href="midterm-review.html#lasso"><i class="fa fa-check"></i>LASSO</a></li>
<li class="chapter" data-level="" data-path="midterm-review.html"><a href="midterm-review.html#splines-1"><i class="fa fa-check"></i>Splines</a></li>
<li class="chapter" data-level="" data-path="midterm-review.html"><a href="midterm-review.html#generalzed-additive-models-gams"><i class="fa fa-check"></i>Generalzed additive models (GAMs)</a></li>
<li class="chapter" data-level="" data-path="midterm-review.html"><a href="midterm-review.html#knn-and-decision-trees"><i class="fa fa-check"></i>KNN and decision trees</a></li>
<li class="chapter" data-level="" data-path="midterm-review.html"><a href="midterm-review.html#logistic-regression-1"><i class="fa fa-check"></i>Logistic regression</a></li>
<li class="chapter" data-level="" data-path="midterm-review.html"><a href="midterm-review.html#bagging-and-random-forests-1"><i class="fa fa-check"></i>Bagging and random forests</a></li>
<li class="chapter" data-level="" data-path="midterm-review.html"><a href="midterm-review.html#test-set-performance"><i class="fa fa-check"></i>Test set performance</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Final Project</b></span></li>
<li class="chapter" data-level="" data-path="final-project-instructions.html"><a href="final-project-instructions.html"><i class="fa fa-check"></i>Final Project Instructions</a><ul>
<li class="chapter" data-level="" data-path="final-project-instructions.html"><a href="final-project-instructions.html#overview"><i class="fa fa-check"></i>Overview</a></li>
<li class="chapter" data-level="" data-path="final-project-instructions.html"><a href="final-project-instructions.html#initial-check-in"><i class="fa fa-check"></i>Initial check-in</a></li>
<li class="chapter" data-level="" data-path="final-project-instructions.html"><a href="final-project-instructions.html#final-report"><i class="fa fa-check"></i>Final report</a></li>
<li class="chapter" data-level="" data-path="final-project-instructions.html"><a href="final-project-instructions.html#final-presentation"><i class="fa fa-check"></i>Final presentation</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="cross-validation-1.html"><a href="cross-validation-1.html"><i class="fa fa-check"></i><b>A</b> Cross-Validation</a><ul>
<li class="chapter" data-level="A.1" data-path="cross-validation-1.html"><a href="cross-validation-1.html#objects"><i class="fa fa-check"></i><b>A.1</b> Objects</a></li>
<li class="chapter" data-level="A.2" data-path="cross-validation-1.html"><a href="cross-validation-1.html#subsetting"><i class="fa fa-check"></i><b>A.2</b> Subsetting</a></li>
<li class="chapter" data-level="A.3" data-path="cross-validation-1.html"><a href="cross-validation-1.html#writing-r-functions"><i class="fa fa-check"></i><b>A.3</b> Writing R functions</a></li>
<li class="chapter" data-level="A.4" data-path="cross-validation-1.html"><a href="cross-validation-1.html#for-loops-and-control-flow"><i class="fa fa-check"></i><b>A.4</b> <code>for</code>-loops and control flow</a></li>
<li class="chapter" data-level="A.5" data-path="cross-validation-1.html"><a href="cross-validation-1.html#building-our-cross-validation-function"><i class="fa fa-check"></i><b>A.5</b> Building our cross-validation function!</a></li>
<li class="chapter" data-level="A.6" data-path="cross-validation-1.html"><a href="cross-validation-1.html#aside-apply-functions"><i class="fa fa-check"></i><b>A.6</b> Aside: <code>apply()</code> functions</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="splines-2.html"><a href="splines-2.html"><i class="fa fa-check"></i><b>B</b> Splines</a><ul>
<li class="chapter" data-level="B.1" data-path="splines-2.html"><a href="splines-2.html#exercise"><i class="fa fa-check"></i><b>B.1</b> Exercise</a></li>
<li class="chapter" data-level="B.2" data-path="splines-2.html"><a href="splines-2.html#debriefing"><i class="fa fa-check"></i><b>B.2</b> Debriefing</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MATH 253: Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="bagging-and-random-forests" class="section level1">
<h1><span class="header-section-number">Topic 13</span> Bagging and Random Forests</h1>
<div id="exercises-11" class="section level2">
<h2><span class="header-section-number">13.1</span> Exercises</h2>
<p><strong>You can download a template RMarkdown file to start from <a href="template_rmds/13-bagging-rf.Rmd">here</a>.</strong></p>
<p>Install the <code>randomForest</code> package in the Console before starting these exercises.</p>
<p>We will continue looking at high resolution aerial image data to classify different parts of images into different types of urban land cover. Data from the <a href="https://archive.ics.uci.edu/ml/datasets/Urban+Land+Cover">UCI Machine Learning Repository</a> include the observed type of land cover (determined by human visual inspection) and “spectral, size, shape, and texture information” computed from the image. You will be predicting the <code>class</code> variable which gives the type of urban land cover.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load required packages</span>
<span class="kw">library</span>(dplyr)
<span class="kw">library</span>(rpart)
<span class="kw">library</span>(randomForest)
<span class="kw">library</span>(caret)

<span class="co"># Read in data</span>
land &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;https://www.macalester.edu/~ajohns24/data/land_cover.csv&quot;</span>)
land &lt;-<span class="st"> </span>land <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">filter</span>(class <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;asphalt &quot;</span>,<span class="st">&quot;grass &quot;</span>, <span class="st">&quot;tree &quot;</span>)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">mutate</span>(<span class="dt">class =</span> <span class="kw">droplevels</span>(class))</code></pre></div>
<p><br> <br> <br></p>
<ol style="list-style-type: decimal">
<li>Fit a random forest with <code>caret</code> with 10-fold CV, and call this object <code>rf_mod_cv</code>. This time the code is not provided. Try to figure out how to construct the <code>train()</code> command by looking back at previous examples from other methods. Some guidance:
<ul>
<li>Visit <code>caret</code>’s (extensive) <a href="https://topepo.github.io/caret/available-models.html">manual</a> and search for “random forest” in the top right search bar. There will be <em>many</em> results. Find the entry that only has <code>randomForest</code> listed in the “Libraries” column. (The instructor is not familiar with the other packages.) Pay attention to the “method Value” column.</li>
<li>Also pay attention to the “Tuning Parameters” column. <strong>Answer these next 2 questions in your homework:</strong> What is the tuning parameter called? What does this stand for? (To answer the second question, look up the documentation for the <code>randomForest()</code> function by entering <code>?randomForest</code> in the Console.)</li>
<li>Don’t worry about the <code>tuneGrid</code> option for now. If you don’t supply this, <code>caret</code> picks sensible default values for varying the tuning parameter.</li>
<li>(In general) Don’t worry about including <code>na.action</code> unless you end up getting an error message that has something to do with <code>NA</code>’s.</li>
</ul></li>
</ol>
<p><br> <br> <br></p>
<ol start="2" style="list-style-type: decimal">
<li><p>Now that you have your <code>train()</code> command set up, let’s time how long it takes to execute. Wrap your entire command within the <code>system.time()</code> function as below:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">system.time</span>({
rf_mod_cv &lt;-<span class="st"> </span><span class="kw">train</span>(???)
})</code></pre></div>
<p>The number under <code>elapsed</code> gives the runtime in seconds.</p></li>
</ol>
<p><br> <br> <br></p>
<ol start="3" style="list-style-type: decimal">
<li>Ah! But as discussed in the video, OOB is another option. Figure out how to modify the <code>trControl</code> part of your command to use OOB error estimation (look up the documentation for <code>caret</code>’s <code>trainControl()</code> function), and save this new object as <code>rf_mod_oob</code>. Also time your command with <code>system.time()</code> as above. How does the runtime here compare to using CV?</li>
</ol>
<p><br> <br> <br></p>
<ol start="4" style="list-style-type: decimal">
<li>Print <code>rf_mod_oob</code> and <code>rf_mod_cv</code>. (Just type the object names. Running this code displays some summary output for these models that <code>caret</code> has computed.)
<ol style="list-style-type: lower-alpha">
<li>How do the estimated test accuracies compare between the two test error estimation methods?</li>
<li>Now let’s just focus on <code>rf_mod_oob</code>. What value of the tuning parameter corresponds to performing <strong>just bagging</strong> (bagging but not random forests)? Why?</li>
<li>Plot estimated test accuracy versus the tuning parameter. (For any object resulting from <code>caret</code>’s <code>train()</code> function, you can put that object inside <code>plot()</code> to make such a plot. That is, you can just run <code>plot(rf_mod_oob)</code>.)</li>
<li>The other values of the tuning parameters correspond to random forests. How do random forests compare to bagging? Is this dependent on the tuning parameter? Discuss in terms of the bias-variance tradeoff.</li>
</ol></li>
</ol>
<p><br> <br></p>
<p>Note: You don’t have to include this in your homework, but you could refer back to your test accuracy from a single decision tree from the last set of exercises (where you made <code>tree_mod1</code> in Exercise 6) and check: do random forests actually improve on the single decision tree?</p>
<p><br> <br> <br></p>
<ol start="5" style="list-style-type: decimal">
<li><strong>Confusion matrix</strong><br />
The OOB principle can be used to generate a test confusion matrix (as opposed to a training confusion matrix). This is displayed when you print <code>rf_mod_oob$finalModel</code>. Rows are predicted classes, and columns are true classes. In parts a and b, we’ll nail down the difference between test and training confusion matrices.
<ol style="list-style-type: lower-alpha">
<li>For a particular case in the dataset, how do we determine the trees for which the case is OOB? How do we use these trees to make a single prediction for this case? Thus, how is the test confusion matrix constructed?</li>
<li>This is contrast to a training confusion matrix where we use <strong>all</strong> trees to make a prediction for a particular case–even trees that were built on data that included this case. Why is the test confusion matrix preferable to a training confusion matrix?</li>
<li>Look at the errors that were made for our land use classification. Why (contextually) do you think some errors are more common than others?</li>
</ol></li>
</ol>
<p><br> <br> <br></p>
<ol start="6" style="list-style-type: decimal">
<li><p><strong>Variable importance measures</strong><br />
Because bagging and random forests use tons of trees, the nice interpretability of single decision trees is lost. However, we can still get a measure of how important the different variables were in this classification task. For each of the 147 predictors, the code below gives the “total decrease in node impurities (as measured by the Gini index) from splitting on the variable, averaged over all trees” (package documentation).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">var_imp_rf &lt;-<span class="st"> </span>randomForest<span class="op">::</span><span class="kw">importance</span>(rf_mod_oob<span class="op">$</span>finalModel)
<span class="co"># Sort by importance with dplyr&#39;s arrange()</span>
var_imp_rf &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">predictor =</span> <span class="kw">rownames</span>(var_imp_rf), <span class="dt">MeanDecreaseGini =</span> var_imp_rf[,<span class="st">&quot;MeanDecreaseGini&quot;</span>]) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">arrange</span>(<span class="kw">desc</span>(MeanDecreaseGini))
<span class="co"># Top 20</span>
<span class="kw">head</span>(var_imp_rf, <span class="dv">20</span>)
<span class="co"># Bottom 10</span>
<span class="kw">tail</span>(var_imp_rf, <span class="dv">10</span>)</code></pre></div>
<ol style="list-style-type: lower-alpha">
<li>Check out the codebook for these variables <a href="https://archive.ics.uci.edu/ml/datasets/Urban+Land+Cover">here</a>. The descriptions of the variables aren’t the greatest…but does this ranking make some contextual sense?</li>
<li>It has been found that this random forest measure of variable importance is ok generally but can tend to favor predictors with a lot of unique values. Explain briefly why it makes sense that this can happen by thinking about the recursive binary splitting algorithm for a single tree.</li>
</ol></li>
</ol>
<p><br> <br> <br> <br></p>
<p>Extra! If you want a challenge, try implementing a random forest by yourself using a for-loop. You’ll need a number of pieces of R syntax to get this up and running:</p>
<ul>
<li>Rely on the <code>rpart()</code> function to build the individual trees. You’ll need to supply a formula object for the first part (like <code>class ~ x1+x4+x10</code>). How to do that….? Read on.</li>
<li>Look at documentation for the <code>colnames()</code> and <code>setdiff()</code> functions to get a character vector of the predictors.</li>
<li>Look at the <code>paste()</code> function for combining entries in a character vector into one string.</li>
<li>Look at the <code>as.formula()</code> function.</li>
<li>Look at <code>dplyr</code>’s <code>sample_n()</code> function for performing bootstrapping.</li>
<li>You’ll want to <code>set.seed()</code>…but inside or outside the for-loop?</li>
</ul>
<p>Note: this is a simpler version of a random forest in which a random subset of the predictors is used for an entire single tree rather than different random subsets at each split. To actually implement the random subset of predictors at each split, we’d need to code decision trees from scratch…which is possible, but much more adventurous!</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Use colnames() and setdiff() to get a character vector of the predictors:</span>
predictors &lt;-<span class="st"> </span>???
<span class="co"># Total number of trees in the forest</span>
num_trees &lt;-<span class="st"> </span><span class="dv">100</span>
<span class="co"># Compute the square root of the number of predictors.</span>
<span class="co"># You&#39;ll want to round() this</span>
num_pred_per_tree &lt;-<span class="st"> </span>???
<span class="co"># Set up contain for the trees</span>
tree_list &lt;-<span class="st"> </span><span class="kw">vector</span>(<span class="st">&quot;list&quot;</span>, <span class="dt">length =</span> num_trees)
<span class="co"># Loop!</span>
<span class="cf">for</span> (i <span class="cf">in</span> <span class="kw">seq_len</span>(num_trees)) {
    land_boot &lt;-<span class="st"> </span><span class="kw">sample_n</span>(???)
    random_predictors &lt;-<span class="st"> </span><span class="kw">sample</span>(???)
    tree_formula &lt;-<span class="st"> </span>???
    tree_boot &lt;-<span class="st"> </span><span class="kw">rpart</span>(???)
    tree_list[[i]] &lt;-<span class="st"> </span>tree_boot
}</code></pre></div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="decision-trees.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="support-vector-machines-part-1.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
