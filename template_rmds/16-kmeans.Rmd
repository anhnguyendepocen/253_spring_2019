---
title: "K-Means Clustering"
author: "Your Name"
output: 
  html_document:
    toc: true
    toc_float: true
urlcolor: blue
---

```{r}
library(dplyr)
library(ggplot2)
```


# Exercise 1

To iron out the K-means algorithm, we'll look at a small example of physical measurements (sepal length and width) of 5 irises in the first exercise:

```{r}
irises <- data.frame(
    Width = c(0.5,0.5,2.3,2.8,3.0),
    Length = c(2.0,2.5,1.3,1.8,1.3)
)
ggplot(irises, aes(x = Width, y = Length)) + 
    geom_point() + 
    geom_text(aes(label = c(1:5)), vjust = 1.5) + 
    lims(x = c(0,4), y = c(0,4))
```

Cluster these 5 flowers by hand using K=2 clusters. For the first stage of initialization, use the random cluster assignments shown below in the `cluster` column. Show your work for 2 iterations of the centroiding and reassignment steps.

```{r}
cbind(irises, data.frame(cluster = c(2,1,1,1,2)))
```






# Exercise 2

The `kmeans()` function in R performs K-means clustering. Check your work from the previous exercise.

```{r}
irises_clust2 <- kmeans(irises, centers = 2)

# Check out the cluster assignments
irises_clust2$cluster

# Plot the clusters
irises <- irises %>% 
    mutate(cluster = irises_clust2$cluster)
ggplot(irises, aes(y = Length, x = Width, color = as.factor(cluster))) + 
    geom_point() + 
    lims(x = c(0,4), y = c(0,4))
```






# Exercise 3

```{r}
data(iris)
```

Part of performing K-means clustering is choosing an appropriate value for K.

## Part a

To explore the impact of K, perform a K-means cluster analysis of the `iris` data using just `Sepal.Length` and `Sepal.Width` (for ease of visualization). Use the following code for $K=2$ and adapt it to explore $K=3$ and $K=20$.

```{r}
# Take only Sepal.Length & Sepal.Width
iris_sub <- iris %>%
    dplyr::select(Sepal.Length, Sepal.Width)

# K=2 clusters
km_clust2 <- kmeans(iris_sub, centers = 2)
ggplot(iris_sub, aes(x = Sepal.Width, y = Sepal.Length, color = as.factor(km_clust2$cluster))) + 
    geom_point()
```

## Part b

Based on these plots, discuss the pros and cons of low and high K.






# Exercise 4

Before moving on, let's get more comfortable working with unfamiliar objects in R.

## Part a

Look at the help page for the `kmeans()` function and scroll down to the "Value:" section. What is the name of the component of a `kmeans` object that contains a measure of total within-cluster variation?

## Part b

Whenever we want to extract a certain component of an R object that is a list, we can use the code `list_object$name_of_component`. Display the measure of total within-cluster variation for `km_clust2`.

<br>
<br>
<br>

# Exercise 5

A strategy for picking K is to compare the total squared distance of each case from its assigned centroid (the `total within-cluster sum of squares`) for different values of K.

## Part a

Write a for-loop to calculate the total sum of squared distances for each K in $\{1, 2, ..., 50\}$. Plot these squared distances versus K.

```{r}
# Create storage vector for total within-cluster sum of squares
tot_wc_ss <- rep(0, 50)

# Loop
for (i in 1:50) {
    # Perform k means clustering on iris_sub
    ???

    # Store the total within-cluster sum of squares
    tot_wc_ss[i] <- ???$???
}
plot(1:50, tot_wc_ss)
```

## Part b

Based on this plot, which values of K seem reasonable? Explain.





